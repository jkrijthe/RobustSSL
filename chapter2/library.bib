Automatically generated by Mendeley Desktop 1.11
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@inproceedings{Cai2007,
author = {Cai, Deng and He, Xiaofei and Han, Jiawei},
booktitle = {IEEE 11th International Conference on Computer Vision},
doi = {10.1109/ICCV.2007.4408856},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cai, He, Han - 2007 - Semi-supervised Discriminant Analysis.pdf:pdf},
isbn = {978-1-4244-1630-1},
pages = {1--7},
title = {{Semi-supervised Discriminant Analysis}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4408856},
year = {2007}
}
@article{Cheplygina2010,
author = {Cheplygina, Veronika},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cheplygina - 2010 - Random Subspace Method for One-Class Classifiers.pdf:pdf},
title = {{Random Subspace Method for One-Class Classifiers}},
year = {2010}
}
@inproceedings{Pfahringer2000,
author = {Pfahringer, Bernhard and Giraud-carrier, Christophe},
booktitle = {Proceedings of the 17th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pfahringer, Giraud-carrier - 2000 - Meta-Learning by Landmarking Various Learning Algorithms.pdf:pdf},
pages = {743--750},
title = {{Meta-Learning by Landmarking Various Learning Algorithms}},
year = {2000}
}
@article{Tibshirani1996,
author = {Tibshirani, Robert},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tibshirani - 1996 - Regression shrinkage and selection via the lasso.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B},
keywords = {quadratic programming,regression,shrinkage,subset selection},
number = {1},
pages = {267--288},
title = {{Regression shrinkage and selection via the lasso}},
url = {http://www.jstor.org/stable/10.2307/2346178},
volume = {58},
year = {1996}
}
@article{Heller2012,
author = {Heller, Ruth and Heller, Yair and Gorfine, Malka},
doi = {10.1093/biomet/ass070},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Heller, Heller, Gorfine - 2012 - A consistent multivariate test of association based on ranks of distances.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = dec,
number = {2},
pages = {503--510},
title = {{A consistent multivariate test of association based on ranks of distances}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/ass070},
volume = {100},
year = {2012}
}
@article{Lehmann1993,
author = {Lehmann, EL},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lehmann - 1993 - The Fisher, Neyman-Pearson theories of testing hypotheses One theory or two.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {424},
pages = {1242--1249},
title = {{The Fisher, Neyman-Pearson theories of testing hypotheses: One theory or two?}},
url = {http://www.jstor.org/stable/10.2307/2291263},
volume = {88},
year = {1993}
}
@inproceedings{Xing2013,
author = {Xing, Yan and Cai, H and Cai, Yanguang and Hejlesen, Ole and Toft, Egon},
booktitle = {Proceedings of 2013 Chinese \ldots},
doi = {10.1007/978-3-642-38466-0},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Xing et al. - 2013 - Preliminary Evaluation of Classification Complexity Measures on Imbalanced Data.pdf:pdf},
isbn = {9783642384660},
keywords = {classification,data complexity,\'{a} imbalanced data \'{a}},
pages = {189--196},
publisher = {Springer},
title = {{Preliminary Evaluation of Classification Complexity Measures on Imbalanced Data}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-38466-0\_22},
year = {2013}
}
@article{Buhlmann2002,
author = {Buhlmann, Peter and Yu, Bin},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Buhlmann, Yu - 2002 - Analyzing bagging.pdf:pdf},
journal = {The Annals of Statistics},
number = {4},
pages = {927--961},
title = {{Analyzing bagging}},
volume = {30},
year = {2002}
}
@article{Cortes2012a,
author = {Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cortes, Mohri, Rostamizadeh - 2012 - Algorithms for learning kernels based on centered alignment.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {feature selection,kernel methods,learning kernels},
pages = {795--828},
title = {{Algorithms for learning kernels based on centered alignment}},
url = {http://dl.acm.org/citation.cfm?id=2188413},
volume = {13},
year = {2012}
}
@article{Pan2013,
author = {Pan, Wei and Shen, Xiaotong and Liu, Binghui},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pan, Shen, Liu - 2013 - Cluster Analysis Unsupervised Learning via Supervised Learning with a Non-convex Penalty.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {generalized degrees of freedom,gression,grouping,k-means clustering,lasso,penalized re-,tlp,truncated lasso penalty},
pages = {1865--1889},
title = {{Cluster Analysis: Unsupervised Learning via Supervised Learning with a Non-convex Penalty}},
volume = {14},
year = {2013}
}
@article{Datar2004,
address = {New York, New York, USA},
author = {Datar, Mayur and Immorlica, Nicole and Indyk, Piotr and Mirrokni, Vahab S.},
doi = {10.1145/997817.997857},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Datar et al. - 2004 - Locality-sensitive hashing scheme based on p-stable distributions.pdf:pdf},
isbn = {1581138857},
journal = {Proceedings of the twentieth annual symposium on Computational geometry - SCG '04},
keywords = {-stable distributions,approximate nearest neighbor,locally sen-,sitive hashing,sublinear algorithm},
pages = {253},
publisher = {ACM Press},
title = {{Locality-sensitive hashing scheme based on p-stable distributions}},
url = {http://portal.acm.org/citation.cfm?doid=997817.997857},
year = {2004}
}
@inproceedings{Scholkopf2012,
author = {Sch\"{o}lkopf, Bernhard and Janzing, Dominik and Peters, Jonas},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sch\"{o}lkopf, Janzing, Peters - 2012 - On Causal and Anticausal Learning.pdf:pdf},
pages = {1255--1262},
title = {{On Causal and Anticausal Learning}},
url = {http://arxiv.org/abs/1206.6471},
year = {2012}
}
@article{Gneiting2007,
author = {Gneiting, Tilmann and Raftery, Adrian E},
doi = {10.1198/016214506000001437},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gneiting, Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Estimation.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {bayes factor,bregman divergence,brier score,coherent,continuous ranked probability score,cross-validation,distribution,entropy,kernel score,loss function,minimum contrast estimation,negative definite function,prediction interval,predictive,quantile forecast,scoring rule,skill score,strictly proper,utility function},
month = mar,
number = {477},
pages = {359--378},
title = {{Strictly Proper Scoring Rules, Prediction, and Estimation}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214506000001437},
volume = {102},
year = {2007}
}
@article{Janzing2013,
author = {Janzing, Dominik and Balduzzi, David and Grosse-Wentrup, Moritz and Sch\"{o}lkopf, Bernhard},
doi = {10.1214/13-AOS1145},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Janzing et al. - 2013 - Quantifying causal influences.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = oct,
number = {5},
pages = {2324--2358},
title = {{Quantifying causal influences}},
url = {http://projecteuclid.org/euclid.aos/1383661266},
volume = {41},
year = {2013}
}
@article{Snijders1991,
author = {Snijders, Tom A.B.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Snijders - 1991 - Enumeration and simulation methods for 0–1 matrices with given marginals.pdf:pdf},
journal = {Psychometrika},
keywords = {0-1 matrices with given,adjacency matrices,an important type of,ecology,in the study of,introduction,marginals,monte carlo methods,networks,observation is relational data,random digraphs,reciprocity,social networks,unequal probability sampling},
number = {3},
pages = {397--417},
title = {{Enumeration and simulation methods for 0–1 matrices with given marginals}},
url = {http://www.springerlink.com/index/B3224G6274141M68.pdf},
volume = {56},
year = {1991}
}
@inproceedings{Grunwald2000,
author = {Gr\"{u}nwald, Peter},
booktitle = {Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gr\"{u}nwald - 2000 - Maximum entropy and the glasses you are looking through.pdf:pdf},
pages = {238--246},
title = {{Maximum entropy and the glasses you are looking through}},
url = {http://dl.acm.org/citation.cfm?id=2073975},
year = {2000}
}
@article{Friedman2001,
author = {Friedman, Jerome H.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Friedman - 2001 - Greedy Function Approximation A Gradient Boosting Machine.pdf:pdf},
journal = {The Annals of Statistics},
number = {5},
pages = {1189--1232},
title = {{Greedy Function Approximation: A Gradient Boosting Machine}},
url = {http://home.olemiss.edu/~xdang/676/Greedy\_function\_approximation\_a\_gradient\_bossting\_machine.pdf http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Greedy+Function+Approximation:+A+Gradient+Boosting+Machine\#3},
volume = {29},
year = {2001}
}
@inproceedings{Fujino2005,
author = {Fujino, Akinori and Ueda, Naonori and Saito, Kazumi},
booktitle = {Proceedings of the National Conference on Artificial Intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fujino, Ueda, Saito - 2005 - A hybrid generativediscriminative approach to semi-supervised classifier design.pdf:pdf},
number = {2},
pages = {764--769},
title = {{A hybrid generative/discriminative approach to semi-supervised classifier design}},
url = {http://www.aaai.org/Papers/AAAI/2005/AAAI05-120.pdf},
volume = {20},
year = {2005}
}
@article{Halevy2009,
author = {Halevy, Alon and Norvig, Peter and Pereira, Fernando},
doi = {10.1109/MIS.2009.36},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Halevy, Norvig, Pereira - 2009 - The Unreasonable Effectiveness of Data.pdf:pdf},
issn = {1541-1672},
journal = {IEEE Intelligent Systems},
month = mar,
number = {2},
pages = {8--12},
title = {{The Unreasonable Effectiveness of Data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4804817},
volume = {24},
year = {2009}
}
@article{Wang2013,
author = {Wang, Jun and Jebara, Tony and Chang, Shih-Fu},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Jebara, Chang - 2013 - Semi-Supervised Learning Using Greedy Max-Cut.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {771--800},
title = {{Semi-Supervised Learning Using Greedy Max-Cut}},
url = {http://www.ee.columbia.edu/ln/dvmm/publications/13/ggmc\_13.pdf},
volume = {14},
year = {2013}
}
@article{Wilson1997,
author = {Wilson, D Randall and Martinez, Tony R},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wilson, Martinez - 1997 - Improved Heterogeneous Distance Functions.pdf:pdf},
pages = {1--34},
title = {{Improved Heterogeneous Distance Functions}},
volume = {6},
year = {1997}
}
@article{Xu2009,
address = {New York, New York, USA},
author = {Xu, Linli and White, Martha and Schuurmans, Dale},
doi = {10.1145/1553374.1553519},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Xu, White, Schuurmans - 2009 - Optimal reverse prediction.pdf:pdf},
isbn = {9781605585161},
journal = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09},
pages = {1--8},
publisher = {ACM Press},
title = {{Optimal reverse prediction}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553519},
year = {2009}
}
@article{Li2006,
author = {Li, Tao and Zhu, Shenghuo and Ogihara, Mitsunori},
doi = {10.1007/s10115-006-0013-y},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li, Zhu, Ogihara - 2006 - Using discriminant analysis for multi-class classification an experimental investigation.pdf:pdf},
issn = {0219-1377},
journal = {Knowledge and Information Systems},
keywords = {discriminant analysis,multi-class classification},
month = mar,
number = {4},
pages = {453--472},
title = {{Using discriminant analysis for multi-class classification: an experimental investigation}},
url = {http://www.springerlink.com/index/10.1007/s10115-006-0013-y},
volume = {10},
year = {2006}
}
@book{Hastie2001,
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hastie, Tibshirani, Friedman - 2001 - The Elements of Statistical Learning.pdf:pdf},
publisher = {Spinger},
title = {{The Elements of Statistical Learning}},
url = {http://www-stat.stanford.edu/~tibs/book/},
year = {2001}
}
@article{VanRooden2011,
abstract = {The clinical heterogeneity of Parkinson's disease (PD) may point at the existence of subtypes. Because subtypes likely reflect distinct underlying etiologies, their identification may facilitate future genetic and pharmacotherapeutic studies. Aim of this study was to identify subtypes by a data-driven approach applied to a broad spectrum of motor and nonmotor features of PD. Data of motor and nonmotor PD symptoms were collected in 802 patients in two different European prevalent cohorts. A model-based cluster analysis was conducted on baseline data of 344 patients of a Dutch cohort (PROPARK). Reproducibility of these results was tested in data of the second annual assessment of the same cohort and validated in an independent Spanish cohort (ELEP) of 357 patients. The subtypes were subsequently characterized on clinical and demographic variables. Four similar PD subtypes were identified in two different populations and are largely characterized by differences in the severity of nondopaminergic features and motor complications: Subtype 1 was mildly affected in all domains, Subtype 2 was predominantly characterized by severe motor complications, Subtype 3 was affected mainly on nondopaminergic domains without prominent motor complications, while Subtype 4 was severely affected on all domains. The subtypes had largely similar mean disease durations (nonsignificant differences between three clusters) but showed considerable differences with respect to their association with demographic and clinical variables. In prevalent disease, PD subtypes are largely characterized by the severity of nondopaminergic features and motor complications and likely reflect complex interactions between disease mechanisms, treatment, aging, and gender.},
author = {van Rooden, Stephanie M and Colas, Fabrice P. R. and Mart\'{\i}nez-Mart\'{\i}n, Pablo and Visser, Martine and Verbaan, Dagmar and Marinus, Johan and Chaudhuri, Ray K and Kok, Joost N and van Hilten, Jacobus J},
doi = {10.1002/mds.23346},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/van Rooden et al. - 2011 - Clinical subtypes of Parkinson's disease.pdf:pdf},
issn = {1531-8257},
journal = {Movement disorders : official journal of the Movement Disorder Society},
keywords = {Aged,Cluster Analysis,Cohort Studies,Disease Progression,Female,Germany,Humans,Male,Middle Aged,Neurologic Examination,Parkinson Disease,Parkinson Disease: classification,Parkinson Disease: physiopathology,Reproducibility of Results,Spain,Time Factors},
month = jan,
number = {1},
pages = {51--8},
pmid = {21322019},
title = {{Clinical subtypes of Parkinson's disease.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21322019},
volume = {26},
year = {2011}
}
@book{Gelman2003,
author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Rubin, Donald B.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman et al. - 2003 - Bayesian Data Analysis.pdf:pdf},
title = {{Bayesian Data Analysis}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/cbdv.200490137/abstract http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Bayesian+Data+Analysis\#0},
year = {2003}
}
@article{Hamsici2008,
abstract = {We present an algorithm which provides the one-dimensional subspace where the Bayes error is minimized for the C class problem with homoscedastic Gaussian distributions. Our main result shows that the set of possible one-dimensional spaces v, for which the order of the projected class means is identical, defines a convex region with associated convex Bayes error function g(v). This allows for the minimization of the error function using standard convex optimization algorithms. Our algorithm is then extended to the minimization of the Bayes error in the more general case of heteroscedastic distributions. This is done by means of an appropriate kernel mapping function. This result is further extended to obtain the d-dimensional solution for any given d, by iteratively applying our algorithm to the null space of the (d - 1)-dimensional solution. We also show how this result can be used to improve up on the outcomes provided by existing algorithms, and derive a low-computational cost, linear approximation. Extensive experimental validations are provided to demonstrate the use of these algorithms in classification, data analysis and visualization.},
author = {Hamsici, Onur C and Martinez, Aleix M},
doi = {10.1109/TPAMI.2007.70717},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hamsici, Martinez - 2008 - Bayes optimality in linear discriminant analysis.pdf:pdf},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Bayes Theorem,Computer Simulation,Discriminant Analysis,Image Enhancement,Image Enhancement: methods,Image Interpretation, Computer-Assisted,Image Interpretation, Computer-Assisted: methods,Linear Models,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Reproducibility of Results,Sensitivity and Specificity},
month = apr,
number = {4},
pages = {647--57},
pmid = {18276970},
title = {{Bayes optimality in linear discriminant analysis.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18276970},
volume = {30},
year = {2008}
}
@article{Chaudhuri2006,
author = {Chaudhuri, Kamalika and Healy, DG and Schapira, AHV},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chaudhuri, Healy, Schapira - 2006 - Non-motor symptoms of Parkinson's disease diagnosis and management.pdf:pdf},
journal = {The Lancet Neurology},
pages = {235--245},
title = {{Non-motor symptoms of Parkinson's disease : diagnosis and management}},
url = {http://www.sciencedirect.com/science/article/pii/S1474442206703738},
volume = {5},
year = {2006}
}
@article{Armagan2013,
author = {Armagan, a. and Dunson, D. B. and Lee, J. and Bajwa, W. U. and Strawn, N.},
doi = {10.1093/biomet/ast028},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Armagan et al. - 2013 - Posterior consistency in linear models under shrinkage priors.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = jul,
number = {4},
pages = {1011--1018},
title = {{Posterior consistency in linear models under shrinkage priors}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/ast028},
volume = {100},
year = {2013}
}
@misc{Gelman2012,
author = {Gelman, Andrew},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman - 2012 - Little Data How traditional statistical ideas remain relevant in a big-data world.pdf:pdf},
title = {{Little Data : How traditional statistical ideas remain relevant in a big-data world}},
year = {2012}
}
@inproceedings{Giffin2007,
archivePrefix = {arXiv},
arxivId = {arXiv:0708.1593v2},
author = {Giffin, Adom and Caticha, Ariel},
booktitle = {27th International Workshop on Bayesian Inference and Maximum Entropy Methods in Science and Engineering},
eprint = {arXiv:0708.1593v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Giffin, Caticha - 2007 - Updating Probabilities with Data and Moments.pdf:pdf},
keywords = {bayes theorem,expectation value,moment,relative entropy},
pages = {74--84},
title = {{Updating Probabilities with Data and Moments}},
url = {http://arxiv.org/abs/0708.1593},
year = {2007}
}
@inproceedings{Brazdil1994,
author = {Brazdil, Pavel B. and Gama, Joao and Henery, Bob},
booktitle = {Proceedings of the European conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brazdil, Gama, Henery - 1994 - Characterizing the applicability of classification algorithms using meta-level learning.pdf:pdf},
pages = {83--102},
title = {{Characterizing the applicability of classification algorithms using meta-level learning}},
url = {http://link.springer.com/chapter/10.1007/3-540-57868-4\_52},
year = {1994}
}
@inproceedings{Chapelle2002,
author = {Chapelle, Olivier and Weston, Jason and Sch\"{o}lkopf, Bernhard},
booktitle = {Advances in Neural Information Processing Systems 14},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chapelle, Weston, Sch\"{o}lkopf - 2002 - Cluster kernels for semi-supervised learning.pdf:pdf},
pages = {585--592},
title = {{Cluster kernels for semi-supervised learning}},
url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/AA13.pdf},
year = {2002}
}
@book{Lehmann1998,
author = {Lehmann, E. L. and Casella, G.},
publisher = {Springer-Verlag},
title = {{Theory of Point Estimation}},
year = {1998}
}
@book{Bertsekas1982,
author = {Bertsekas, Dimitri P.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bertsekas - 1982 - Constrained optimization and Lagrange multiplier methods.pdf:pdf},
publisher = {Academic Press},
title = {{Constrained optimization and Lagrange multiplier methods}},
url = {http://adsabs.harvard.edu/abs/1982colm.book.....b},
year = {1982}
}
@article{Cozman2003,
author = {Cozman, Fabio Gagliardi and Cohen, Ira and Cirelo, Marcelo Cesar},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cozman, Cohen, Cirelo - 2003 - Semi-Supervised Learning of Mixture Models.pdf:pdf},
journal = {Proceedings of the Twentieth International Conference on Machine Learning},
title = {{Semi-Supervised Learning of Mixture Models}},
url = {http://www.aaai.org/Papers/ICML/2003/ICML03-016.pdf},
year = {2003}
}
@article{Chiou2007,
author = {Chiou, Jeng-Min and Li, Pai-Ling},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chiou, Li - 2007 - Functional clustering and identifying substructures of longitudinal data.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B},
keywords = {analysis,classification,clustering,functional data,functional principal component,modes of variation,stochastic processes},
pages = {679--699},
title = {{Functional clustering and identifying substructures of longitudinal data}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2007.00605.x/full},
volume = {69},
year = {2007}
}
@article{Gretton,
author = {Gretton, Arthur and Sch\"{o}lkopf, Bernhard},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gretton, Sch\"{o}lkopf - Unknown - A Kernel Method for the Two-Sample-Problem.pdf:pdf},
title = {{A Kernel Method for the Two-Sample-Problem}}
}
@article{Hay2008,
abstract = {There is mounting evidence of a gap between Evidence-based Medicine (EBM) and physician clinical practice, in part because EBM is averaged global evidence gathered from exogenous populations which may not be relevant to local circumstances. Local endogenous evidence, collected in particular and 'real world' patient populations may be more relevant, convincing and timely for clinical practice. Evidence Farming (EF) is a concept to provide such local evidence through the systematic collection of clinical experience to guide more effective practice.},
author = {Hay, M Cameron and Weisner, Thomas S and Subramanian, Saskia and Duan, Naihua and Niedzinski, Edmund J and Kravitz, Richard L},
doi = {10.1111/j.1365-2753.2008.01009.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hay et al. - 2008 - Harnessing experience exploring the gap between evidence-based medicine and clinical practice.pdf:pdf},
issn = {1365-2753},
journal = {Journal of evaluation in clinical practice},
keywords = {Attitude of Health Personnel,California,Clinical Medicine,Clinical Medicine: education,Clinical Medicine: organization \& administration,Data Collection,Decision Making,Decision Support Techniques,Diffusion of Innovation,Evidence-Based Medicine,Evidence-Based Medicine: education,Evidence-Based Medicine: organization \& administra,Focus Groups,Health Knowledge, Attitudes, Practice,Health Services Needs and Demand,Humans,Information Storage and Retrieval,Outcome Assessment (Health Care),Patient Care Planning,Physicians,Physicians: psychology,Pilot Projects,Practice Guidelines as Topic,Questionnaires,Randomized Controlled Trials as Topic},
month = oct,
number = {5},
pages = {707--13},
pmid = {19018899},
title = {{Harnessing experience: exploring the gap between evidence-based medicine and clinical practice.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19018899},
volume = {14},
year = {2008}
}
@inproceedings{Szummer2000,
author = {Szummer, Martin and Jaakkola, Tommi},
booktitle = {Advances in Neural Information Processing Systems 13},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Szummer, Jaakkola - 2000 - Kernel expansions with unlabeled examples.pdf:pdf},
pages = {626--632},
title = {{Kernel expansions with unlabeled examples}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.222\&rep=rep1\&type=pdf},
year = {2000}
}
@inproceedings{Hoekstra1996,
author = {Hoekstra, Aarnoud and Duin, Robert P.W.},
booktitle = {Proceedings of the 13th International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hoekstra, Duin - 1996 - On the nonlinearity of pattern classifiers.pdf:pdf},
pages = {271--275},
title = {{On the nonlinearity of pattern classifiers}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=547429},
year = {1996}
}
@inproceedings{Webb2002,
author = {Webb, Geoffrey I. and Brain, Damien},
booktitle = {Proceedings of the 2002 Pacific Rim Knowledge Acquisition Workshop},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Webb, Brain - 2002 - Generality is predictive of prediction accuracy.pdf:pdf},
pages = {117--130},
title = {{Generality is predictive of prediction accuracy}},
url = {http://www.csse.monash.edu/~webb/cgi-bin/publications.cgi?author=Webb\&keywords=Occams Razor\&pagetitle=Publications\%3A Occam's razor\&format=BibTeX\&sortby=type\&showabstract=y\&showkeywords=y},
year = {2002}
}
@article{Hanselmann2013,
abstract = {Digital staining for the automated annotation of mass spectrometry imaging (MSI) data has previously been achieved using state-of-the-art classifiers such as random forests or support vector machines (SVMs). However, the training of such classifiers requires an expert to label exemplary data in advance. This process is time-consuming and hence costly, especially if the tissue is heterogeneous. In theory, it may be sufficient to only label a few highly representative pixels of an MS image, but it is not known a priori which pixels to select. This motivates active learning strategies in which the algorithm itself queries the expert by automatically suggesting promising candidate pixels of an MS image for labeling. Given a suitable querying strategy, the number of required training labels can be significantly reduced while maintaining classification accuracy. In this work, we propose active learning for convenient annotation of MSI data. We generalize a recently proposed active learning method to the multiclass case and combine it with the random forest classifier. Its superior performance over random sampling is demonstrated on secondary ion mass spectrometry data, making it an interesting approach for the classification of MS images.},
author = {Hanselmann, Michael and R\"{o}der, Jens and K\"{o}the, Ullrich and Renard, Bernhard Y and Heeren, Ron M.A. and Hamprecht, Fred A.},
doi = {10.1021/ac3023313},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hanselmann et al. - 2013 - Active learning for convenient annotation and classification of secondary ion mass spectrometry images.pdf:pdf},
issn = {1520-6882},
journal = {Analytical Chemistry},
month = jan,
number = {1},
pages = {147--55},
pmid = {23157438},
title = {{Active learning for convenient annotation and classification of secondary ion mass spectrometry images.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23157438},
volume = {85},
year = {2013}
}
@article{Cao2007,
author = {Cao, Bin and Chen, Zheng},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cao, Chen - 2007 - Feature Selection in a Kernel Space.pdf:pdf},
title = {{Feature Selection in a Kernel Space}},
year = {2007}
}
@article{Lv2013,
author = {Lv, Jinchi},
doi = {10.1214/13-AOS1149},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lv - 2013 - Impacts of high dimensionality in finite samples.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = aug,
number = {4},
pages = {2236--2262},
title = {{Impacts of high dimensionality in finite samples}},
url = {http://projecteuclid.org/euclid.aos/1382547520},
volume = {41},
year = {2013}
}
@misc{Efron,
author = {Efron, Bradley},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Efron - Unknown - Frequentist Accuracy of Bayesian Estimates.pdf:pdf},
title = {{Frequentist Accuracy of Bayesian Estimates}}
}
@article{Meinshausen2010,
author = {Meinshausen, Nicolai and B\"{u}hlmann, Peter},
doi = {10.1111/j.1467-9868.2010.00740.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Meinshausen, B\"{u}hlmann - 2010 - Stability selection.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {high dimensional data,resampling,stability selection,structure estimation},
month = jul,
number = {4},
pages = {417--473},
title = {{Stability selection}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2010.00740.x},
volume = {72},
year = {2010}
}
@phdthesis{Hamers2012,
author = {Hamers, Adrian},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hamers - 2012 - The Evolution of Coeval Stellar Hierarchical Triple Systems.pdf:pdf},
school = {Utrecht University},
title = {{The Evolution of Coeval Stellar Hierarchical Triple Systems}},
year = {2012}
}
@techreport{Seeger2001,
author = {Seeger, Matthias},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Seeger - 2001 - Learning with labeled and unlabeled data.pdf:pdf},
pages = {1--62},
title = {{Learning with labeled and unlabeled data}},
url = {http://infoscience.epfl.ch/record/161327/files/review.pdf},
year = {2001}
}
@article{Menon,
author = {Menon, Aditya Krishna},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Menon - Unknown - Large-Scale Support Vector Machines Algorithms and Theory.pdf:pdf},
title = {{Large-Scale Support Vector Machines : Algorithms and Theory}}
}
@article{Jones2012,
author = {Jones, Emrys A and Deininger, S\"{o}ren-oliver and Hogendoorn, Pancras C W and Deelder, Andr\'{e} M and Mcdonnell, Liam A},
doi = {10.1016/j.jprot.2012.06.014},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jones et al. - 2012 - Imaging mass spectrometry statistical analysis.pdf:pdf},
issn = {1874-3919},
journal = {Journal of Proteomics},
keywords = {Biomarker discovery,Data analysis,Molecular histology,imaging mass spectrometry},
number = {16},
pages = {4962--4989},
publisher = {Elsevier B.V.},
title = {{Imaging mass spectrometry statistical analysis}},
url = {http://dx.doi.org/10.1016/j.jprot.2012.06.014},
volume = {75},
year = {2012}
}
@article{Ireland1968,
author = {Ireland, C.T. and Kullback, S.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ireland, Kullback - 1968 - Minimum Discrimination Information Estimation.pdf:pdf},
journal = {Biometrics},
number = {3},
pages = {707--713},
title = {{Minimum Discrimination Information Estimation}},
url = {http://www.jstor.org/stable/10.2307/2528330},
volume = {24},
year = {1968}
}
@article{Ho2002a,
author = {Ho, Tin Kam},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho - 2002 - A Data Complexity Analysis of Comparative Advantages of Decision Forest Constructors.pdf:pdf},
journal = {Pattern Analysis and Applications},
number = {2},
pages = {102--112},
title = {{A Data Complexity Analysis of Comparative Advantages of Decision Forest Constructors}},
volume = {5},
year = {2002}
}
@article{Gratiet2014,
author = {Gratiet, Loic and Garnier, Josselin},
doi = {10.1007/s10994-014-5437-0},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gratiet, Garnier - 2014 - Asymptotic analysis of the learning curve for Gaussian process regression.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {asymptotic mean squared error,convergence rate,gaussian process regression,generalization error,learning curves},
month = mar,
title = {{Asymptotic analysis of the learning curve for Gaussian process regression}},
url = {http://link.springer.com/10.1007/s10994-014-5437-0},
year = {2014}
}
@unpublished{Amasyali2009,
author = {Amasyali, M Fatih and Ersoy, Okan K.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Amasyali, Ersoy - 2009 - A Study of Meta Learning for Regression.pdf:pdf},
institution = {Purdue University},
title = {{A Study of Meta Learning for Regression}},
year = {2009}
}
@article{Culp2008a,
author = {Culp, Mark and Michailidis, George},
doi = {10.1198/106186008X344748},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Culp, Michailidis - 2008 - An Iterative Algorithm for Extending Learners to a Semi-Supervised Setting(2).pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {convergence,iterative algorithm,linear smoothers,semi-supervised learning},
month = sep,
number = {3},
pages = {545--571},
title = {{An Iterative Algorithm for Extending Learners to a Semi-Supervised Setting}},
url = {http://www.tandfonline.com/doi/abs/10.1198/106186008X344748},
volume = {17},
year = {2008}
}
@article{Brodley1995,
author = {Brodley, Carla E.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brodley - 1995 - Recursive automatic bias selection for classifier construction.pdf:pdf},
journal = {Machine Learning},
keywords = {automatic algorithm selection,decision trees,hybrid classifiers,inductive bias,learning from},
pages = {63--94},
title = {{Recursive automatic bias selection for classifier construction}},
url = {http://link.springer.com/article/10.1023/A:1022686102325},
volume = {94},
year = {1995}
}
@inproceedings{Zhou2007,
author = {Zhou, Zhi-hua and Zhan, De-Chuan and Yang, Qiang},
booktitle = {Proceedings of the 22nd national conference on Artificial intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Zhan, Yang - 2007 - Semi-supervised learning with very few labeled training examples.pdf:pdf},
title = {{Semi-supervised learning with very few labeled training examples}},
url = {http://www.aaai.org/Papers/AAAI/2007/AAAI07-107.pdf},
year = {2007}
}
@article{Zhou2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1201.3571v1},
author = {Zhou, Hua and Wu, Yichao},
eprint = {arXiv:1201.3571v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Wu - 2012 - A Generic Path Algorithm for Regularized Statistical Estimation.pdf:pdf},
keywords = {gaussian graphical model,generalized linear model,lasso,log-concave density estimation,ordinary differential equations,quasi-likelihoods,regularization,shape restricted regression,solution path},
number = {1},
pages = {1--28},
title = {{A Generic Path Algorithm for Regularized Statistical Estimation}},
year = {2012}
}
@inproceedings{Bottou2011,
author = {Bottou, Leon and Bousquet, Olivier},
booktitle = {Advances in Neural Information Processing Systems 24},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bottou, Bousquet - 2011 - The Tradeoffs of Large-Scale Learning.pdf:pdf},
pages = {In Advances in Neural Information Processing Syste},
title = {{The Tradeoffs of Large-Scale Learning}},
url = {http://books.google.com/books?hl=en\&lr=\&id=JPQx7s2L1A8C\&oi=fnd\&pg=PA351\&dq=The+Tradeoffs+of+Large+Scale+Learning\&ots=vbhayjhcGc\&sig=kWCMo7N51TgoLQSVSv2f\_ILArjo http://books.google.com/books?hl=en\&lr=\&id=JPQx7s2L1A8C\&oi=fnd\&pg=PA351\&dq=The+Tradeoffs+of+Large-Scale+Learning\&ots=vbjaAkg8Fe\&sig=chdz7lCKXTFdUaLPYAgH\_FfgLmA},
year = {2011}
}
@article{Witten2011,
abstract = {We consider the supervised classification setting, in which the data consist of p features measured on n observations, each of which belongs to one of K classes. Linear discriminant analysis (LDA) is a classical method for this problem. However, in the high-dimensional setting where p ≫ n, LDA is not appropriate for two reasons. First, the standard estimate for the within-class covariance matrix is singular, and so the usual discriminant rule cannot be applied. Second, when p is large, it is difficult to interpret the classification rule obtained from LDA, since it involves all p features. We propose penalized LDA, a general approach for penalizing the discriminant vectors in Fisher's discriminant problem in a way that leads to greater interpretability. The discriminant problem is not convex, so we use a minorization-maximization approach in order to efficiently optimize it when convex penalties are applied to the discriminant vectors. In particular, we consider the use of L(1) and fused lasso penalties. Our proposal is equivalent to recasting Fisher's discriminant problem as a biconvex problem. We evaluate the performances of the resulting methods on a simulation study, and on three gene expression data sets. We also survey past methods for extending LDA to the high-dimensional setting, and explore their relationships with our proposal.},
author = {Witten, Daniela M. and Tibshirani, Robert},
doi = {10.1111/j.1467-9868.2011.00783.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Witten, Tibshirani - 2011 - Penalized classification using Fisher's linear discriminant.pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society. Series B, Statistical methodology},
keywords = {classification,discriminant analysis,feature selection,high dimensional problems,lasso,linear,supervised learning},
month = nov,
number = {5},
pages = {753--772},
pmid = {22323898},
title = {{Penalized classification using Fisher's linear discriminant.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3272679\&tool=pmcentrez\&rendertype=abstract},
volume = {73},
year = {2011}
}
@article{Guyon2003,
author = {Guyon, Isabelle and Elisseeff, Andre},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Guyon, Elisseeff - 2003 - An Introduction to Variable and Feature Selection.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {bioinformatics,clustering,computational biology,ery,feature selection,filters,gene expression,genomics,information retrieval,information theory,microarray,model selection,pattern discov-,proteomics,qsar,space dimensionality reduction,statistical testing,support vector machines,text classification,variable selection,wrappers},
pages = {1157--1182},
title = {{An Introduction to Variable and Feature Selection}},
volume = {3},
year = {2003}
}
@book{Aubin2000,
author = {Aubin, Jean-Pierre},
publisher = {John Wiley \& Sons},
title = {{Applied functional analysis}},
volume = {47},
year = {2000}
}
@article{Liu2010,
abstract = {Most genomic data have ultra-high dimensions with more than 10,000 genes (probes). Regularization methods with L₁ and L(p) penalty have been extensively studied in survival analysis with high-dimensional genomic data. However, when the sample size n << m (the number of genes), directly identifying a small subset of genes from ultra-high (m > 10, 000) dimensional data is time-consuming and not computationally efficient. In current microarray analysis, what people really do is select a couple of thousands (or hundreds) of genes using univariate analysis or statistical tests, and then apply the LASSO-type penalty to further reduce the number of disease associated genes. This two-step procedure may introduce bias and inaccuracy and lead us to miss biologically important genes.},
author = {Liu, Zhenqiu and Chen, Dechang and Tan, Ming and Jiang, Feng and Gartenhaus, Ronald B},
doi = {10.1186/1471-2105-11-606},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Liu et al. - 2010 - Kernel based methods for accelerated failure time model with ultra-high dimensional data.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Gene Expression Profiling,Gene Expression Profiling: methods,Linear Models,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Survival Analysis},
month = jan,
number = {5},
pages = {606},
pmid = {21176134},
title = {{Kernel based methods for accelerated failure time model with ultra-high dimensional data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21550884},
volume = {11},
year = {2010}
}
@article{Byrd1995,
author = {Byrd, Richard H. and Lu, Peihuang and Nocedal, Jorge and Zhu, Ciyou},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Byrd et al. - 1995 - A limited memory algorithm for bound constrained optimization.pdf:pdf},
journal = {SIAM Journal on Scientific Computing},
title = {{A limited memory algorithm for bound constrained optimization}},
url = {http://epubs.siam.org/doi/pdf/10.1137/0916069},
year = {1995}
}
@article{Dy2004a,
author = {Dy, Jennifer G. and Brodley, Carla E.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dy, Brodley - 2004 - Feature selection for unsupervised learning.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {clustering,expectation-maximization,feature selection,unsupervised learning},
pages = {845--889},
title = {{Feature selection for unsupervised learning}},
url = {http://dl.acm.org/citation.cfm?id=1016787},
volume = {5},
year = {2004}
}
@article{Loog2014b,
author = {Loog, Marco and Jensen, Are C},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
title = {{Semi-Supervised Nearest Mean Classification through a constrained Log-Likelihood}},
year = {2014},
note = {conditionally accepted}
}
@article{Schmidhuber2012,
author = {Schmidhuber, Jurgen},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schmidhuber - 2012 - Philosophers \& futurists, catch up.pdf:pdf},
journal = {Journal of Consciousness Studies},
number = {1},
pages = {173--182},
title = {{Philosophers \& futurists, catch up}},
url = {http://www.idsia.ch/~juergen/2012futurists.pdf},
year = {2012}
}
@article{Breiman2001,
author = {Breiman, Leo},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Breiman - 2001 - Statistical Modeling The Two Cultures.pdf:pdf},
journal = {Statistical Science},
number = {3},
pages = {199--231},
title = {{Statistical Modeling: The Two Cultures}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Statistical+Modeling+:+The+Two+Cultures\#2 http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Statistical+Modeling:+The+Two+Cultures\#2},
volume = {16},
year = {2001}
}
@inproceedings{Jaakkola1999,
author = {Jaakkola, Tommi S. and Haussler, David},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jaakkola, Haussler - 1999 - Exploiting generative models in discriminative classifiers.pdf:pdf},
pages = {487--493},
title = {{Exploiting generative models in discriminative classifiers}},
url = {http://www.uniroma2.it/didattica/BdDD/deposito/jaakkola98exploiting-haussler.pdf},
year = {1999}
}
@article{Trappenberg,
author = {Trappenberg, Thomas},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Trappenberg - Unknown - A brief introduction to probabilistic machine learning with neuroscientific relations.pdf:pdf},
title = {{A brief introduction to probabilistic machine learning with neuroscientific relations}}
}
@article{Loog,
author = {Loog, Marco},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - Unknown - Semi-Supervised Linear Discriminant Analysis through Moment-Constraint Parameter Estimation(2).pdf:pdf},
keywords = {affine invariant,classification,constraints,linear discriminant analysis,moment,semi-supervised learning},
title = {{Semi-Supervised Linear Discriminant Analysis through Moment-Constraint Parameter Estimation}}
}
@article{Jager2013a,
abstract = {The accuracy of published medical research is critical for scientists, physicians and patients who rely on these results. However, the fundamental belief in the medical literature was called into serious question by a paper suggesting that most published medical research is false. Here we adapt estimation methods from the genomics community to the problem of estimating the rate of false discoveries in the medical literature using reported \$P\$-values as the data. We then collect \$P\$-values from the abstracts of all 77 430 papers published in The Lancet, The Journal of the American Medical Association, The New England Journal of Medicine, The British Medical Journal, and The American Journal of Epidemiology between 2000 and 2010. Among these papers, we found 5322 reported \$P\$-values. We estimate that the overall rate of false discoveries among reported results is 14\% (s.d. 1\%), contrary to previous claims. We also found that there is no a significant increase in the estimated rate of reported false discovery results over time (0.5\% more false positives (FP) per year, \$P = 0.18\$) or with respect to journal submissions (0.5\% more FP per 100 submissions, \$P = 0.12\$). Statistical analysis must allow for false discoveries in order to make claims on the basis of noisy data. But our analysis suggests that the medical literature remains a reliable record of scientific progress.},
author = {Jager, Leah R and Leek, Jeffrey T},
doi = {10.1093/biostatistics/kxt007},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jager, Leek - 2013 - An estimate of the science-wise false discovery rate and application to the top medical literature.pdf:pdf},
issn = {1468-4357},
journal = {Biostatistics (Oxford, England)},
keywords = {false discovery rate,genomics,meta-analysis,multiple testing,science-wise false discovery rate},
month = sep,
pages = {1--12},
pmid = {24068246},
title = {{An estimate of the science-wise false discovery rate and application to the top medical literature.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24068246},
year = {2013}
}
@book{Berger1985,
author = {Berger, James O},
publisher = {Springer},
title = {{Statistical decision theory and Bayesian analysis}},
year = {1985}
}
@article{Schmidt1995,
author = {Schmidt, Karsten and Stahlecker, Peter},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schmidt, Stahlecker - 1995 - Reducing the Maximum Risk of Regression Estimators by Polyhedral Projection.pdf:pdf},
journal = {Journal of Statistical Computation and Simulation},
number = {1},
pages = {1--15},
title = {{Reducing the Maximum Risk of Regression Estimators by Polyhedral Projection}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00949659508811648},
volume = {52},
year = {1995}
}
@inproceedings{Tax2005,
author = {Tax, David M.J. and Duin, Robert P.W.},
booktitle = {Proceedings of the Sixteenth Annual Symposium of the Pattern Recognition Association of South Africa},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tax, Duin - 2005 - Characterizing one-class datasets.pdf:pdf},
number = {4},
pages = {21--26},
title = {{Characterizing one-class datasets}},
url = {http://mediamatica.ewi.tudelft.nl/sites/default/files/TaxDui2005.pdf},
volume = {1},
year = {2005}
}
@article{Wickenberg-Bolin2006,
abstract = {Supervised learning for classification of cancer employs a set of design examples to learn how to discriminate between tumors. In practice it is crucial to confirm that the classifier is robust with good generalization performance to new examples, or at least that it performs better than random guessing. A suggested alternative is to obtain a confidence interval of the error rate using repeated design and test sets selected from available examples. However, it is known that even in the ideal situation of repeated designs and tests with completely novel samples in each cycle, a small test set size leads to a large bias in the estimate of the true variance between design sets. Therefore different methods for small sample performance estimation such as a recently proposed procedure called Repeated Random Sampling (RSS) is also expected to result in heavily biased estimates, which in turn translates into biased confidence intervals. Here we explore such biases and develop a refined algorithm called Repeated Independent Design and Test (RIDT).},
author = {Wickenberg-Bolin, Ulrika and G\"{o}ransson, Hanna and Frykn\"{a}s, M\aa rten and Gustafsson, Mats G and Isaksson, Anders},
doi = {10.1186/1471-2105-7-127},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wickenberg-Bolin et al. - 2006 - Improved variance estimation of classification performance via reduction of bias caused by small sample size.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Analysis of Variance,Artificial Intelligence,Bias (Epidemiology),Diagnosis, Computer-Assisted,Diagnosis, Computer-Assisted: methods,Gene Expression Profiling,Gene Expression Profiling: methods,Humans,Models, Biological,Models, Statistical,Neoplasm Proteins,Neoplasm Proteins: analysis,Neoplasms,Neoplasms: diagnosis,Neoplasms: metabolism,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Reproducibility of Results,Sample Size,Sensitivity and Specificity,Tumor Markers, Biological,Tumor Markers, Biological: analysis},
month = jan,
pages = {127},
pmid = {16533392},
title = {{Improved variance estimation of classification performance via reduction of bias caused by small sample size.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1435937\&tool=pmcentrez\&rendertype=abstract},
volume = {7},
year = {2006}
}
@article{Mai2013,
author = {Mai, Qing and Zou, Hui},
doi = {10.1093/biomet/ass062},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mai, Zou - 2013 - The Kolmogorov filter for variable screening in high-dimensional binary classification.pdf:pdf},
journal = {Biometrika},
number = {1},
pages = {229--234},
title = {{The Kolmogorov filter for variable screening in high-dimensional binary classification}},
url = {http://biomet.oxfordjournals.org/content/100/1/229.short},
volume = {100},
year = {2013}
}
@article{Gama1995,
author = {Gama, Joao and Brazdil, Pavel B.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gama, Brazdil - 1995 - Characterization of Classification Algorithms.pdf:pdf},
journal = {Progress in Artificial Intelligence},
pages = {189--200},
title = {{Characterization of Classification Algorithms}},
url = {http://link.springer.com/chapter/10.1007/3-540-60428-6\_16},
year = {1995}
}
@article{Nguyen2010,
author = {Nguyen, XuanLong and Wainwright, Martin J. and Jordan, Michael I.},
doi = {10.1109/TIT.2010.2068870},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nguyen, Wainwright, Jordan - 2010 - Estimating Divergence Functionals and the Likelihood Ratio by Convex Risk Minimization.pdf:pdf},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
month = nov,
number = {11},
pages = {5847--5861},
title = {{Estimating Divergence Functionals and the Likelihood Ratio by Convex Risk Minimization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5605355},
volume = {56},
year = {2010}
}
@article{Wilson,
archivePrefix = {arXiv},
arxivId = {arXiv:1302.4245v2},
author = {Wilson, Andrew Gordon and Adams, Ryan Prescott},
eprint = {arXiv:1302.4245v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wilson, Adams - Unknown - Gaussian Process Covariance Kernels for Pattern Discovery and Extrapolation.pdf:pdf},
title = {{Gaussian Process Covariance Kernels for Pattern Discovery and Extrapolation}}
}
@inproceedings{Rao1995,
author = {Rao, R. Bharat and Gordon, Diana and Spears, William},
booktitle = {Proceedings of the 12th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rao, Gordon, Spears - 1995 - For Every Generalization Action, Is there really an Equal and Opposite Reaction Analysis of the conservatio.pdf:pdf},
pages = {471--479},
title = {{For Every Generalization Action, Is there really an Equal and Opposite Reaction? Analysis of the conservation Law for Generalization Performance}},
url = {http://www.researchgate.net/publication/2516136\_For\_Every\_Generalization\_Action\_Is\_There\_Really\_An\_Equal\_And\_Opposite\_Reaction\_Analysis\_of\_the\_Conservation\_Law\_for\_Generalization\_Performance/file/79e4150b866697f897.pdf},
year = {1995}
}
@incollection{Cozman2006,
author = {Cozman, F and Cohen, Ira},
booktitle = {Semi-Supervised Learning},
chapter = {4},
editor = {Chapelle, Olivier and Sch\"{o}lkopf, Bernhard and Zien, A},
pages = {56--72},
publisher = {MIT press},
title = {{Risks of Semi-Supervised Learning}},
year = {2006}
}
@article{Suzuki2013,
author = {Suzuki, Taiji and Sugiyama, Masashi},
doi = {10.1214/13-AOS1095},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Suzuki, Sugiyama - 2013 - Fast learning rate of multiple kernel learning Trade-off between sparsity and smoothness.pdf:pdf},
journal = {The Annals of Statistics},
keywords = {additive model,and phrases,convergence rate,elastic-net,multiple kernel learning,reproducing kernel hilbert spaces,restricted isometry,smoothness,sparse learning},
number = {3},
pages = {1381--1405},
title = {{Fast learning rate of multiple kernel learning: Trade-off between sparsity and smoothness}},
url = {http://projecteuclid.org/euclid.aos/1375362553},
volume = {41},
year = {2013}
}
@article{Vilalta2002,
author = {Vilalta, Ricardo and Drissi, Youssef},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vilalta, Drissi - 2002 - A perspective view and survey of meta-learning.pdf:pdf},
journal = {Artificial Intelligence Review},
keywords = {classification,inductive learning,meta-knowledge},
number = {1997},
pages = {77--95},
title = {{A perspective view and survey of meta-learning}},
url = {http://link.springer.com/article/10.1023/A:1019956318069},
year = {2002}
}
@inproceedings{Dasgupta2002,
author = {Dasgupta, Sanjoy and Littman, Michael L. and McAlles},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dasgupta, Littman, McAlles - 2002 - PAC generalization bounds for co-training.pdf:pdf},
pages = {375--382},
title = {{PAC generalization bounds for co-training}},
url = {http://books.google.com/books?hl=en\&lr=\&id=PGrlRWV5-v0C\&oi=fnd\&pg=PA375\&dq=PAC+Generalization+Bounds+for+Co-training\&ots=auaN1CGPip\&sig=0dID1oXJYgeENxwSzfsntvwz\_oU},
year = {2002}
}
@article{Sabato2013,
author = {Sabato, Sivan and Srebro, Nathan and Tishby, Naftali},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sabato, Srebro, Tishby - 2013 - Distribution-Dependent Sample Complexity of Large Margin Learning.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {distribution-dependence,linear classifiers,sample complexity,supervised learning},
pages = {2119--2149},
title = {{Distribution-Dependent Sample Complexity of Large Margin Learning}},
volume = {14},
year = {2013}
}
@article{Berthet2013,
author = {Berthet, Quentin and Rigollet, Philippe},
doi = {10.1214/13-AOS1127},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Berthet, Rigollet - 2013 - Optimal detection of sparse principal components in high dimension.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = aug,
number = {4},
pages = {1780--1815},
title = {{Optimal detection of sparse principal components in high dimension}},
url = {http://projecteuclid.org/euclid.aos/1378386239},
volume = {41},
year = {2013}
}
@unpublished{Looga,
author = {Loog, Marco and Jensen, Are C},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog, Jensen - Unknown - A Constrained Log-Likelihood Formulation for Semi-Supervised Nearest Mean Classification.pdf:pdf},
keywords = {constrained estimation,log-likelihood,nearest mean classifier,semi-supervised learning},
number = {1},
title = {{A Constrained Log-Likelihood Formulation for Semi-Supervised Nearest Mean Classification}},
volume = {1}
}
@article{Fumera,
author = {Fumera, Giorgio and Roli, Fabio and Serrau, Alessandra},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fumera, Roli, Serrau - Unknown - A Theoretical Analysis of Bagging as a Linear Combination of Classifiers.pdf:pdf},
pages = {1--19},
title = {{A Theoretical Analysis of Bagging as a Linear Combination of Classifiers}}
}
@article{Lee2004,
author = {Lee, Yoonkyung and Lin, Yi and Wahba, Grace},
doi = {10.1198/016214504000000098},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lee, Lin, Wahba - 2004 - Multicategory Support Vector Machines.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {generalized approximate cross-validation,method,nonparametric classi cation method,quadratic programming,regularization,reproducing kernel hilbert space},
month = mar,
number = {465},
pages = {67--81},
title = {{Multicategory Support Vector Machines}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214504000000098},
volume = {99},
year = {2004}
}
@unpublished{Bresson2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1210.0699v1},
author = {Bresson, Xavier and Zhang, Ruiliang},
booktitle = {arXiv preprint},
eprint = {arXiv:1210.0699v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bresson, Zhang - 2012 - TV-SVM Total Variation Support Vector Machine for Semi-Supervised Data Classification.pdf:pdf},
title = {{TV-SVM: Total Variation Support Vector Machine for Semi-Supervised Data Classification}},
url = {http://arxiv.org/abs/1210.0699},
year = {2012}
}
@inproceedings{Shalev-Shwartz2007,
author = {Shalev-Shwartz, Shai and Singer, Yoram},
booktitle = {Proceedings of the 24th International Conference on Machine learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shalev-Shwartz, Singer - 2007 - Pegasos Primal estimated sub-gradient solver for svm.pdf:pdf},
pages = {807--814},
title = {{Pegasos: Primal estimated sub-gradient solver for svm}},
url = {http://link.springer.com/article/10.1007/s10107-010-0420-4},
year = {2007}
}
@article{Bruand2011,
abstract = {Mass Spectrometric Imaging (MSI) is a molecular imaging technique that allows the generation of 2D ion density maps for a large complement of the active molecules present in cells and sectioned tissues. Automatic segmentation of such maps according to patterns of co-expression of individual molecules can be used for discovery of novel molecular signatures (molecules that are specifically expressed in particular spatial regions). However, current segmentation techniques are biased toward the discovery of higher abundance molecules and large segments; they allow limited opportunity for user interaction, and validation is usually performed by similarity to known anatomical features. We describe here a novel method, AMASS (Algorithm for MSI Analysis by Semi-supervised Segmentation). AMASS relies on the discriminating power of a molecular signal instead of its intensity as a key feature, uses an internal consistency measure for validation, and allows significant user interaction and supervision as options. An automated segmentation of entire leech embryo data images resulted in segmentation domains congruent with many known organs, including heart, CNS ganglia, nephridia, nephridiopores, and lateral and ventral regions, each with a distinct molecular signature. Likewise, segmentation of a rat brain MSI slice data set yielded known brain features and provided interesting examples of co-expression between distinct brain regions. AMASS represents a new approach for the discovery of peptide masses with distinct spatial features of expression. Software source code and installation and usage guide are available at http://bix.ucsd.edu/AMASS/ .},
author = {Bruand, Jocelyne and Alexandrov, Theodore and Sistla, Srinivas and Wisztorski, Maxence and Meriaux, C\'{e}line and Becker, Michael and Salzet, Michel and Fournier, Isabelle and Macagno, Eduardo and Bafna, Vineet},
doi = {10.1021/pr2005378},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bruand et al. - 2011 - AMASS algorithm for MSI analysis by semi-supervised segmentation.pdf:pdf},
issn = {1535-3907},
journal = {Journal of proteome research},
keywords = {Algorithms,Animals,Automatic Data Processing,Brain,Brain: metabolism,Cluster Analysis,Computational Biology,Computational Biology: methods,Gene Expression Regulation,Gene Expression Regulation, Developmental,Image Processing, Computer-Assisted,Image Processing, Computer-Assisted: methods,Leeches,Mass Spectrometry,Mass Spectrometry: methods,Peptides,Peptides: chemistry,Rats,Spectrometry, Mass, Matrix-Assisted Laser Desorpti},
month = oct,
number = {10},
pages = {4734--43},
pmid = {21800894},
title = {{AMASS: algorithm for MSI analysis by semi-supervised segmentation.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3190602\&tool=pmcentrez\&rendertype=abstract},
volume = {10},
year = {2011}
}
@article{Huang2013,
abstract = {Traditionally, the hinge loss is used to construct support vector machine (SVM) classifiers. The hinge loss is related to the shortest distance between sets and the corresponding classifier is hence sensitive to noise and unstable for re-sampling. In contrast, the pinball loss is related to the quantile distance and the result is less sensitive. The pinball loss has been deeply studied and widely applied in regression but it has not been used for classification. In this paper, we propose a SVM classifier with the pinball loss, called pin-SVM, and investigate its properties, including noise insensitivity, robustness, and misclassification error. Besides, insensitive zone is applied to the pin-SVM and a sparse model is obtained. Compared to the SVM with the hinge loss, the proposed pin-SVM has the same computational complexity and enjoys noise insensitivity and re-sampling stability.},
author = {Huang, Xiaolin and Shi, Lei and Suykens, Johan a K},
doi = {D7CF84C8-DF7E-492B-A669-0B4CFDEE5D3D},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huang, Shi, Suykens - 2013 - Support Vector Machine Classifier with Pinball Loss.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = sep,
number = {5},
pages = {984--997},
pmid = {24062537},
title = {{Support Vector Machine Classifier with Pinball Loss.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24062537},
volume = {36},
year = {2013}
}
@article{Yamada2013a,
author = {Yamada, Makoto and Sugiyama, Masashi and Sese, Jun},
doi = {10.1007/s10994-013-5423-y},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yamada, Sugiyama, Sese - 2013 - Least-squares independence regression for non-linear causal inference under non-Gaussian noise.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {causal inference,information,least-squares independence regression,non-gaussian,non-linear,squared-loss mutual},
month = nov,
title = {{Least-squares independence regression for non-linear causal inference under non-Gaussian noise}},
url = {http://link.springer.com/10.1007/s10994-013-5423-y},
year = {2013}
}
@article{Zhao2013,
author = {Zhao, Ming-Jie and Edakunni, Narayanan and Pocock, Adam and Brown, Gavin},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhao et al. - 2013 - Beyond Fano’s Inequality Bounds on the Optimal F-Score , BER , and Cost-Sensitive Risk and Their Implications.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {balanced error rate,conditional entropy,cost-sensitive risk,f $\beta$ -measure,f-score},
pages = {1033--1090},
title = {{Beyond Fano’s Inequality : Bounds on the Optimal F-Score , BER , and Cost-Sensitive Risk and Their Implications}},
url = {http://jmlr.csail.mit.edu/papers/volume14/zhao13a/zhao13a.pdf},
volume = {14},
year = {2013}
}
@article{Samsudin2010,
author = {Samsudin, Noor a. and Bradley, Andrew P.},
doi = {10.1016/j.patcog.2010.05.010},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Samsudin, Bradley - 2010 - Nearest neighbour group-based classification.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Compound classification,Nearest neighbour,group-based classification},
month = oct,
number = {10},
pages = {3458--3467},
publisher = {Elsevier},
title = {{Nearest neighbour group-based classification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0031320310002116},
volume = {43},
year = {2010}
}
@article{Subramanya2011a,
author = {Subramanya, A and Bilmes, Jeff},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Subramanya, Bilmes - 2011 - Semi-supervised learning with measure propagation.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {graph-based semi-supervised learning,large-scale semi-supervised,learning,non-parametric models,transductive inference},
pages = {3311--3370},
title = {{Semi-supervised learning with measure propagation}},
url = {http://dl.acm.org/citation.cfm?id=2078212},
volume = {12},
year = {2011}
}
@phdthesis{Marlin2008,
author = {Marlin, BM},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Marlin - 2008 - Missing data problems in machine learning.pdf:pdf},
school = {University of Toronto},
title = {{Missing data problems in machine learning}},
url = {http://www-devel.cs.ubc.ca/~bmarlin/research/phd\_thesis/marlin-phd-thesis.pdf},
year = {2008}
}
@article{Astorino2007,
author = {Astorino, A and Fuduli, A},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Astorino, Fuduli - 2007 - Nonsmooth optimization techniques for Semi-Supervised Classification.pdf:pdf},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {nonsmooth optimization,semi,supervised learning},
number = {12},
pages = {2135--2142},
title = {{Nonsmooth optimization techniques for Semi-Supervised Classification}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4359288},
volume = {29},
year = {2007}
}
@article{Kawakita2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1204.3965v1},
author = {Kawakita, Masanori and Kanamori, Takafumi},
eprint = {arXiv:1204.3965v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kawakita, Kanamori - 2013 - Semi-Supervised learning with Density-Ratio Estimation.pdf:pdf},
journal = {Machine Learning},
number = {2},
pages = {189--209},
title = {{Semi-Supervised learning with Density-Ratio Estimation}},
volume = {91},
year = {2013}
}
@article{Kappen2013,
author = {Kappen, Hilbert J. and G\'{o}mez, Vicen\c{c}},
doi = {10.1007/s10994-013-5427-7},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kappen, G\'{o}mez - 2013 - The Variational Garrote.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {mean-field,sparse regression,spike-and-slab,variational approximation},
month = dec,
number = {January 2012},
title = {{The Variational Garrote}},
url = {http://link.springer.com/10.1007/s10994-013-5427-7},
year = {2013}
}
@article{Goldberg2009,
author = {Goldberg, Andrew B. and Zhu, Xiaojin},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Goldberg, Zhu - 2009 - Keepin'it real semi-supervised learning with realistic tuning.pdf:pdf},
journal = {NAACL HLT 2009 Workshop on Semi-supervised Learning for Natural Language Processing},
title = {{Keepin'it real: semi-supervised learning with realistic tuning}},
url = {http://dl.acm.org/citation.cfm?id=1621832},
year = {2009}
}
@inproceedings{Szummer2002,
author = {Szummer, Martin and Jaakkola, Tommi S.},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Szummer, Jaakkola - 2002 - Information regularization with partially labeled data.pdf:pdf},
pages = {1025--1032},
title = {{Information regularization with partially labeled data}},
url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/AA69.pdf},
year = {2002}
}
@inproceedings{Li2009,
author = {Li, Yu-Feng and Kwok, James T. and Zhou, Zhi-hua},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li, Kwok, Zhou - 2009 - Semi-supervised learning using label mean.pdf:pdf},
pages = {633--640},
title = {{Semi-supervised learning using label mean}},
url = {http://dl.acm.org/citation.cfm?id=1553456},
year = {2009}
}
@article{Gelman2011,
author = {Gelman, Andrew},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman - 2011 - Induction and deduction in Bayesian data analysis.pdf:pdf},
journal = {Rationality, Markets and Morals (RMM)},
pages = {67--78},
title = {{Induction and deduction in Bayesian data analysis}},
url = {http://www.stat.columbia.edu/~gelman/research/unpublished/philosophy\_online4.pdf},
volume = {2},
year = {2011}
}
@article{Webb1996,
author = {Webb, Geoffrey I.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Webb - 1996 - Further Experimental Evidence against the Utility of Occam's Razor.pdf:pdf},
journal = {Journal of Artificial Intelligence Research},
pages = {397--417},
title = {{Further Experimental Evidence against the Utility of Occam's Razor}},
url = {http://arxiv.org/abs/cs/9605101},
volume = {4},
year = {1996}
}
@article{Castelli1994,
author = {Castelli, Vittorio and Cover, Thomas M.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Castelli, Cover - 1995 - On the exponential value of labeled samples.pdf:pdf},
journal = {Pattern Recognition Letters},
pages = {105--111},
title = {{On the exponential value of labeled samples}},
volume = {16},
year = {1995}
}
@article{Shipp2001,
author = {Shipp, Catherine A. and Kuncheva, Ludmila I},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shipp, Kuncheva - 2001 - Four Measures of Data Complexity for Bootstrapping, Splitting and Feature Sampling.pdf:pdf},
isbn = {0000000000000},
journal = {Proc. CIMA},
title = {{Four Measures of Data Complexity for Bootstrapping, Splitting and Feature Sampling}},
url = {http://www.bangor.ac.uk/~mas00a/papers/cslkAIDA01.pdf},
year = {2001}
}
@book{Chapelle2006,
author = {Chapelle, Olivier and Sch\"{o}lkopf, Bernhard and Zien, Alexander},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chapelle, Sch\"{o}lkopf, Zien - 2006 - Semi-supervised learning.pdf:pdf},
isbn = {9780262033589},
publisher = {MIT press},
title = {{Semi-supervised learning}},
year = {2006}
}
@article{Mooij,
author = {Mooij, Joris M and Heskes, Tom and Janzing, Dominik and Sch\"{o}lkopf, Bernhard},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mooij et al. - Unknown - On Causal Discovery with Cyclic Additive Noise Models.pdf:pdf},
pages = {1--9},
title = {{On Causal Discovery with Cyclic Additive Noise Models}}
}
@article{Zadeh2012,
author = {Zadeh, Reza Bosagh and Goel, Ashish},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zadeh, Goel - 2012 - Dimension independent similarity computation.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {cosine,dice,dimension independent,jaccard,mapreduce,overlap,similarity},
pages = {1605--1626},
title = {{Dimension independent similarity computation}},
url = {http://arxiv.org/abs/1206.2082},
volume = {14},
year = {2012}
}
@article{Sun2014,
author = {Sun, Peng and Reid, Mark D. and Zhou, Jie},
doi = {10.1007/s10994-014-5434-3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sun, Reid, Zhou - 2014 - An improved multiclass LogitBoost using adaptive-one-vs-one.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = feb,
title = {{An improved multiclass LogitBoost using adaptive-one-vs-one}},
url = {http://link.springer.com/10.1007/s10994-014-5434-3},
year = {2014}
}
@article{Shannon1948,
author = {Shannon, Claude Elwood},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shannon - 1948 - A mathematical theory of communication.pdf:pdf},
journal = {The Bell System Technical Journal},
number = {J},
pages = {379--423},
title = {{A mathematical theory of communication}},
url = {http://dl.acm.org/citation.cfm?id=584093},
volume = {27},
year = {1948}
}
@article{Rosasco2004,
abstract = {In this letter, we investigate the impact of choosing different loss functions from the viewpoint of statistical learning theory. We introduce a convexity assumption, which is met by all loss functions commonly used in the literature, and study how the bound on the estimation error changes with the loss. We also derive a general result on the minimizer of the expected risk for a convex loss function in the case of classification. The main outcome of our analysis is that for classification, the hinge loss appears to be the loss of choice. Other things being equal, the hinge loss leads to a convergence rate practically indistinguishable from the logistic loss rate and much better than the square loss rate. Furthermore, if the hypothesis space is sufficiently rich, the bounds obtained for the hinge loss are not loosened by the thresholding stage.},
author = {Rosasco, Lorenzo and {De Vito}, Ernesto and Caponnetto, Andrea and Piana, Michele and Verri, Alessandro},
doi = {10.1162/089976604773135104},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rosasco et al. - 2004 - Are loss functions all the same.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Learning,Learning: physiology,Linear Models,Models, Neurological,Statistics as Topic},
month = may,
number = {5},
pages = {1063--76},
pmid = {15070510},
title = {{Are loss functions all the same?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15070510},
volume = {16},
year = {2004}
}
@inproceedings{Duin2010,
author = {Duin, Robert P.W. and Loog, Marco and Pȩkalska, Elzbieta and Tax, David M.J.},
booktitle = {Proceedings of the 20th International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Duin et al. - 2010 - Feature-based dissimilarity space classification.pdf:pdf},
pages = {46--55},
title = {{Feature-based dissimilarity space classification}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-17711-8\_5},
year = {2010}
}
@book{Kawakita2014,
author = {Kawakita, Masanori and Takeuchi, Jun’ichi},
booktitle = {Neural Networks},
doi = {10.1016/j.neunet.2014.01.016},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kawakita, Takeuchi - 2014 - Safe semi-supervised learning based on weighted likelihood.pdf:pdf},
isbn = {8192802361},
issn = {08936080},
month = jan,
publisher = {Elsevier Ltd},
title = {{Safe semi-supervised learning based on weighted likelihood}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0893608014000288},
year = {2014}
}
@inproceedings{Cortes2011,
author = {Cortes, Corinna and Mohri, Mehryar},
booktitle = {Algorithmic Learning Theory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cortes, Mohri - 2011 - Domain adaptation in regression.pdf:pdf},
title = {{Domain adaptation in regression}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-24412-4\_25},
year = {2011}
}
@article{Wainwright2008,
author = {Wainwright, Martin J. and Jordan, Michael I.},
doi = {10.1561/2200000001},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wainwright, Jordan - 2008 - Graphical models, exponential families, and variational inference.pdf:pdf},
journal = {Foundations and Trends in Machine Learning},
pages = {1--305},
title = {{Graphical models, exponential families, and variational inference}},
url = {http://dl.acm.org/citation.cfm?id=1498841},
volume = {1},
year = {2008}
}
@article{Lampert2014,
author = {Lampert, Thomas a. and Gan\c{c}arski, Pierre},
doi = {10.1007/s10994-013-5432-x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lampert, Gan\c{c}arski - 2014 - The bane of skew.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {aucpr,evaluation,performance,precision,recall,roc,skew},
month = jan,
title = {{The bane of skew}},
url = {http://link.springer.com/10.1007/s10994-013-5432-x},
year = {2014}
}
@article{Soares2004,
author = {Soares, Carlos and Brazdil, Pavel B. and Kuba, Petr},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Soares, Brazdil, Kuba - 2004 - A Meta-Learning Method to Select the KernelWidth in Support Vector Regression.pdf:pdf},
journal = {Machine learning},
keywords = {gaussian kernel,learning rankings,meta-learning,parameter setting,support vector machines},
pages = {195--209},
title = {{A Meta-Learning Method to Select the KernelWidth in Support Vector Regression}},
url = {http://link.springer.com/article/10.1023/b:mach.0000015879.28004.9b},
volume = {54},
year = {2004}
}
@misc{Kleijn2012,
author = {Kleijn, Bas and Vaart, Aad Van Der and Zanten, Harry Van},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kleijn, Vaart, Zanten - 2012 - Nonparametric Bayesian Statistics - Intro Bayesian inference.pdf:pdf},
number = {September},
title = {{Nonparametric Bayesian Statistics - Intro Bayesian inference}},
year = {2012}
}
@article{Kullback1968,
author = {Kullback, S.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kullback - 1968 - Probability Densities with Given Marginals.pdf:pdf},
journal = {The Annals of Mathematical Statistics},
number = {4},
pages = {1236--1243},
title = {{Probability Densities with Given Marginals}},
url = {http://www.jstor.org/stable/10.2307/2239692},
volume = {39},
year = {1968}
}
@article{Erren2007,
author = {Erren, Thomas C and Bourne, Philip E},
doi = {10.1371/journal.pcbi.0030102},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Erren, Bourne - 2007 - Ten simple rules for a good poster presentation.pdf:pdf},
issn = {1553-7358},
journal = {PLoS computational biology},
keywords = {Algorithms,Audiovisual Aids,Biomedical Research,Communication,Congresses as Topic,Exhibits as Topic,Information Dissemination,Information Dissemination: methods,Professional Competence},
month = may,
number = {5},
pages = {e102},
pmid = {17530921},
title = {{Ten simple rules for a good poster presentation.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1876493\&tool=pmcentrez\&rendertype=abstract},
volume = {3},
year = {2007}
}
@article{Grasse1997,
author = {Bar-On, J.R. and Grasse, K. A.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bar-On, Grasse - 1997 - Global Optimization of a Quadratic Functional with Quadratic Equality Constraints , Part 2 1.pdf:pdf},
journal = {Journal of Optimization Theory and Applications},
keywords = {global optimization,quadratic equality constraints,quadratic functionals},
number = {3},
pages = {547--556},
title = {{Global Optimization of a Quadratic Functional with Quadratic Equality Constraints , Part 2 1}},
volume = {93},
year = {1997}
}
@inproceedings{Joulin2012,
author = {Joulin, Armand and Bach, Francis},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Joulin, Bach - 2012 - A convex relaxation for weakly supervised classifiers.pdf:pdf},
keywords = {MIL,convex relaxation,weak supervision},
pages = {1279--1286},
title = {{A convex relaxation for weakly supervised classifiers}},
url = {http://arxiv.org/abs/1206.6413},
year = {2012}
}
@article{Rendell1990,
author = {Rendell, Larry and Cho, Howard},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rendell, Cho - 1990 - Empirical learning as a function of concept character.pdf:pdf},
journal = {Machine Learning},
keywords = {concepts as functions,empirical concept learning,experimental studies},
pages = {267--298},
title = {{Empirical learning as a function of concept character}},
url = {http://www.springerlink.com/index/K5311727465WLH07.pdf},
volume = {5},
year = {1990}
}
@article{Hsu2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1106.2363v1},
author = {Hsu, Daniel and Kakade, Sham M. and Zhang, Tong},
eprint = {arXiv:1106.2363v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hsu, Kakade, Zhang - 2011 - An analysis of random design linear regression.pdf:pdf},
journal = {arXiv preprint},
title = {{An analysis of random design linear regression}},
url = {http://arxiv.org/abs/1106.2363},
year = {2011}
}
@book{Wu2007,
author = {Wu, Xindong and Kumar, Vipin and {Ross Quinlan}, J. and Ghosh, Joydeep and Yang, Qiang and Motoda, Hiroshi and McLachlan, Geoffrey J. and Ng, Angus and Liu, Bing and Yu, Philip S. and Zhou, Zhi-Hua and Steinbach, Michael and Hand, David J. and Steinberg, Dan},
booktitle = {Knowledge and Information Systems},
doi = {10.1007/s10115-007-0114-2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wu et al. - 2007 - Top 10 algorithms in data mining.pdf:pdf},
isbn = {1011500701},
issn = {0219-1377},
month = dec,
number = {1},
pages = {1--37},
title = {{Top 10 algorithms in data mining}},
url = {http://www.springerlink.com/index/10.1007/s10115-007-0114-2},
volume = {14},
year = {2007}
}
@article{Zhao2013a,
author = {Zhao, Junlong and Leng, Chenlei and Li, Lexin and Wang, Hansheng},
doi = {10.1214/13-AOS1165},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhao et al. - 2013 - High-dimensional influence measure.pdf:pdf},
journal = {The Annals of Statistics},
number = {5},
pages = {2639--2667},
title = {{High-dimensional influence measure}},
url = {http://projecteuclid.org/euclid.aos/1384871348},
volume = {41},
year = {2013}
}
@article{Wang2007a,
author = {Wang, Junhui and Shen, Xiaotong and Pan, Wei},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Shen, Pan - 2007 - On transductive support vector machines.pdf:pdf},
journal = {Contemporary Mathematics},
pages = {7--19},
title = {{On transductive support vector machines}},
url = {http://www.stat.umn.edu/~xshen/paper/tsvm.pdf http://books.google.com/books?hl=en\&lr=\&id=hXi\_rG3NELsC\&oi=fnd\&pg=PA7\&dq=On+transductive+support+vector+machines\&ots=GZGFVmVuKj\&sig=MgTezchQc8K2plQ7hXlKNmr4Itw},
volume = {443},
year = {2007}
}
@article{Sohn1999,
author = {Sohn, So Young},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sohn - 1999 - Meta Analysis of Classification Algorithms for Pattern Recognition.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {11},
pages = {1137--1144},
title = {{Meta Analysis of Classification Algorithms for Pattern Recognition}},
volume = {21},
year = {1999}
}
@article{Bartlett2006,
author = {Bartlett, Peter L and Jordan, Michael I. and McAuliffe, Jon D},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bartlett, Jordan, McAuliffe - 2006 - Convexity, Classification, and Risk Bounds.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {boosting,convex optimization},
month = mar,
number = {473},
pages = {138--156},
title = {{Convexity, Classification, and Risk Bounds}},
volume = {101},
year = {2006}
}
@inproceedings{Skurichina2002,
author = {Skurichina, Marina and Kuncheva, Ludmila I and Duin, Robert P.W.},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Skurichina, Kuncheva, Duin - 2002 - Bagging and Boosting for the Nearest Mean Classifier Effects of Sample Size on Diversity and Accura.pdf:pdf},
pages = {62--71},
publisher = {Springer},
title = {{Bagging and Boosting for the Nearest Mean Classifier : Effects of Sample Size on Diversity and Accuracy}},
year = {2002}
}
@inproceedings{Hernandez-reyes2005,
author = {Hern\'{a}ndez-reyes, Edith and Carrasco-Ochoa, J.A. and Mart\'{\i}nez-trinidad, J Fco},
booktitle = {Proceedings of the 10th Iberoamerican Congress on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hern\'{a}ndez-reyes, Carrasco-Ochoa, Mart\'{\i}nez-trinidad - 2005 - Classifier Selection Based on Data Complexity Measures.pdf:pdf},
pages = {586--592},
title = {{Classifier Selection Based on Data Complexity Measures}},
year = {2005}
}
@inproceedings{Ben-David2008,
author = {Ben-David, Shai and Lu, Tyler and P\'{a}l, David},
booktitle = {Proceedings of the 21st Annual Conference on Learning Theory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-David, Lu, P\'{a}l - 2008 - Does Unlabeled Data Provably Help Worst-case Analysis of the Sample Complexity of Semi-Supervised Learning.pdf:pdf},
pages = {33--44},
title = {{Does Unlabeled Data Provably Help? Worst-case Analysis of the Sample Complexity of Semi-Supervised Learning.}},
url = {http://www.cs.toronto.edu/~tl/papers/ssl.pdf},
year = {2008}
}
@article{Zhang2013a,
author = {Zhang, Q. and Qian, P. Z. G.},
doi = {10.1093/biomet/ast034},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang, Qian - 2013 - Designs for crossvalidating approximation models.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = sep,
number = {4},
pages = {997--1004},
title = {{Designs for crossvalidating approximation models}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/ast034},
volume = {100},
year = {2013}
}
@inproceedings{Goldberg2007,
author = {Goldberg, Andrew B. and Zhu, Xiaojin and Wright, Stephen},
booktitle = {International Conference on Artificial Intelligence and Statistics},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Goldberg, Zhu, Wright - 2007 - Dissimilarity in graph-based semi-supervised classification.pdf:pdf},
number = {1},
pages = {55--162},
title = {{Dissimilarity in graph-based semi-supervised classification}},
url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/AISTATS07\_GoldbergZW.pdf},
year = {2007}
}
@inproceedings{Ho2001a,
author = {Ho, Tin Kam},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho - 2001 - Data Complexity Analysis for Classifier Combination(2).pdf:pdf},
pages = {53--67},
title = {{Data Complexity Analysis for Classifier Combination}},
year = {2001}
}
@article{Belkin2006,
author = {Belkin, Mikhail and Niyogi, Partha and Sindhwani, Vikas},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Belkin, Niyogi, Sindhwani - 2006 - Manifold regularization A geometric framework for learning from labeled and unlabeled examples.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {fold learning,graph transduction,kernel methods,mani-,regularization,semi-supervised learning,spectral graph theory,support vector machines,unlabeled data},
pages = {2399--2434},
title = {{Manifold regularization: A geometric framework for learning from labeled and unlabeled examples}},
url = {http://dl.acm.org/citation.cfm?id=1248632},
volume = {7},
year = {2006}
}
@article{Wang2009,
author = {Wang, Xiaozhe and Smith-Miles, Kate and Hyndman, Rob},
doi = {10.1016/j.neucom.2008.10.017},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Smith-Miles, Hyndman - 2009 - Rule induction for forecasting method selection Meta-learning the characteristics of univariate time series.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
month = jun,
number = {10-12},
pages = {2581--2594},
title = {{Rule induction for forecasting method selection: Meta-learning the characteristics of univariate time series}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231208005134},
volume = {72},
year = {2009}
}
@article{Shore1980,
author = {Shore, John E. and Johnson, Rodney W.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shore, Johnson - 1980 - Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
number = {1},
pages = {26--37},
title = {{Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1056144},
volume = {26},
year = {1980}
}
@article{Halkidi2001,
author = {Halkidi, Maria},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Halkidi - 2001 - On Clustering Validation Techniques.pdf:pdf},
keywords = {cluster validity,clustering algorithms,unsupervised learning,validity indices},
pages = {107--145},
title = {{On Clustering Validation Techniques}},
year = {2001}
}
@article{Smith-Miles2008,
author = {Smith-Miles, Kate},
doi = {10.1145/1456650.1456656},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Smith-Miles - 2008 - Cross-disciplinary perspectives on meta-learning for algorithm selection.pdf:pdf},
issn = {03600300},
journal = {ACM Computing Surveys},
month = dec,
number = {1},
pages = {1--25},
title = {{Cross-disciplinary perspectives on meta-learning for algorithm selection}},
url = {http://portal.acm.org/citation.cfm?doid=1456650.1456656},
volume = {41},
year = {2008}
}
@article{Sugiyama2009,
author = {Sugiyama, Masashi and Id\'{e}, Tsuyoshi and Nakajima, Shinichi and Sese, Jun},
doi = {10.1007/s10994-009-5125-7},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sugiyama et al. - 2009 - Semi-supervised local Fisher discriminant analysis for dimensionality reduction.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = jul,
number = {1-2},
pages = {35--61},
title = {{Semi-supervised local Fisher discriminant analysis for dimensionality reduction}},
url = {http://www.springerlink.com/index/10.1007/s10994-009-5125-7},
volume = {78},
year = {2009}
}
@article{Liang2007,
archivePrefix = {arXiv},
arxivId = {arXiv:0710.4618v1},
author = {Liang, Feng and Mukherjee, Sayan and West, Mike},
doi = {10.1214/088342307000000032},
eprint = {arXiv:0710.4618v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Liang, Mukherjee, West - 2007 - The Use of Unlabeled Data in Predictive Modeling.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {and phrases,bayesian analysis,bayesian kernel regression,latent factor models,mixture models,pervised learning,predictive distribution,semisu-,unlabeled data},
month = may,
number = {2},
pages = {189--205},
title = {{The Use of Unlabeled Data in Predictive Modeling}},
url = {http://projecteuclid.org/euclid.ss/1190905518},
volume = {22},
year = {2007}
}
@article{Macia2013,
author = {Maci\`{a}, N\'{u}ria and Bernad\'{o}-Mansilla, Ester},
doi = {10.1016/j.ins.2013.08.059},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Maci\`{a}, Bernad\'{o}-Mansilla - 2013 - Towards UCI A mindful repository design.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
month = sep,
title = {{Towards UCI+: A mindful repository design}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0020025513006336},
year = {2013}
}
@article{Buhmann2010,
archivePrefix = {arXiv},
arxivId = {arXiv:1006.0375v1},
author = {Buhmann, Joachim M},
eprint = {arXiv:1006.0375v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Buhmann - 2010 - Information theoretic model validation for clustering.pdf:pdf},
number = {X},
title = {{Information theoretic model validation for clustering}},
volume = {2010},
year = {2010}
}
@inproceedings{Widrow1960,
author = {Widrow, Bernard and Hoff, Marcian E.},
booktitle = {IRE WESCON Convention Record 4},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Widrow, Hoff - 1960 - Adaptive switching circuits.pdf:pdf},
pages = {96--104},
title = {{Adaptive switching circuits.}},
url = {http://www-isl.stanford.edu/people/widrow/papers/c1960adaptiveswitching.pdf},
year = {1960}
}
@inproceedings{Ho1996,
author = {Ho, Tin Kam and Kleinberg, Eeugene M.},
booktitle = {International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho, Kleinberg - 1996 - Building projectable classifiers of arbitrary complexity.pdf:pdf},
pages = {880--885},
title = {{Building projectable classifiers of arbitrary complexity}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=547202},
volume = {2},
year = {1996}
}
@article{Arnold2000,
author = {Arnold, Bernard F. and Stahlecker, Peter},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Arnold, Stahlecker - 2000 - The minimax adjustment principle.pdf:pdf},
journal = {Mathematical methods of operations research},
keywords = {ellipsoidal information,minimax,minimax adjustment principle,principle,projection estimator,supply policy},
pages = {103--113},
title = {{The minimax adjustment principle}},
url = {http://link.springer.com/article/10.1007/s001860050005},
volume = {51},
year = {2000}
}
@article{Gelman2013c,
author = {Gelman, Andrew and Betancourt, Michael},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Betancourt - 2013 - Does quantum uncertainty have a place in everyday applied statistics.pdf:pdf},
journal = {The Behavioral and brain sciences},
number = {August},
title = {{Does quantum uncertainty have a place in everyday applied statistics?}},
url = {http://www.stat.columbia.edu/~gelman/research/published/quantum.pdf},
year = {2013}
}
@article{O'Neill1978,
author = {O'Neill, Terence J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/O'Neill - 1978 - Normal discrimination with unclassified observations.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {unclassified},
number = {364},
pages = {821--826},
title = {{Normal discrimination with unclassified observations}},
url = {http://amstat.tandfonline.com/doi/full/10.1080/01621459.1978.10480106},
volume = {73},
year = {1978}
}
@article{Ye2007a,
address = {New York, New York, USA},
author = {Ye, Jieping},
doi = {10.1145/1273496.1273633},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ye - 2007 - Least squares linear discriminant analysis.pdf:pdf},
isbn = {9781595937933},
journal = {Proceedings of the 24th International Conference on Machine Learning},
keywords = {18,3,8,are linear combinations of,class separability,derived features in lda,dimension reduction,least squares,linear discriminant anal-,linear regression,the,the data achieves maximum,the orig-,ysis},
pages = {1087--1093},
publisher = {ACM Press},
title = {{Least squares linear discriminant analysis}},
url = {http://portal.acm.org/citation.cfm?doid=1273496.1273633},
year = {2007}
}
@article{Rasmussen2005,
author = {Rasmussen, Carl Edward and Williams, Christopher K I},
publisher = {MIT Press},
title = {{Gaussian Processes for Machine Learning}},
year = {2005}
}
@article{Claassen2005,
author = {Claassen, Tom and Heskes, Tom},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Claassen, Heskes - 2005 - A Logical Characterization of Constraint-Based Causal Discovery.pdf:pdf},
title = {{A Logical Characterization of Constraint-Based Causal Discovery}},
year = {2005}
}
@article{Mann2010,
author = {Mann, Gideon S. and McCallum, Andrew Kachites},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mann, McCallum - 2010 - Generalized expectation criteria for semi-supervised learning with weakly labeled data.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {condi-,generalized expectation criteria,logistic regression,semi-supervised learning,tional random fields},
pages = {955--984},
title = {{Generalized expectation criteria for semi-supervised learning with weakly labeled data}},
url = {http://dl.acm.org/citation.cfm?id=1756038},
volume = {11},
year = {2010}
}
@inproceedings{Ratsaby1995,
author = {Ratsaby, Joel and Venkatesht, Santosh S.},
booktitle = {Proceedings of the 8th Annual conference on Computational learning theory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ratsaby, Venkatesht - 1995 - Learning from a mixture of labeled and unlabeled examples with parametric side information.pdf:pdf},
pages = {412--417},
title = {{Learning from a mixture of labeled and unlabeled examples with parametric side information}},
url = {http://dl.acm.org/citation.cfm?id=225348},
year = {1995}
}
@article{Kleinberg2000,
author = {Kleinberg, E.M.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kleinberg - 2000 - On the algorithmic implementation of stochastic discrimination.pdf:pdf},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {5},
pages = {473--490},
publisher = {IEEE},
title = {{On the algorithmic implementation of stochastic discrimination}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=857004},
volume = {22},
year = {2000}
}
@article{Jain1999a,
author = {Jain, A.K. and Murty, M.N. and Flynn, P.J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jain, Murty, Flynn - 1999 - Data clustering a review.pdf:pdf},
journal = {ACM computing surveys (CSUR)},
number = {3},
title = {{Data clustering: a review}},
url = {http://dl.acm.org/citation.cfm?id=331504},
volume = {31},
year = {1999}
}
@phdthesis{Hillebrand2012,
author = {Hillebrand, Arne},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hillebrand - 2012 - Separating a polygonal environment into a multi-layered environment.pdf:pdf},
keywords = {branch,explicit corridor map,ge-,graphs,local search,multi-layered environment,multicut,netic algorithm,price},
school = {Utrecht University},
title = {{Separating a polygonal environment into a multi-layered environment}},
year = {2012}
}
@article{Braga-Neto2004,
abstract = {MOTIVATION: Microarray classification typically possesses two striking attributes: (1) classifier design and error estimation are based on remarkably small samples and (2) cross-validation error estimation is employed in the majority of the papers. Thus, it is necessary to have a quantifiable understanding of the behavior of cross-validation in the context of very small samples. RESULTS: An extensive simulation study has been performed comparing cross-validation, resubstitution and bootstrap estimation for three popular classification rules-linear discriminant analysis, 3-nearest-neighbor and decision trees (CART)-using both synthetic and real breast-cancer patient data. Comparison is via the distribution of differences between the estimated and true errors. Various statistics for the deviation distribution have been computed: mean (for estimator bias), variance (for estimator precision), root-mean square error (for composition of bias and variance) and quartile ranges, including outlier behavior. In general, while cross-validation error estimation is much less biased than resubstitution, it displays excessive variance, which makes individual estimates unreliable for small samples. Bootstrap methods provide improved performance relative to variance, but at a high computational cost and often with increased bias (albeit, much less than with resubstitution).},
author = {Braga-Neto, Ulisses M and Dougherty, Edward R.},
doi = {10.1093/bioinformatics/btg419},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Braga-Neto, Dougherty - 2004 - Is cross-validation valid for small-sample microarray classification.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Benchmarking,Benchmarking: methods,Breast Neoplasms,Breast Neoplasms: diagnosis,Breast Neoplasms: genetics,Computer Simulation,Gene Expression Profiling,Gene Expression Profiling: methods,Genetic Predisposition to Disease,Genetic Predisposition to Disease: genetics,Genetic Testing,Genetic Testing: methods,Humans,Models, Genetic,Models, Statistical,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Pattern Recognition, Automated,Reproducibility of Results,Sample Size,Sensitivity and Specificity},
month = feb,
number = {3},
pages = {374--80},
pmid = {14960464},
title = {{Is cross-validation valid for small-sample microarray classification?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/14960464},
volume = {20},
year = {2004}
}
@article{Gelly2007,
author = {Gelly, Sylvain and Kocsis, Levente and Schoenauer, Marc and Sebag, Michele and Silver, David and Szepesvari, Csaba and Teytaud, Olivier},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelly et al. - 2007 - The Grand Challenge of Computer Go Monte Carlo Tree Search and Extensions.pdf:pdf},
title = {{The Grand Challenge of Computer Go : Monte Carlo Tree Search and Extensions}},
year = {2007}
}
@inproceedings{VanderMaaten2013,
author = {{Van der Maaten}, Laurens and Chen, Minmin and Tyree, Stephen and Weinberger, Kilian Q.},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Van der Maaten et al. - 2013 - Learning with Marginalized Corrupted Features.pdf:pdf},
pages = {410--418},
title = {{Learning with Marginalized Corrupted Features}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/ICML2013\_vandermaaten13},
year = {2013}
}
@article{Bottou2012,
author = {Bottou, L\'{e}on},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bottou - 2012 - Stochastic Gradient Descent Tricks.pdf:pdf},
journal = {Neural Networks: Tricks of the Trade},
number = {1},
pages = {1--16},
title = {{Stochastic Gradient Descent Tricks}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-35289-8\_25},
year = {2012}
}
@phdthesis{Colas,
author = {Colas, Fabrice P. R.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Colas - 2009 - Data Mining Scenarios.pdf:pdf},
isbn = {9789090238883},
title = {{Data Mining Scenarios}},
year = {2009}
}
@article{Zhou2005,
author = {Zhou, Zhi-hua and Li, Ming},
doi = {10.1109/TKDE.2005.186},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Li - 2005 - Tri-training exploiting unlabeled data using three classifiers.pdf:pdf},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
month = nov,
number = {11},
pages = {1529--1541},
title = {{Tri-training: exploiting unlabeled data using three classifiers}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1512038},
volume = {17},
year = {2005}
}
@article{Devroye1982,
abstract = {Consider the basic discrimination problem based on a sample of size n drawn from the distribution of (X, Y) on the Borel sets of Rdx \{O, 1\}. If 0 < R*< is a given number, and 'n - 0 is an arbitrary positive sequence, then for any discrimination rule one can find a distribution for (X, Y), not depending upon n, with Bayes probability of error R* such that the probability of error (Rn) of the discrimination rule is larger than R* + 'On for infinitely many n. We give a formal proof of this result, which is a generalization of a result by Cover [1].},
author = {Devroye, L},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Devroye - 1982 - Any discrimination rule can have an arbitrarily bad probability of error for finite sample size.pdf:pdf},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = feb,
number = {2},
pages = {154--7},
pmid = {21869021},
title = {{Any discrimination rule can have an arbitrarily bad probability of error for finite sample size.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21869021},
volume = {4},
year = {1982}
}
@inproceedings{Roli2002,
author = {Roli, Fabio and Raudys, \v{S}arūnas and Marcialis, Gian Luca},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Roli, Raudys, Marcialis - 2002 - An experimental comparison of fixed and trained fusion rules for crisp classifier outputs.pdf:pdf},
title = {{An experimental comparison of fixed and trained fusion rules for crisp classifier outputs}},
url = {http://link.springer.com/chapter/10.1007/3-540-45428-4\_23},
year = {2002}
}
@article{Shaffer1991,
author = {Shaffer, Juliet Popper},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shaffer - 1991 - The Gauss-Markov Theorem and Random Regressors.pdf:pdf},
journal = {The American Statistician},
keywords = {best linear unbiased estimators,finite-,linear regression,population sampling,unbiased esti-},
number = {4},
pages = {269--273},
title = {{The Gauss-Markov Theorem and Random Regressors}},
volume = {45},
year = {1991}
}
@phdthesis{Macia2011,
author = {Macia, Nuria},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Macia - 2011 - Data Complexity in Supervised Learning A Far-Reaching Implication.pdf:pdf},
title = {{Data Complexity in Supervised Learning: A Far-Reaching Implication}},
year = {2011}
}
@article{Culp2008b,
abstract = {Graph-based learning provides a useful approach for modeling data in classification problems. In this modeling scenario, the relationship between labeled and unlabeled data impacts the construction and performance of classifiers, and therefore a semi-supervised learning framework is adopted. We propose a graph classifier based on kernel smoothing. A regularization framework is also introduced, and it is shown that the proposed classifier optimizes certain loss functions. Its performance is assessed on several synthetic and real benchmark data sets with good results, especially in settings where only a small fraction of the data are labeled.},
author = {Culp, Mark and Michailidis, George},
doi = {10.1109/TPAMI.2007.70765},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Culp, Michailidis - 2008 - Graph-based semisupervised learning.pdf:pdf},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Cluster Analysis,Computer Simulation,Data Interpretation, Statistical,Models, Statistical,Pattern Recognition, Automated,Pattern Recognition, Automated: methods},
month = jan,
number = {1},
pages = {174--9},
pmid = {18000333},
title = {{Graph-based semisupervised learning.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18000333},
volume = {30},
year = {2008}
}
@article{Hand2014,
author = {Hand, David J.},
doi = {10.1214/13-STS446},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hand - 2014 - Wonderful Examples, but Let’s not Close Our Eyes.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Frequentist, likelihood inference, Neyman-Pearson ,and phrases,frequentist,hypothesis testing,informative and thought-provoking,likelihood inference,making specific comments,neyman,on each of these,pearson,schools of inference,space prohibits me from},
month = feb,
number = {1},
pages = {98--100},
title = {{Wonderful Examples, but Let’s not Close Our Eyes}},
url = {http://projecteuclid.org/euclid.ss/1399645735},
volume = {29},
year = {2014}
}
@misc{Mitchell1980,
author = {Mitchell, Tom M.},
booktitle = {Psychology},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mitchell - 1980 - The need for biases in learning generalizations.pdf:pdf},
title = {{The need for biases in learning generalizations}},
url = {http://dml.cs.byu.edu/~cgc/docs/mldm\_tools/Reading/Need for Bias.pdf},
year = {1980}
}
@inproceedings{Shaffer1994,
author = {Schaffer, Cullen},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schaffer - 1994 - A conservation law for generalization performance.pdf:pdf},
title = {{A conservation law for generalization performance}},
url = {http://dml.cs.byu.edu/~cgc/docs/mldm\_tools/Reading/LCG.pdf},
year = {1994}
}
@article{Chandrasekaran2013a,
archivePrefix = {arXiv},
arxivId = {arXiv:1211.1073v2},
author = {Chandrasekaran, Venkat and Jordan, Michael I.},
eprint = {arXiv:1211.1073v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chandrasekaran, Jordan - 2013 - Computational and statistical tradeoffs via convex relaxation(2).pdf:pdf},
journal = {Proceedings of the National Academy of Sciences},
keywords = {convex geometry,convex relaxation,high-dimensional statistics,massive datasets},
number = {13},
pages = {1181--1190},
title = {{Computational and statistical tradeoffs via convex relaxation}},
url = {http://www.pnas.org/content/110/13/E1181.short},
volume = {110},
year = {2013}
}
@article{Finucane2014,
author = {Finucane, Mariel M. and Paciorek, Christopher J. and Danaei, Goodarz and Ezzati, Majid},
doi = {10.1214/13-STS427},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Finucane et al. - 2014 - Bayesian Estimation of Population-Level Trends in Measures of Health Status.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Bayesian inference, hierarchical models, combining,and phrases},
month = feb,
number = {1},
pages = {18--25},
title = {{Bayesian Estimation of Population-Level Trends in Measures of Health Status}},
url = {http://projecteuclid.org/euclid.ss/1399645724},
volume = {29},
year = {2014}
}
@article{Ioannidis2005,
abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
author = {Ioannidis, John P.A.},
doi = {10.1371/journal.pmed.0020124},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ioannidis - 2005 - Why most published research findings are false.pdf:pdf},
issn = {1549-1676},
journal = {PLoS medicine},
keywords = {Bias (Epidemiology),Data Interpretation,Likelihood Functions,Meta-Analysis as Topic,Odds Ratio,Publishing,Reproducibility of Results,Research Design,Sample Size,Statistical},
month = aug,
number = {8},
pages = {e124},
pmid = {16060722},
title = {{Why most published research findings are false.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1182327\&tool=pmcentrez\&rendertype=abstract},
volume = {2},
year = {2005}
}
@article{Chapelle2007,
abstract = {Most literature on support vector machines (SVMs) concentrates on the dual optimization problem. In this letter, we point out that the primal problem can also be solved efficiently for both linear and nonlinear SVMs and that there is no reason for ignoring this possibility. On the contrary, from the primal point of view, new families of algorithms for large-scale SVM training can be investigated.},
author = {Chapelle, Olivier},
doi = {10.1162/neco.2007.19.5.1155},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chapelle - 2007 - Training a support vector machine in the primal.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Models, Theoretical,Neural Networks (Computer),Nonlinear Dynamics,Pattern Recognition, Automated},
month = may,
number = {5},
pages = {1155--78},
pmid = {17381263},
title = {{Training a support vector machine in the primal.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17381263},
volume = {19},
year = {2007}
}
@article{Morrison2011,
abstract = {A growing body of literature shows that one's working memory (WM) capacity can be expanded through targeted training. Given the established relationship between WM and higher cognition, these successful training studies have led to speculation that WM training may yield broad cognitive benefits. This review considers the current state of the emerging WM training literature, and details both its successes and limitations. We identify two distinct approaches to WM training, strategy training and core training, and highlight both the theoretical and practical motivations that guide each approach. Training-related increases in WM capacity have been successfully demonstrated across a wide range of subject populations, but different training techniques seem to produce differential impacts upon the broader landscape of cognitive abilities. In particular, core WM training studies seem to produce more far-reaching transfer effects, likely because they target domain-general mechanisms of WM. The results of individual studies encourage optimism regarding the value of WM training as a tool for general cognitive enhancement. However, we discuss several limitations that should be addressed before the field endorses the value of this approach.},
author = {Morrison, Alexandra B and Chein, Jason M},
doi = {10.3758/s13423-010-0034-0},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Morrison, Chein - 2011 - Does working memory training work The promise and challenges of enhancing cognition by training working memory.pdf:pdf},
issn = {1531-5320},
journal = {Psychonomic bulletin \& review},
keywords = {Adult,Aged,Attention,Child,Cognition,Humans,Intelligence,Judgment,Memory Disorders,Memory Disorders: therapy,Memory, Short-Term,Middle Aged,Pattern Recognition, Visual,Practice (Psychology),Serial Learning,Transfer (Psychology),Treatment Outcome,Verbal Learning,Young Adult},
month = feb,
number = {1},
pages = {46--60},
pmid = {21327348},
title = {{Does working memory training work? The promise and challenges of enhancing cognition by training working memory.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21327348},
volume = {18},
year = {2011}
}
@article{Bickel2009,
author = {Bickel, Steffen and Br\"{u}ckner, Michael and Scheffer, Tobias},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bickel, Br\"{u}ckner, Scheffer - 2009 - Discriminative learning under covariate shift.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {covariate shift,discriminative learning,transfer learning},
pages = {2137--2155},
title = {{Discriminative learning under covariate shift}},
url = {http://dl.acm.org/citation.cfm?id=1755858},
volume = {10},
year = {2009}
}
@inproceedings{Cozman2002,
author = {Cozman, FG and Cohen, Ira},
booktitle = {FLAIRS Conference},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cozman, Cohen - 2002 - Unlabeled Data Can Degrade Classification Performance of Generative Classifiers.pdf:pdf},
pages = {327--331},
title = {{Unlabeled Data Can Degrade Classification Performance of Generative Classifiers.}},
url = {http://www.aaai.org/Papers/FLAIRS/2002/FLAIRS02-065.pdf},
year = {2002}
}
@inproceedings{Weiss2008,
author = {Weiss, Yair and Torralba, Antonio and Fergus, Rob},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Weiss, Torralba, Fergus - 2008 - Spectral Hashing.pdf:pdf},
pages = {1753--1760},
title = {{Spectral Hashing.}},
url = {https://papers.nips.cc/paper/3383-spectral-hashing.pdf},
year = {2008}
}
@article{Nguyen2010a,
author = {Nguyen, XuanLong and Wainwright, Martin J. and Jordan, Michael I.},
doi = {10.1109/TIT.2010.2068870},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nguyen, Wainwright, Jordan - 2010 - Estimating Divergence Functionals and the Likelihood Ratio by Convex Risk Minimization(2).pdf:pdf},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
month = nov,
number = {11},
pages = {5847--5861},
title = {{Estimating Divergence Functionals and the Likelihood Ratio by Convex Risk Minimization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5605355},
volume = {56},
year = {2010}
}
@inproceedings{Druck2007,
author = {Druck, Gregory and Pal, Chris and McCallum, Andrew Kachites and Zhu, Xiaojin},
booktitle = {Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Druck et al. - 2007 - Semi-supervised classification with hybrid generativediscriminative methods.pdf:pdf},
isbn = {9781595936097},
keywords = {discriminative,hybrid generative,methods,semi-supervised learning,text classification},
pages = {280--289},
title = {{Semi-supervised classification with hybrid generative/discriminative methods}},
url = {http://dl.acm.org/citation.cfm?id=1281225},
year = {2007}
}
@inproceedings{Jaakkola2002,
author = {Jaakkola, MST and Szummer, Martin},
booktitle = {Advances in Neural Information Processing Systems 14},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jaakkola, Szummer - 2002 - Partially labeled classification with Markov random walks.pdf:pdf},
pages = {945--952},
title = {{Partially labeled classification with Markov random walks}},
url = {http://books.google.com/books?hl=en\&lr=\&id=GbC8cqxGR7YC\&oi=fnd\&pg=PA945\&dq=Partially+labeled+classification+with+Markov+random+walks\&ots=ZvP5J\_YBx6\&sig=dk27TWzUdp9G-e9OyvfYcGR14ro},
year = {2002}
}
@article{Wasserman,
author = {Wasserman, Larry},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wasserman - Unknown - Rise of the machines.pdf:pdf},
issn = {1743-9159},
title = {{Rise of the machines.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22226582}
}
@inproceedings{Foulds2011,
author = {Foulds, James and Smyth, Padhraic},
booktitle = {SIAM International Conference on Data Mining},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Foulds, Smyth - 2011 - Multi-instance mixture models and semi-supervised learning.pdf:pdf},
number = {Mi},
title = {{Multi-instance mixture models and semi-supervised learning}},
url = {http://siam.omnibooksonline.com/2011datamining/data/papers/256.pdf},
year = {2011}
}
@article{Reid2011,
author = {Reid, Mark D. and Williamson, Robert C.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Reid, Williamson - 2011 - Information, divergence and risk for binary experiments.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {classification,divergence,loss functions,regret bounds,statistical information},
pages = {731--817},
title = {{Information, divergence and risk for binary experiments}},
url = {http://dl.acm.org/citation.cfm?id=2021029},
volume = {12},
year = {2011}
}
@inproceedings{Huang2006,
author = {Huang, Jiayuan and Smola, Alex and Gretton, Arthur and Borgwardt, Karsten M. and Sch\"{o}lkopf, Bernhard},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huang et al. - 2006 - Correcting sample selection bias by unlabeled data.pdf:pdf},
pages = {601--608},
title = {{Correcting sample selection bias by unlabeled data}},
url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/NIPS2006\_915.pdf},
year = {2006}
}
@inproceedings{Duch2012,
author = {Duch, Włodzisław and Jankowski, Norbert and Maszczyk, Tomasz},
booktitle = {International Joint Conference on Neural Networks},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Duch, Jankowski, Maszczyk - 2012 - Make it cheap learning with O (nd) complexity.pdf:pdf},
number = {2},
title = {{Make it cheap: learning with O (nd) complexity}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6252380},
year = {2012}
}
@article{Zhang2013,
author = {Zhang, Li},
doi = {10.1214/13-AOS1141},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang - 2013 - Nearly optimal minimax estimator for high-dimensional sparse linear regression.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {62J05, 62G20, 62C20, Minimax estimation, linear re},
month = aug,
number = {4},
pages = {2149--2175},
title = {{Nearly optimal minimax estimator for high-dimensional sparse linear regression}},
url = {http://projecteuclid.org/euclid.aos/1382547516},
volume = {41},
year = {2013}
}
@misc{Bache2013,
author = {Bache, K. and Lichman, M.},
publisher = {University of California, Irvine, School of Information and Computer Sciences},
title = {{\{UCI\} Machine Learning Repository}},
url = {http://archive.ics.uci.edu/ml},
year = {2013}
}
@article{Kanamori2012,
author = {Kanamori, Takafumi and Takeda, A and Suzuki, T},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kanamori, Takeda, Suzuki - 2012 - A Conjugate Property between Loss Functions and Uncertainty Sets in Classification Problems.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {consistency,convex conjugate,loss function,uncertainty set},
pages = {1461--1504},
title = {{A Conjugate Property between Loss Functions and Uncertainty Sets in Classification Problems}},
url = {http://arxiv.org/abs/1204.6583},
volume = {14},
year = {2012}
}
@inproceedings{Lafferty2007,
author = {Lafferty, John D. and Wasserman, Larry},
booktitle = {Advances in Neural Information Processing Systems 20},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lafferty, Wasserman - 2007 - Statistical analysis of semi-supervised regression.pdf:pdf},
pages = {801----808},
title = {{Statistical analysis of semi-supervised regression}},
url = {http://repository.cmu.edu/compsci/1030/},
year = {2007}
}
@article{Hoogerbrugge1983,
author = {Hoogerbrugge, Ronald and Willig, Simon J. and Kistemaker, Piet G.},
doi = {10.1021/ac00261a016},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hoogerbrugge, Willig, Kistemaker - 1983 - Discriminant analysis by double stage principal component analysis.pdf:pdf},
issn = {0003-2700},
journal = {Analytical Chemistry},
month = sep,
number = {11},
pages = {1710--1712},
title = {{Discriminant analysis by double stage principal component analysis}},
url = {http://pubs.acs.org/doi/abs/10.1021/ac00261a016},
volume = {55},
year = {1983}
}
@article{Shen2013,
abstract = {As the molecular marker density grows, there is a strong need in both genome-wide association studies and genomic selection to fit models with a large number of parameters. Here we present a computationally efficient generalized ridge regression (RR) algorithm for situations in which the number of parameters largely exceeds the number of observations. The computationally demanding parts of the method depend mainly on the number of observations and not the number of parameters. The algorithm was implemented in the R package bigRR based on the previously developed package hglm. Using such an approach, a heteroscedastic effects model (HEM) was also developed, implemented, and tested. The efficiency for different data sizes were evaluated via simulation. The method was tested for a bacteria-hypersensitive trait in a publicly available Arabidopsis data set including 84 inbred lines and 216,130 SNPs. The computation of all the SNP effects required <10 sec using a single 2.7-GHz core. The advantage in run time makes permutation test feasible for such a whole-genome model, so that a genome-wide significance threshold can be obtained. HEM was found to be more robust than ordinary RR (a.k.a. SNP-best linear unbiased prediction) in terms of QTL mapping, because SNP-specific shrinkage was applied instead of a common shrinkage. The proposed algorithm was also assessed for genomic evaluation and was shown to give better predictions than ordinary RR.},
author = {Shen, Xia and Alam, Moudud and Fikse, Freddy and R\"{o}nneg\aa rd, Lars},
doi = {10.1534/genetics.112.146720},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shen et al. - 2013 - A novel generalized ridge regression method for quantitative genetics.pdf:pdf},
issn = {1943-2631},
journal = {Genetics},
keywords = {Algorithms,Arabidopsis,Arabidopsis: genetics,Genetics, Population,Genetics, Population: methods,Genome, Plant,Genome-Wide Association Study,Genome-Wide Association Study: methods,Models, Genetic,Polymorphism, Single Nucleotide,Quantitative Trait Loci},
month = apr,
number = {4},
pages = {1255--68},
pmid = {23335338},
title = {{A novel generalized ridge regression method for quantitative genetics.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23335338},
volume = {193},
year = {2013}
}
@article{Jaffe2013,
abstract = {BACKGROUND: Significance analysis plays a major role in identifying and ranking genes, transcription factor binding sites, DNA methylation regions, and other high-throughput features associated with illness. We propose a new approach, called gene set bagging, for measuring the probability that a gene set replicates in future studies. Gene set bagging involves resampling the original high-throughput data, performing gene-set analysis on the resampled data, and confirming that biological categories replicate in the bagged samples.

RESULTS: Using both simulated and publicly-available genomics data, we demonstrate that significant categories in a gene set enrichment analysis may be unstable when subjected to resampling. We show our method estimates the replication probability (R), the probability that a gene set will replicate as a significant result in future studies, and show in simulations that this method reflects replication better than each set's p-value.

CONCLUSIONS: Our results suggest that gene lists based on p-values are not necessarily stable, and therefore additional steps like gene set bagging may improve biological inference on gene sets.},
author = {Jaffe, Andrew E and Storey, John D and Ji, Hongkai and Leek, Jeffrey T},
doi = {10.1186/1471-2105-14-360},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jaffe et al. - 2013 - Gene set bagging for estimating the probability a statistically significant result will replicate.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {dna methylation,gene expression,gene ontology,gene set enrichment analysis},
month = jan,
pages = {360},
pmid = {24330332},
title = {{Gene set bagging for estimating the probability a statistically significant result will replicate.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3890500\&tool=pmcentrez\&rendertype=abstract},
volume = {14},
year = {2013}
}
@article{Gilovich1985,
author = {Gilovich, Thomas and Vallone, Robert and Tversky, Amos},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gilovich, Vallone, Tversky - 1985 - The Hot Hand in Basketball On the Misperception of Random Sequences.pdf:pdf},
journal = {Cognitive Psychology},
pages = {295--314},
title = {{The Hot Hand in Basketball: On the Misperception of Random Sequences}},
volume = {17},
year = {1985}
}
@incollection{Opper1996,
address = {New York},
author = {Opper, Manfred and Kinzel, Wolfgang},
booktitle = {Models of Neural Networks III},
doi = {10.1007/978-1-4612-0723-8\_5},
editor = {Domany, Eytan and Hemmen, J. Leo and Schulten, Klaus},
isbn = {978-1-4612-6882-6},
pages = {151--209},
publisher = {Springer},
title = {{Statistical Mechanics of Generalization}},
year = {1996}
}
@article{Vandewalle2013,
author = {Vandewalle, Vincent and Biernacki, Christophe},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vandewalle, Biernacki - 2013 - A predictive deviance criterion for selecting a generative model in semi-supervised classification.pdf:pdf},
journal = {Computational Statistics \& Data Analysis},
pages = {220--236},
title = {{A predictive deviance criterion for selecting a generative model in semi-supervised classification}},
url = {http://www.sciencedirect.com/science/article/pii/S0167947313000546},
volume = {64},
year = {2013}
}
@unpublished{Tan2013,
author = {Tan, Yimin and Zhu, X},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tan, Zhu - 2013 - Dragging Density-Ratio Bagging.pdf:pdf},
pages = {1--10},
title = {{Dragging: Density-Ratio Bagging}},
url = {https://minds.wisconsin.edu/bitstream/handle/1793/65831/TR1795.pdf?sequence=1},
year = {2013}
}
@book{Zhang2010,
author = {Zhang, Cun-Hui},
booktitle = {The Annals of Statistics},
doi = {10.1214/09-AOS729},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang - 2010 - Nearly unbiased variable selection under minimax concave penalty.pdf:pdf},
isbn = {9040210063},
issn = {0090-5364},
keywords = {and phrases,least squares,model selection,penalized estimation,variable selection},
month = apr,
number = {2},
pages = {894--942},
title = {{Nearly unbiased variable selection under minimax concave penalty}},
url = {http://projecteuclid.org/euclid.aos/1266586618},
volume = {38},
year = {2010}
}
@article{Sejdinovic2013,
author = {Sejdinovic, Dino and Sriperumbudur, Bharath and Gretton, Arthur and Fukumizu, Kenji},
doi = {10.1214/13-AOS1140},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sejdinovic et al. - 2013 - Equivalence of distance-based and RKHS-based statistics in hypothesis testing.pdf:pdf},
journal = {The Annals of Statistics},
number = {5},
pages = {2263--2291},
title = {{Equivalence of distance-based and RKHS-based statistics in hypothesis testing}},
url = {http://projecteuclid.org/euclid.aos/1383661264},
volume = {41},
year = {2013}
}
@techreport{Vaart2012,
author = {van der Vaart, Aad and van Zanten, Harry},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/van der Vaart, van Zanten - 2012 - Nonparametric Bayesian Statistics.pdf:pdf},
title = {{Nonparametric Bayesian Statistics}},
year = {2012}
}
@inproceedings{Ben-David2007,
author = {Rakhlin, Alexander and Caponnetto, Andrea},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rakhlin, Caponnetto - Unknown - Stability of k-means clustering.pdf:pdf},
title = {{Stability of k-means clustering}}
}
@article{Vanschoren2012,
author = {Vanschoren, Joaquin and Blockeel, Hendrik and Pfahringer, Bernhard and Holmes, Geoffrey},
doi = {10.1007/s10994-011-5277-0},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vanschoren et al. - 2012 - Experiment databases.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = jan,
number = {2},
pages = {127--158},
title = {{Experiment databases}},
url = {http://www.springerlink.com/index/10.1007/s10994-011-5277-0},
volume = {87},
year = {2012}
}
@article{Breiman1996,
author = {Breiman, Leo},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Breiman - 1996 - Bagging predictors.pdf:pdf},
journal = {Machine learning},
keywords = {aggregation,averaging,bootstrap,combining},
pages = {123--140},
title = {{Bagging predictors}},
url = {http://www.springerlink.com/index/L4780124W2874025.pdf},
volume = {140},
year = {1996}
}
@article{Harville2013,
author = {Approach, Nondenominational Model-based and Harville, David A},
doi = {10.1080/00031305.2013.836987},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Approach, Harville - 2013 - The Need for More Emphasis on Prediction a “Nondenominational” Model-Based Approach.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
month = sep,
number = {September},
pages = {130902084821000},
title = {{The Need for More Emphasis on Prediction: a “Nondenominational” Model-Based Approach}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00031305.2013.836987},
year = {2013}
}
@article{Krijthe2014,
address = {To Appear},
author = {Krijthe, Jesse H. and Loog, Marco},
editor = {Appear, To},
journal = {International Conference on Pattern Recognition},
title = {{Implicitly Constrained Semi-Supervised Linear Discriminant Analysis}},
volume = {Forthcomin},
year = {2014}
}
@article{Wand2002,
author = {Wand, M.P.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wand - 2002 - Vector Differential Calculus in Statistics.pdf:pdf},
journal = {The American Statistician},
keywords = {best linear prediction,generalized linear,generalized linear mixed model,information matrix,matrix differential calculus,maximum likelihood estimation,model,penalized quasi-likelihood,score equation},
number = {1},
pages = {1--8},
title = {{Vector Differential Calculus in Statistics}},
volume = {56},
year = {2002}
}
@article{Hartley1968b,
author = {Hartley, H.O. and Rao, J.N.K.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hartley, Rao - 1968 - Classification and Estimation in Analysis of Variance Problems.pdf:pdf},
journal = {Revue de l'Institut International de Statistique},
number = {2},
pages = {141--147},
title = {{Classification and Estimation in Analysis of Variance Problems}},
url = {http://www.jstor.org/stable/10.2307/1401602},
volume = {36},
year = {1968}
}
@article{Witten2010,
abstract = {We consider the problem of clustering observations using a potentially large set of features. One might expect that the true underlying clusters present in the data differ only with respect to a small fraction of the features, and will be missed if one clusters the observations using the full set of features. We propose a novel framework for sparse clustering, in which one clusters the observations using an adaptively chosen subset of the features. The method uses a lasso-type penalty to select the features. We use this framework to develop simple methods for sparse K-means and sparse hierarchical clustering. A single criterion governs both the selection of the features and the resulting clusters. These approaches are demonstrated on simulated data and on genomic data sets.},
author = {Witten, Daniela M. and Tibshirani, Robert},
doi = {10.1198/jasa.2010.tm09415},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Witten, Tibshirani - 2010 - A framework for feature selection in clustering.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {hierarchical clustering,high-dimensional,k-means clustering,lasso,model selection,sparsity,unsupervised learning},
month = jun,
number = {490},
pages = {713--726},
pmid = {20811510},
title = {{A framework for feature selection in clustering.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2930825\&tool=pmcentrez\&rendertype=abstract},
volume = {105},
year = {2010}
}
@inproceedings{McDowell2012,
author = {McDowell, Luke and Aha, David W.},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/McDowell, Aha - 2012 - Semi-supervised collective classification via hybrid label regularization.pdf:pdf},
pages = {975--982},
title = {{Semi-supervised collective classification via hybrid label regularization}},
url = {http://arxiv.org/abs/1206.6467},
year = {2012}
}
@article{Meulman2003,
author = {Meulman, Jaqueline J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Meulman - 2003 - Prediction and classification in nonlinear data analysis Something old, something new, something borrowed, something blue.pdf:pdf},
journal = {Psychometrika},
keywords = {additive prediction components,apoe3 data,boost-,boston housing data,categorical data,cervix cancer data,clustering on variable subsets,cosa,data mining,distance based clustering,forward stagewise additive modeling,genomics,ing,monotonic regression,multiple regression,optimal scaling,optimal scoring,ordinal data,proteomics,regres-,sion splines,statistical learning,sys-,tems biology},
number = {December},
pages = {493--517},
title = {{Prediction and classification in nonlinear data analysis: Something old, something new, something borrowed, something blue}},
url = {http://www.springerlink.com/index/f475j21236661230.pdf},
year = {2003}
}
@article{Zhang2004a,
author = {Zhang, Tong},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang - 2004 - Statistical Behavior and Consistency of Classification Methods Based on Convex Risk Minimization.pdf:pdf},
journal = {The Annals of Statistics},
number = {1},
pages = {56--134},
title = {{Statistical Behavior and Consistency of Classification Methods Based on Convex Risk Minimization}},
url = {http://www.jstor.org/stable/10.2307/3448494},
volume = {32},
year = {2004}
}
@article{Crammer2006,
author = {Crammer, Koby and Dekel, Ofer and Keshet, Joseph and Shalev-Shwartz, Shai and Singer, Yoram},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Crammer et al. - 2006 - Online passive-aggressive algorithms.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {551--585},
title = {{Online passive-aggressive algorithms}},
url = {http://dl.acm.org/citation.cfm?id=1248566},
volume = {7},
year = {2006}
}
@article{Fraley2002,
author = {Fraley, Chris and Raftery, Adrian. E.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fraley, Raftery - 2002 - Model-based clustering, discriminant analysis, and density estimation.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {458},
title = {{Model-based clustering, discriminant analysis, and density estimation}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214502760047131},
volume = {97},
year = {2002}
}
@article{Cook2012,
author = {Cook, Dianne and Wickham, Hadley},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cook, Wickham - 2012 - tourrGui A gWidgets GUI for the Tour to Explore.pdf:pdf},
keywords = {dynamic graphics,exploratory data analysis,interactive graphics,mining,multivariate data visualization,visual data},
number = {6},
title = {{tourrGui : A gWidgets GUI for the Tour to Explore}},
volume = {49},
year = {2012}
}
@article{Meding2012,
abstract = {In clinical diagnostics, it is of outmost importance to correctly identify the source of a metastatic tumor, especially if no apparent primary tumor is present. Tissue-based proteomics might allow correct tumor classification. As a result, we performed MALDI imaging to generate proteomic signatures for different tumors. These signatures were used to classify common cancer types. At first, a cohort comprised of tissue samples from six adenocarcinoma entities located at different organ sites (esophagus, breast, colon, liver, stomach, thyroid gland, n = 171) was classified using two algorithms for a training and test set. For the test set, Support Vector Machine and Random Forest yielded overall accuracies of 82.74 and 81.18\%, respectively. Then, colon cancer liver metastasis samples (n = 19) were introduced into the classification. The liver metastasis samples could be discriminated with high accuracy from primary tumors of colon cancer and hepatocellular carcinoma. Additionally, colon cancer liver metastasis samples could be successfully classified by using colon cancer primary tumor samples for the training of the classifier. These findings demonstrate that MALDI imaging-derived proteomic classifiers can discriminate between different tumor types at different organ sites and in the same site.},
author = {Meding, Stephan and Nitsche, Ulrich and Balluff, Benjamin and Elsner, Mareike and Rauser, Sandra and Sch\"{o}ne, C\'{e}drik and Nipp, Martin and Maak, Matthias and Feith, Marcus and Ebert, Matthias P and Friess, Helmut and Langer, Rupert and H\"{o}fler, Heinz and Zitzelsberger, Horst and Rosenberg, Robert and Walch, Axel},
doi = {10.1021/pr200784p},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Meding et al. - 2012 - Tumor classification of six common cancer types based on proteomic profiling by MALDI imaging.pdf:pdf},
issn = {1535-3907},
journal = {Journal of proteome research},
keywords = {Adenocarcinoma,Adenocarcinoma: metabolism,Adenocarcinoma: secondary,Algorithms,Humans,Neoplasms,Neoplasms: diagnosis,Neoplasms: metabolism,Neoplasms: pathology,Proteome,Proteome: metabolism,Proteomics,Sensitivity and Specificity,Spectrometry, Mass, Matrix-Assisted Laser Desorpti,Support Vector Machines},
month = mar,
number = {3},
pages = {1996--2003},
pmid = {22224404},
title = {{Tumor classification of six common cancer types based on proteomic profiling by MALDI imaging.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22224404},
volume = {11},
year = {2012}
}
@article{Nowak2008,
abstract = {When applying hierarchical clustering algorithms to cluster patient samples from microarray data, the clustering patterns generated by most algorithms tend to be dominated by groups of highly differentially expressed genes that have closely related expression patterns. Sometimes, these genes may not be relevant to the biological process under study or their functions may already be known. The problem is that these genes can potentially drown out the effects of other genes that are relevant or have novel functions. We propose a procedure called complementary hierarchical clustering that is designed to uncover the structures arising from these novel genes that are not as highly expressed. Simulation studies show that the procedure is effective when applied to a variety of examples. We also define a concept called relative gene importance that can be used to identify the influential genes in a given clustering. Finally, we analyze a microarray data set from 295 breast cancer patients, using clustering with the correlation-based distance measure. The complementary clustering reveals a grouping of the patients which is uncorrelated with a number of known prognostic signatures and significantly differing distant metastasis-free probabilities.},
author = {Nowak, Gen and Tibshirani, Robert},
doi = {10.1093/biostatistics/kxm046},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nowak, Tibshirani - 2008 - Complementary hierarchical clustering.pdf:pdf},
issn = {1468-4357},
journal = {Biostatistics (Oxford, England)},
keywords = {Algorithms,Breast Neoplasms,Breast Neoplasms: genetics,Cluster Analysis,Computer Simulation,Female,Fuzzy Logic,Gene Expression,Gene Expression Profiling,Gene Expression Profiling: methods,Gene Expression Profiling: statistics \& numerical ,Genetic Markers,Humans,Information Storage and Retrieval,Information Storage and Retrieval: methods,Neoplasm Metastasis,Neoplasm Metastasis: genetics,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Pattern Recognition, Automated,Principal Component Analysis,Reference Values},
month = jul,
number = {3},
pages = {467--83},
pmid = {18093965},
title = {{Complementary hierarchical clustering.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3294318\&tool=pmcentrez\&rendertype=abstract},
volume = {9},
year = {2008}
}
@article{Norton2003,
author = {Norton, John D},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Norton - 2003 - A Material Theory of Induction.pdf:pdf},
journal = {Philosophy of Science},
number = {October},
pages = {647--670},
title = {{A Material Theory of Induction}},
volume = {70},
year = {2003}
}
@article{Bollen,
archivePrefix = {arXiv},
arxivId = {arXiv:1010.3003v1},
author = {Bollen, Johan and Mao, Huina and Zeng, Xiao-jun},
eprint = {arXiv:1010.3003v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bollen, Mao, Zeng - Unknown - Twitter mood predicts the stock market .pdf:pdf},
pages = {1--8},
title = {{Twitter mood predicts the stock market .}}
}
@inproceedings{Sørensen2010,
author = {S\o rensen, Lauge and Loog, Marco and Tax, DMJ},
booktitle = {Structural, Syntactic, and Statistical Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/S\o rensen, Loog, Tax - 2010 - Dissimilarity-based multiple instance learning.pdf:pdf},
keywords = {bag,dissimilarity measure,dissimilarity representation,multiple instance learning},
pages = {129--138},
title = {{Dissimilarity-based multiple instance learning}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-14980-1\_12},
year = {2010}
}
@article{Spokoiny2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1111.3029v4},
author = {Spokoiny, Vladimir},
eprint = {arXiv:1111.3029v4},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Spokoiny - 2012 - Parametric estimation. Finite sample theory.pdf:pdf},
journal = {The Annals of Statistics},
pages = {1--67},
title = {{Parametric estimation. Finite sample theory}},
url = {http://projecteuclid.org/euclid.aos/1360332187},
year = {2012}
}
@unpublished{Lockhart2013,
author = {Lockhart, Richard and Taylor, Jonathan and Tibshirani, Ryan J. and Tibshirani, Robert},
booktitle = {arXiv preprint},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lockhart et al. - 2013 - A significance test for the lasso.pdf:pdf},
keywords = {lasso,least angle regression,p-value,significance test},
title = {{A significance test for the lasso}},
url = {http://repository.cmu.edu/statistics/131/?utm\_source=repository.cmu.edu\%2Fstatistics\%2F131\&utm\_medium=PDF\&utm\_campaign=PDFCoverPages},
year = {2013}
}
@article{Schaffer1993,
author = {Schaffer, Cullen},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schaffer - 1993 - Selecting a classification method by cross-validation.pdf:pdf},
journal = {Machine Learning},
keywords = {classification,cross-validation,decision trees,neural networks},
pages = {135--143},
title = {{Selecting a classification method by cross-validation}},
url = {http://link.springer.com/article/10.1007/BF00993106},
volume = {13},
year = {1993}
}
@article{Ireland1968a,
author = {Ireland, C.T. and Kullback, S.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ireland, Kullback - 1968 - Contingence tables with given marginals.pdf:pdf},
journal = {Biometrika},
number = {1},
pages = {179--188},
title = {{Contingence tables with given marginals}},
volume = {55},
year = {1968}
}
@inproceedings{Brefeld2006,
author = {Brefeld, Ulf and G\"{a}rtner, Thomas and Scheffer, Tobias and Wrobel, Stefan},
booktitle = {Proceedings of the 23rd International Conference on Machine learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brefeld et al. - 2006 - Efficient co-regularised least squares regression.pdf:pdf},
pages = {137--144},
title = {{Efficient co-regularised least squares regression}},
url = {http://dl.acm.org/citation.cfm?id=1143862},
year = {2006}
}
@article{Xu2011,
abstract = {We consider two desired properties of learning algorithms: *sparsity* and *algorithmic stability*. Both properties are believed to lead to good generalization ability. We show that these two properties are fundamentally at odds with each other: a sparse algorithm cannot be stable and vice versa. Thus, one has to trade off sparsity and stability in designing a learning algorithm. In particular, our general result implies that \$\backslash ell\_1\$-regularized regression (Lasso) cannot be stable, while \$\backslash ell\_2\$-regularized regression is known to have strong stability properties and is therefore not sparse.},
author = {Xu, Huan and Caramanis, Constantine and Mannor, Shie},
doi = {10.1109/TPAMI.2011.177},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Xu, Caramanis, Mannor - 2011 - Sparse Algorithms are not Stable A No-free-lunch Theorem.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = aug,
pages = {1--8},
pmid = {21844627},
title = {{Sparse Algorithms are not Stable: A No-free-lunch Theorem.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21844627},
year = {2011}
}
@article{Gaffke1989,
author = {Gaffke, Norbert and Heiligers, Berthold},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gaffke, Heiligers - 1989 - Bayes, Admissible, and Minimax Linear Estimators in Linear Models with Restricted Parameter Space.pdf:pdf},
journal = {Statistics: A Journal of Theoretical and Applied Statistics},
pages = {487--508},
title = {{Bayes, Admissible, and Minimax Linear Estimators in Linear Models with Restricted Parameter Space}},
volume = {4},
year = {1989}
}
@inproceedings{Furnkranz2001,
author = {F\"{u}rnkranz, Johannes and Petrak, Johann},
booktitle = {Proceedings of the ECML/PKDD Workshop on Integrating Aspects of Data Mining, Decision Support and Meta-Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/F\"{u}rnkranz, Petrak - 2001 - An Evaluation of Landmarking Variants.pdf:pdf},
pages = {57--68},
title = {{An Evaluation of Landmarking Variants}},
url = {http://ai.ijs.si/branax/iddm-2001-proceedings/paper9.pdf},
year = {2001}
}
@article{Meier2008,
author = {Meier, Lukas and {Van De Geer}, Sara and B\"{u}hlmann, Peter},
doi = {10.1111/j.1467-9868.2007.00627.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Meier, Van De Geer, B\"{u}hlmann - 2008 - The group lasso for logistic regression.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {categorical data,co-ordinate descent algorithm,dna splice site,group variable,high dimensional generalized linear,model,penalized likelihood,selection},
month = jan,
number = {1},
pages = {53--71},
title = {{The group lasso for logistic regression}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2007.00627.x},
volume = {70},
year = {2008}
}
@inproceedings{Ben-David2012,
author = {Ben-David, Shai and Loker, David and Srebro, Nathan and Sridharan, Karthik},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-David et al. - 2012 - Minimizing the misclassification error rate using a surrogate convex loss.pdf:pdf},
pages = {1863----1870},
title = {{Minimizing the misclassification error rate using a surrogate convex loss}},
year = {2012}
}
@inproceedings{Yu2012,
address = {Edinburgh, Scotland},
author = {Yu, AW and Su, Hao and Fei-Fei, L},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yu, Su, Fei-Fei - 2012 - Efficient euclidean projections onto the intersection of norm balls.pdf:pdf},
pages = {433--440},
title = {{Efficient euclidean projections onto the intersection of norm balls}},
url = {http://arxiv.org/abs/1206.4638},
year = {2012}
}
@article{Itai1991,
author = {Itai, Alon},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Itai - 1991 - Learnability distributions with respect to fixed.pdf:pdf},
pages = {377--389},
title = {{Learnability distributions with respect to fixed}},
volume = {86},
year = {1991}
}
@article{Loog2012a,
author = {Loog, Marco},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - 2012 - Semi-supervised linear discriminant analysis using moment constraints.pdf:pdf},
journal = {Partially Supervised Learning, LNCS},
pages = {32--41},
title = {{Semi-supervised linear discriminant analysis using moment constraints}},
url = {http://www.springerlink.com/index/A3T1U8542T092156.pdf},
volume = {7081},
year = {2012}
}
@article{Poggio2004,
abstract = {Developing theoretical foundations for learning is a key step towards understanding intelligence. 'Learning from examples' is a paradigm in which systems (natural or artificial) learn a functional relationship from a training set of examples. Within this paradigm, a learning algorithm is a map from the space of training sets to the hypothesis space of possible functional solutions. A central question for the theory is to determine conditions under which a learning algorithm will generalize from its finite training set to novel examples. A milestone in learning theory was a characterization of conditions on the hypothesis space that ensure generalization for the natural class of empirical risk minimization (ERM) learning algorithms that are based on minimizing the error on the training set. Here we provide conditions for generalization in terms of a precise stability property of the learning process: when the training set is perturbed by deleting one example, the learned hypothesis does not change much. This stability property stipulates conditions on the learning map rather than on the hypothesis space, subsumes the classical theory for ERM algorithms, and is applicable to more general algorithms. The surprising connection between stability and predictivity has implications for the foundations of learning theory and for the design of novel algorithms, and provides insights into problems as diverse as language learning and inverse problems in physics and engineering.},
author = {Poggio, Tomaso and Rifkin, Ryan and Mukherjee, Sayan and Niyogi, Partha},
doi = {10.1038/nature02341},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Poggio et al. - 2004 - General conditions for predictivity in learning theory.pdf:pdf},
issn = {1476-4687},
journal = {Nature},
keywords = {Algorithms,Intelligence,Language,Learning,Learning: physiology,Models, Theoretical,Probability,Research Design},
month = mar,
number = {6981},
pages = {419--22},
pmid = {15042089},
title = {{General conditions for predictivity in learning theory.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15042089},
volume = {428},
year = {2004}
}
@article{Peters2013,
author = {Peters, J. and Buhlmann, P.},
doi = {10.1093/biomet/ast043},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Peters, Buhlmann - 2013 - Identifiability of Gaussian structural equation models with equal error variances.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = nov,
number = {1},
pages = {219--228},
title = {{Identifiability of Gaussian structural equation models with equal error variances}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/ast043},
volume = {101},
year = {2013}
}
@inproceedings{Bennett1998,
author = {Bennett, Kristin P. and Demiriz, Ayhan},
booktitle = {Advances in Neural Information Processing Systems 11},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bennett, Demiriz - 1998 - Semi-supervised support vector machines.pdf:pdf},
title = {{Semi-supervised support vector machines}},
year = {1998}
}
@article{Srivastava2007,
author = {Srivastava, Santosh},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Srivastava - 2007 - Bayesian Quadratic Discriminant Analysis.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {bregman,data-dependent prior,divergence,eigenvalue decomposition,functional analysis,quadratic discriminant analysis,regularized quadratic discriminant analysis,wishart},
pages = {1277--1305},
title = {{Bayesian Quadratic Discriminant Analysis.}},
url = {http://people.ee.duke.edu/~lcarin/SrivastavaGuptaFrigyikBDA.pdf},
volume = {8},
year = {2007}
}
@article{Loog2014a,
author = {Loog, Marco},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - 2014 - Semi-supervised linear discriminant analysis through moment-constraint parameter estimation.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {linear discriminant analysis,semi-supervised learning},
month = mar,
pages = {24--31},
publisher = {Elsevier B.V.},
title = {{Semi-supervised linear discriminant analysis through moment-constraint parameter estimation}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865513000913},
volume = {37},
year = {2014}
}
@article{Isaksson2008,
author = {Isaksson, Anders and Wallman, M. and Goransson, H. and Gustafsson, Mats G},
doi = {10.1016/j.patrec.2008.06.018},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Isaksson et al. - 2008 - Cross-validation and bootstrapping are unreliable in small sample classification.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {performance estimation,supervised classification},
month = oct,
number = {14},
pages = {1960--1965},
title = {{Cross-validation and bootstrapping are unreliable in small sample classification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865508002158},
volume = {29},
year = {2008}
}
@article{Zhou2007b,
address = {New York, New York, USA},
author = {Zhou, Dengyong and Burges, Christopher J. C.},
doi = {10.1145/1273496.1273642},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Burges - 2007 - Spectral clustering and transductive learning with multiple views.pdf:pdf},
isbn = {9781595937933},
journal = {Proceedings of the 24th international conference on Machine learning - ICML '07},
pages = {1159--1166},
publisher = {ACM Press},
title = {{Spectral clustering and transductive learning with multiple views}},
url = {http://portal.acm.org/citation.cfm?doid=1273496.1273642},
year = {2007}
}
@article{Nadler2008,
author = {Nadler, Boaz},
doi = {10.1214/08-AOS618},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nadler - 2008 - Finite sample approximation results for principal component analysis A matrix perturbation approach.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = dec,
number = {6},
pages = {2791--2817},
title = {{Finite sample approximation results for principal component analysis: A matrix perturbation approach}},
url = {http://projecteuclid.org/euclid.aos/1231165185},
volume = {36},
year = {2008}
}
@techreport{Wolpert1996,
author = {Wolpert, David H and Macready, W},
booktitle = {Santa Fe Institute Technical Report},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wolpert, Macready - 1996 - Combining Stacking With Bagging To Improve A Learning Algorithm.pdf:pdf},
pages = {1--28},
title = {{Combining Stacking With Bagging To Improve A Learning Algorithm}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.53.9933\&rep=rep1\&type=pdf},
year = {1996}
}
@inproceedings{Arthur2007,
abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, ran- domized seeding technique, we obtain an algorithm that is $\Theta$(log k)-competitive with the optimal clustering. Prelim- inary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
author = {Arthur, David and Vassilvitskii, Sergei},
booktitle = {Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Arthur, Vassilvitskii - 2007 - k-means The Advantages of Careful Seeding.pdf:pdf},
pages = {1027--1035},
title = {{k-means ++ : The Advantages of Careful Seeding}},
year = {2007}
}
@article{Kawano2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1108.5244v3},
author = {Kawano, Shuichi},
eprint = {arXiv:1108.5244v3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kawano - 2012 - Semi-supervised logistic discrimination via labeled data and unlabeled data from different sampling distributions.pdf:pdf},
journal = {arXiv preprint},
keywords = {and phrases,covariate shift,em algorithm,model selection,reg-,semi-supervised learning,ularization},
pages = {1--19},
title = {{Semi-supervised logistic discrimination via labeled data and unlabeled data from different sampling distributions}},
year = {2012}
}
@inproceedings{Cotter2013,
author = {Cotter, Andrew and Shalev-Shwartz, Shai and Srebro, Nathan},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cotter, Shalev-Shwartz, Srebro - 2013 - Learning optimally sparse support vector machines.pdf:pdf},
pages = {266--274},
title = {{Learning optimally sparse support vector machines}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/ICML2013\_cotter13},
year = {2013}
}
@article{Abney2004,
author = {Abney, Steven},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Abney - 2004 - Understanding the yarowsky algorithm.pdf:pdf},
journal = {Computational Linguistics},
number = {3},
pages = {365--395},
title = {{Understanding the yarowsky algorithm}},
url = {http://www.mitpressjournals.org/doi/pdf/10.1162/0891201041850876},
volume = {30},
year = {2004}
}
@article{Balcan2010,
author = {Balcan, Maria-Florina and Blum, Avrim},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balcan, Blum - 2010 - A Discriminative Model for Semi-Supervised Learning.pdf:pdf},
journal = {Journal of the ACM (JACM)},
number = {3},
title = {{A Discriminative Model for Semi-Supervised Learning}},
url = {http://dl.acm.org/citation.cfm?id=1706599},
volume = {57},
year = {2010}
}
@inproceedings{Bottou2010,
author = {Bottou, Leon},
booktitle = {Proceedings of COMPSTAT'2010},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bottou - 2010 - Large-scale machine learning with stochastic gradient descent.pdf:pdf},
keywords = {efficiency,online learning,stochastic gradient descent},
pages = {177--186},
publisher = {Springer},
title = {{Large-scale machine learning with stochastic gradient descent}},
url = {http://link.springer.com/chapter/10.1007/978-3-7908-2604-3\_16},
year = {2010}
}
@article{Weston2006,
address = {New York, New York, USA},
author = {Weston, Jason and Collobert, Ronan and Sinz, Fabian and Bottou, L\'{e}on and Vapnik, Vladimir},
doi = {10.1145/1143844.1143971},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Weston et al. - 2006 - Inference with the Universum.pdf:pdf},
isbn = {1595933832},
journal = {Proceedings of the 23rd international conference on Machine learning - ICML '06},
pages = {1009--1016},
publisher = {ACM Press},
title = {{Inference with the Universum}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143971},
year = {2006}
}
@article{Kulis2009,
author = {Kulis, Brian and Grauman, Kristen},
doi = {10.1109/ICCV.2009.5459466},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kulis, Grauman - 2009 - Kernelized locality-sensitive hashing for scalable image search.pdf:pdf},
isbn = {978-1-4244-4420-5},
journal = {2009 IEEE 12th International Conference on Computer Vision},
month = sep,
number = {Iccv},
pages = {2130--2137},
publisher = {Ieee},
title = {{Kernelized locality-sensitive hashing for scalable image search}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5459466},
year = {2009}
}
@article{Dougherty2001,
abstract = {In order to study the molecular biological differences between normal and diseased tissues, it is desirable to perform classification among diseases and stages of disease using microarray-based gene-expression values. Owing to the limited number of microarrays typically used in these studies, serious issues arise with respect to the design, performance and analysis of classifiers based on microarray data. This paper reviews some fundamental issues facing small-sample classification: classification rules, constrained classifiers, error estimation and feature selection. It discusses both unconstrained and constrained classifier design from sample data, and the contributions to classifier error from constrained optimization and lack of optimality owing to design from sample data. The difficulty with estimating classifier error when confined to small samples is addressed, particularly estimating the error from training data. The impact of small samples on the ability to include more than a few variables as classifier features is explained.},
author = {Dougherty, Edward R.},
doi = {10.1002/cfg.62},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dougherty - 2001 - Small sample issues for microarray-based classification.pdf:pdf},
issn = {1531-6912},
journal = {Comparative and functional genomics},
month = jan,
number = {1},
pages = {28--34},
pmid = {18628896},
title = {{Small sample issues for microarray-based classification.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2447190\&tool=pmcentrez\&rendertype=abstract},
volume = {2},
year = {2001}
}
@article{Castelli1996,
author = {Castelli, Vittorio and Cover, Thomas M.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Castelli, Cover - 1996 - The Relative Value of Labeled and Unlabeled Samples in Pattern Recognition with an Unknown Mixing Parameter.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
number = {6},
pages = {2102},
title = {{The Relative Value of Labeled and Unlabeled Samples in Pattern Recognition with an Unknown Mixing Parameter}},
volume = {42},
year = {1996}
}
@inproceedings{Loog2010,
author = {Loog, Marco},
booktitle = {Proceedings of the 2010 European Conference on Machine learning and Knowledge Discovery in Databases},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - 2010 - Constrained Parameter Estimation for Semi-Supervised Learning The Case of the Nearest Mean Classifier.pdf:pdf},
pages = {291--304},
title = {{Constrained Parameter Estimation for Semi-Supervised Learning: The Case of the Nearest Mean Classifier}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-15883-4\_19},
year = {2010}
}
@unpublished{Liu2014,
author = {Liu, Mingxia and Zhang, Daoqiang},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Liu, Zhang - 2014 - CGS A Novel Pairwise Constraint-Guided Sparse Feature Selection Method.pdf:pdf},
title = {{CGS: A Novel Pairwise Constraint-Guided Sparse Feature Selection Method}},
year = {2014}
}
@article{Tibshirani2004,
abstract = {MOTIVATION: Early cancer detection has always been a major research focus in solid tumor oncology. Early tumor detection can theoretically result in lower stage tumors, more treatable diseases and ultimately higher cure rates with less treatment-related morbidities. Protein mass spectrometry is a potentially powerful tool for early cancer detection. We propose a novel method for sample classification from protein mass spectrometry data. When applied to spectra from both diseased and healthy patients, the 'peak probability contrast' technique provides a list of all common peaks among the spectra, their statistical significance and their relative importance in discriminating between the two groups. We illustrate the method on matrix-assisted laser desorption and ionization mass spectrometry data from a study of ovarian cancers. RESULTS: Compared to other statistical approaches for class prediction, the peak probability contrast method performs as well or better than several methods that require the full spectra, rather than just labelled peaks. It is also much more interpretable biologically. The peak probability contrast method is a potentially useful tool for sample classification from protein mass spectrometry data.},
author = {Tibshirani, Robert and Hastie, Trevor and Narasimhan, Balasubramanian and Soltys, Scott and Shi, Gongyi and Koong, Albert and Le, Quynh-Thu},
doi = {10.1093/bioinformatics/bth357},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tibshirani et al. - 2004 - Sample classification from protein mass spectrometry, by 'peak probability contrasts'.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Artificial Intelligence,Cluster Analysis,Diagnosis, Computer-Assisted,Diagnosis, Computer-Assisted: methods,Female,Humans,Models, Biological,Models, Statistical,Neoplasm Proteins,Neoplasm Proteins: blood,Neoplasm Proteins: classification,Ovarian Neoplasms,Ovarian Neoplasms: blood,Ovarian Neoplasms: classification,Ovarian Neoplasms: diagnosis,Reproducibility of Results,Sensitivity and Specificity,Spectrometry, Mass, Electrospray Ionization,Spectrometry, Mass, Electrospray Ionization: metho,Spectrometry, Mass, Matrix-Assisted Laser Desorpti,Tumor Markers, Biological,Tumor Markers, Biological: blood,Tumor Markers, Biological: classification},
month = nov,
number = {17},
pages = {3034--44},
pmid = {15226172},
title = {{Sample classification from protein mass spectrometry, by 'peak probability contrasts'.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15226172},
volume = {20},
year = {2004}
}
@inproceedings{Kopf2000,
author = {K\"{o}pf, Christian and Taylor, Charles and Keller, Jorg},
booktitle = {Proceedings of the PKDD-00 workshop on data mining, decision support, meta-learning and ILP},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/K\"{o}pf, Taylor, Keller - 2000 - Meta-analysis from data characterisation for meta-learning to meta-regression.pdf:pdf},
number = {Ml},
title = {{Meta-analysis: from data characterisation for meta-learning to meta-regression}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.26.8159},
year = {2000}
}
@article{Bengio2004,
author = {Bengio, Yoshua and Grandvalet, Yves},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bengio, Grandvalet - 2004 - No unbiased estimator of the variance of k-fold cross-validation.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {cross-validation,k-fold cross-validation,statistical comparisons,variance estimators},
pages = {1089--1105},
title = {{No unbiased estimator of the variance of k-fold cross-validation}},
url = {http://dl.acm.org/citation.cfm?id=1044695},
volume = {5},
year = {2004}
}
@inproceedings{Yu2013,
author = {Yu, Felix X. and Liu, Dong and Kumar, Sanjiv and Jebara, Tony and Chang, Shih-Fu},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yu et al. - 2013 - \$\$backslash\$ propto \$ SVM for Learning with Label Proportions.pdf:pdf},
pages = {504--512},
title = {{\$\$$\backslash$backslash\$ propto \$ SVM for Learning with Label Proportions}},
year = {2013}
}
@techreport{Bensusan2000,
author = {Bensusan, H. and Giraud-carrier, Christophe and Kennedy, C.J.},
booktitle = {ILP Work-in-progress \ldots},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bensusan, Giraud-carrier, Kennedy - 2000 - A Higher-order Approach to Meta-learning.pdf:pdf},
institution = {University of Bristol},
title = {{A Higher-order Approach to Meta-learning}},
url = {http://137.222.102.8/Publications/Papers/1000471.pdf},
year = {2000}
}
@inproceedings{Rohrbach2013,
author = {Rohrbach, Marcus and Ebert, Sandra and Schiele, Bernt},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rohrbach, Ebert, Schiele - 2013 - Transfer Learning in a Transductive Setting.pdf:pdf},
pages = {46--54},
title = {{Transfer Learning in a Transductive Setting}},
url = {http://papers.nips.cc/paper/5209-transfer-learning-in-a-transductive-setting},
year = {2013}
}
@inproceedings{Cortes1993,
author = {Cortes, Corinna and Jackel, L.D.},
booktitle = {Advances in Neural Information Processing Systems 6},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cortes, Jackel - 1993 - Learning Cuves Asymptotic Values and Rate of Convergence.pdf:pdf},
pages = {327--334},
title = {{Learning Cuves: Asymptotic Values and Rate of Convergence}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Learning+Cuves:+Asymptotic+Values+and+Rate+of+Convergence\#0},
year = {1993}
}
@article{McLachlan1975,
author = {McLachlan, Geoffrey J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/McLachlan - 1975 - Iterative Reclassification Procedure for Constructing an Asymptotically Optimal Rule of Allocation in Discriminant An.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {350},
pages = {365--369},
title = {{Iterative Reclassification Procedure for Constructing an Asymptotically Optimal Rule of Allocation in Discriminant Analysis}},
volume = {70},
year = {1975}
}
@phdthesis{Druck2011,
author = {Druck, Gregory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Druck - 2011 - Generalized Expectation Criteria for Lightly Supervised Learning.pdf:pdf},
number = {September},
title = {{Generalized Expectation Criteria for Lightly Supervised Learning}},
url = {http://scholarworks.umass.edu/open\_access\_dissertations/440/},
year = {2011}
}
@inproceedings{Kohavi1995,
author = {Kohavi, Ron},
booktitle = {International Joint Conferences on Artificial Intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kohavi - 1995 - A study of cross-validation and bootstrap for accuracy estimation and model selection.pdf:pdf},
number = {2},
pages = {1137--1145},
title = {{A study of cross-validation and bootstrap for accuracy estimation and model selection}},
url = {http://frostiebek.free.fr/docs/Machine Learning/validation-1.pdf},
volume = {14},
year = {1995}
}
@article{Blei2012,
author = {Blei, David M.},
doi = {10.1145/2133806.2133826},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Blei - 2012 - Probabilistic topic models.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
month = apr,
number = {4},
pages = {77},
title = {{Probabilistic topic models}},
url = {http://dl.acm.org/citation.cfm?doid=2133806.2133826},
volume = {55},
year = {2012}
}
@article{Palazzo2014,
author = {Palazzo, Alexander F. and Gregory, T. Ryan},
doi = {10.1371/journal.pgen.1004351},
editor = {Akey, Joshua M.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Palazzo, Gregory - 2014 - The Case for Junk DNA.pdf:pdf},
issn = {1553-7404},
journal = {PLoS Genetics},
month = may,
number = {5},
pages = {e1004351},
title = {{The Case for Junk DNA}},
url = {http://dx.plos.org/10.1371/journal.pgen.1004351},
volume = {10},
year = {2014}
}
@article{Peng2002,
author = {Peng, Yonghong and Flach, Peter A. and Soares, Carlos and Brazdil, Pavel B.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Peng et al. - 2002 - Improved dataset characterisation for meta-learning.pdf:pdf},
journal = {Lecture Notes in Computer Science},
pages = {141--152},
title = {{Improved dataset characterisation for meta-learning}},
url = {http://link.springer.com/chapter/10.1007/3-540-36182-0\_14},
volume = {2534},
year = {2002}
}
@article{Zach2014,
abstract = {In this work, we present a unified view on Markov random fields (MRFs) and recently proposed continuous tight convex relaxations for multilabel assignment in the image plane. These relaxations are far less biased toward the grid geometry than Markov random fields on grids. It turns out that the continuous methods are nonlinear extensions of the well-established local polytope MRF relaxation. In view of this result, a better understanding of these tight convex relaxations in the discrete setting is obtained. Further, a wider range of optimization methods is now applicable to find a minimizer of the tight formulation. We propose two methods to improve the efficiency of minimization. One uses a weaker, but more efficient continuously inspired approach as initialization and gradually refines the energy where it is necessary. The other one reformulates the dual energy enabling smooth approximations to be used for efficient optimization. We demonstrate the utility of our proposed minimization schemes in numerical experiments. Finally, we generalize the underlying energy formulation from isotropic metric smoothness costs to arbitrary nonmetric and orientation dependent smoothness terms.},
author = {Zach, Christopher and H\"{a}ne, Christian and Pollefeys, Marc},
doi = {10.1109/TPAMI.2013.105},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zach, H\"{a}ne, Pollefeys - 2014 - What Is Optimized in Convex Relaxations for Multilabel Problems Connecting Discrete and Continuously Ins.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = jan,
number = {1},
pages = {157--70},
pmid = {24231873},
title = {{What Is Optimized in Convex Relaxations for Multilabel Problems: Connecting Discrete and Continuously Inspired MAP Inference.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24231873},
volume = {36},
year = {2014}
}
@article{Kalousis2001,
author = {Kalousis, Alexandros and Hilario, Melanie},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kalousis, Hilario - 2001 - Model selection via meta-learning a comparative study.pdf:pdf},
journal = {International Journal on Artificial Intelligence Tools},
number = {4},
title = {{Model selection via meta-learning: a comparative study}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0218213001000647},
volume = {10},
year = {2001}
}
@unpublished{Loogb,
author = {Loog, Marco},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - Unknown - Semi-Supervised Linear Discriminant Analysis through Moment-Constraint Parameter Estimation.pdf:pdf},
keywords = {affine invariant,classification,constraints,linear discriminant analysis,moment,semi-supervised learning},
title = {{Semi-Supervised Linear Discriminant Analysis through Moment-Constraint Parameter Estimation}}
}
@article{Wang2009a,
author = {Wang, Fei and Wang, Xin and Li, Tao},
doi = {10.1109/CVPR.2009.5206675},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Wang, Li - 2009 - Beyond the graphs Semi-parametric semi-supervised discriminant analysis.pdf:pdf},
isbn = {978-1-4244-3992-8},
journal = {IEEE Conference on Computer Vision and Pattern Recognition},
month = jun,
pages = {2113--2120},
publisher = {Ieee},
title = {{Beyond the graphs: Semi-parametric semi-supervised discriminant analysis}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5206675},
year = {2009}
}
@article{Bengio2009,
author = {Bengio, Yoshua},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bengio - 2009 - Learning deep architectures for AI.pdf:pdf},
journal = {Foundations and trends® in Machine Learning},
title = {{Learning deep architectures for AI}},
url = {http://dl.acm.org/citation.cfm?id=1658424},
year = {2009}
}
@inproceedings{Duin2002,
author = {Pekalska, Ella and Duin, Robert P.W. and Skurichina, Marina},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pekalska, Duin, Skurichina - 2002 - A discussion on the classifier projection space for classifier combining.pdf:pdf},
pages = {137--148},
title = {{A discussion on the classifier projection space for classifier combining}},
url = {http://www.springerlink.com/index/A98FBKT93AK0YNNE.pdf},
year = {2002}
}
@inproceedings{Szummer2001,
author = {Szummer, Martin and Jaakkola, Tommi},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Szummer, Jaakkola - 2001 - Clustering and efficient use of unlabeled examples.pdf:pdf},
title = {{Clustering and efficient use of unlabeled examples}},
url = {http://www.ai.mit.edu/projects/ntt/projects/MIT2000-08/documents/SzummerJaakkola.pdf},
year = {2001}
}
@inproceedings{Macia2010,
author = {Macia, Nuria and Ho, Tin Kam and Orriols-puig, Albert and Bernad\'{o}-Mansilla, Ester},
booktitle = {Proceedings of the 20th International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Macia et al. - 2010 - The Landscape Contest at ICPR 2010.pdf:pdf},
pages = {29--45},
title = {{The Landscape Contest at ICPR 2010}},
year = {2010}
}
@inproceedings{Cortes2004,
author = {Cortes, Corinna and Mohri, Mehryar},
booktitle = {Advances in Neural Information Processing Systems 16},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cortes, Mohri - 2004 - AUC optimization vs. error rate minimization.pdf:pdf},
pages = {313--320},
title = {{AUC optimization vs. error rate minimization}},
url = {http://books.google.com/books?hl=en\&lr=\&id=0F-9C7K8fQ8C\&oi=fnd\&pg=PA313\&dq=AUC+Optimization+vs+.+Error+Rate+Minimization\&ots=TGKup\_Ra93\&sig=VTdv-C5TW9itNMlz43YJjmxRKAc},
year = {2004}
}
@article{Niyogi2013,
author = {Niyogi, Partha},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Niyogi - 2013 - Manifold Regularization and Semi-supervised Learning Some Theoretical Analyses.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {graph laplacian,manifold regularization,minimax rates,semi-supervised learning},
pages = {1229--1250},
title = {{Manifold Regularization and Semi-supervised Learning : Some Theoretical Analyses}},
volume = {14},
year = {2013}
}
@article{Wang2004,
author = {Wang, Duolao and Murphy, Michael},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Murphy - 2004 - Estimating optimal transformations for multiple regression using the ACE algorithm.pdf:pdf},
journal = {Journal of data science},
keywords = {ace,algorithm,alternating conditional expectation,non-,parametric regression,transformation},
pages = {329--346},
title = {{Estimating optimal transformations for multiple regression using the ACE algorithm}},
url = {http://www.jds-online.com/file\_download/56/JDS-156.pdf},
volume = {2},
year = {2004}
}
@article{Michie1994,
author = {Michie, D and Spiegelhalter, D J and Taylor, C C},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Michie, Spiegelhalter, Taylor - 1994 - Statlog.pdf:pdf},
title = {{Statlog}},
year = {1994}
}
@article{Collobert2006,
author = {Collobert, Ronan and Sinz, Fabian and Weston, Jason and Bottou, Leon},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Collobert et al. - 2006 - Large scale transductive SVMs.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {cccp,semi-supervised learning,transduction,transductive svms},
pages = {1687--1712},
title = {{Large scale transductive SVMs}},
url = {http://dl.acm.org/citation.cfm?id=1248609},
volume = {7},
year = {2006}
}
@article{Dhillon2013,
author = {Dhillon, Paramveer S. and Foster, Dean P. and Kakade, Sham M. and Ungar, Lyle H.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dhillon et al. - 2013 - A Risk Comparison of Ordinary Least Squares vs Ridge Regression.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {pca,ridge regression,risk inflation},
pages = {1505--1511},
title = {{A Risk Comparison of Ordinary Least Squares vs Ridge Regression}},
url = {http://adsabs.harvard.edu/abs/2011arXiv1105.0875D},
volume = {14},
year = {2013}
}
@inproceedings{Nigam2000a,
author = {Nigam, Kamal and Ghani, R},
booktitle = {Proceedings of the 9th International Conference on Information and Knowledge Management},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nigam, Ghani - 2000 - Analyzing the effectiveness and applicability of co-training.pdf:pdf},
isbn = {1581133200},
keywords = {a related set of,blum and mitchell 1,for example,in problem domains where,into,present,research uses labeled and,the features naturally divide,two disjoint sets,unlabeled data},
pages = {86--93},
title = {{Analyzing the effectiveness and applicability of co-training}},
url = {http://dl.acm.org/citation.cfm?id=354805},
year = {2000}
}
@inproceedings{Giraud-Carrier2005,
author = {Giraud-carrier, Christophe and Provost, Foster},
booktitle = {In Proceedings of the ICML-2005 Workshop on Meta-learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Giraud-carrier, Provost - 2005 - Toward a justification of meta-learning Is the no free lunch theorem a show-stopper.pdf:pdf},
pages = {12--19},
title = {{Toward a justification of meta-learning: Is the no free lunch theorem a show-stopper}},
url = {http://dml.cs.byu.edu/~cgc/pubs/ICML2005WS.pdf},
year = {2005}
}
@article{Nock2009,
abstract = {Bartlett et al. (2006) recently proved that a ground condition for surrogates, classification calibration, ties up their consistent minimization to that of the classification risk, and left as an important problem the algorithmic questions about their minimization. In this paper, we address this problem for a wide set which lies at the intersection of classification calibrated surrogates and those of Murata et al. (2004). This set coincides with those satisfying three common assumptions about surrogates. Equivalent expressions for the members-sometimes well known-follow for convex and concave surrogates, frequently used in the induction of linear separators and decision trees. Most notably, they share remarkable algorithmic features: for each of these two types of classifiers, we give a minimization algorithm provably converging to the minimum of any such surrogate. While seemingly different, we show that these algorithms are offshoots of the same "master" algorithm. This provides a new and broad unified account of different popular algorithms, including additive regression with the squared loss, the logistic loss, and the top-down induction performed in CART, C4.5. Moreover, we show that the induction enjoys the most popular boosting features, regardless of the surrogate. Experiments are provided on 40 readily available domains.},
author = {Nock, Richard and Nielsen, Frank},
doi = {10.1109/TPAMI.2008.225},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nock, Nielsen - 2009 - Bregman divergences and surrogates for learning.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Computer Simulation,Decision Support Techniques,Models, Theoretical,Pattern Recognition, Automated,Pattern Recognition, Automated: methods},
month = nov,
number = {11},
pages = {2048--59},
pmid = {19762930},
title = {{Bregman divergences and surrogates for learning.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19762930},
volume = {31},
year = {2009}
}
@article{Bengio2013,
abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
doi = {10.1109/TPAMI.2013.50},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bengio, Courville, Vincent - 2013 - Representation learning a review and new perspectives.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Artificial Intelligence: trends,Humans,Neural Networks (Computer)},
month = aug,
number = {8},
pages = {1798--828},
pmid = {23787338},
title = {{Representation learning: a review and new perspectives.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23787338},
volume = {35},
year = {2013}
}
@article{Reif2012,
author = {Reif, Matthias and Shafait, Faisal and Goldstein, Markus and Breuel, Thomas and Dengel, Andreas},
doi = {10.1007/s10044-012-0280-z},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Reif et al. - 2012 - Automatic classifier selection for non-experts.pdf:pdf},
issn = {1433-7541},
journal = {Pattern Analysis and Applications},
keywords = {classifier recommendation,classifier selection,landmarking,meta-features,meta-learning,regression},
month = jul,
title = {{Automatic classifier selection for non-experts}},
url = {http://www.springerlink.com/index/10.1007/s10044-012-0280-z},
year = {2012}
}
@book{Rothenberg1973,
author = {Rothenberg, Thomas J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rothenberg - 1973 - Efficient Estimation with A Priori Information.pdf:pdf},
publisher = {Yale University Press},
title = {{Efficient Estimation with A Priori Information}},
url = {http://www.getcited.org/pub/101421013},
year = {1973}
}
@article{Kasabov2003,
author = {Kasabov, Nikola and Pang, Shaoning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kasabov, Pang - 2003 - Transductive Support Vector Machines and Applications in Bioinformatics for Promoter Recognition.pdf:pdf},
journal = {Proceedings of the International Conference on Neural networks and signal processing},
keywords = {inductive svm,motif,promoter,promoter recognition,transductive svm},
number = {2},
pages = {31--38},
title = {{Transductive Support Vector Machines and Applications in Bioinformatics for Promoter Recognition}},
volume = {3},
year = {2003}
}
@inproceedings{Loog2012b,
author = {Loog, Marco and Jensen, Are C},
booktitle = {Structural, Syntactic, and Statistical Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog, Jensen - 2012 - Constrained log-likelihood-based semi-supervised linear discriminant analysis.pdf:pdf},
pages = {327--335},
title = {{Constrained log-likelihood-based semi-supervised linear discriminant analysis}},
url = {http://www.springerlink.com/index/U16X1L3015777162.pdf},
year = {2012}
}
@inproceedings{Chen2011,
author = {Chen, Minmin and Weinberger, Kilian Q. and Blitzer, John C.},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chen, Weinberger, Blitzer - 2011 - Co-Training for Domain Adaptation.pdf:pdf},
pages = {2456--2464},
title = {{Co-Training for Domain Adaptation.}},
url = {https://papers.nips.cc/paper/4433-co-training-for-domain-adaptation.pdf},
year = {2011}
}
@article{Goldenberg2009,
author = {Luxburg, Ulrike Von},
doi = {10.1561/2200000008},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Luxburg - 2009 - Clustering Stability An overview.pdf:pdf},
issn = {1935-8237},
journal = {Foundations and Trends® in Machine Learning},
number = {3},
pages = {235--274},
title = {{Clustering Stability: An overview}},
url = {http://www.nowpublishers.com/product.aspx?product=MAL\&doi=2200000008},
volume = {2},
year = {2009}
}
@article{Hartley1968,
author = {Hartley, H.O. and Rao, J.N.K.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hartley, Rao - 1968 - A new estimation for sample theory surveys.pdf:pdf},
journal = {Biometrika},
number = {3},
pages = {547--557},
title = {{A new estimation for sample theory surveys}},
volume = {55},
year = {1968}
}
@inproceedings{Li2011,
author = {Li, Yu-Feng and Zhou, Zhi-hua},
booktitle = {Proceedings of the 28th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li, Zhou - 2011 - Towards making unlabeled data never hurt.pdf:pdf},
pages = {1081----1088},
title = {{Towards making unlabeled data never hurt}},
url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/ICML2011Li\_548.pdf},
year = {2011}
}
@article{Li2013,
author = {Li, YF and Tsang, IW and Kwok, JT and Zhou, ZH},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li et al. - 2013 - Convex and Scalable Weakly Labeled SVMs.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {2151--2188},
title = {{Convex and Scalable Weakly Labeled SVMs}},
url = {http://arxiv.org/abs/1303.1271},
volume = {14},
year = {2013}
}
@inproceedings{Shindler2011,
author = {Shindler, Michael and Wong, Alex and Meyerson, Adam},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shindler, Wong, Meyerson - 2011 - Fast and accurate k-means for large datasets.pdf:pdf},
pages = {2375--2383},
title = {{Fast and accurate k-means for large datasets}},
url = {http://web.engr.oregonstate.edu/~shindler/papers/FastKMeans\_nips11.pdf http://machinelearning.wustl.edu/mlpapers/paper\_files/NIPS2011\_1271.pdf},
year = {2011}
}
@misc{Young,
author = {Young, Peter},
booktitle = {Order A Journal On The Theory Of Ordered Sets And Its Applications},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Young - Unknown - Jackknife and Bootstrap Resampling Methods in Statistical Analysis to Correct for Bias.pdf:pdf},
number = {8},
pages = {1--9},
title = {{Jackknife and Bootstrap Resampling Methods in Statistical Analysis to Correct for Bias}},
volume = {1}
}
@article{Horton2007,
abstract = {Missing data are a recurring problem that can cause bias or lead to inefficient analyses. Development of statistical methods to address missingness have been actively pursued in recent years, including imputation, likelihood and weighting approaches. Each approach is more complicated when there are many patterns of missing values, or when both categorical and continuous random variables are involved. Implementations of routines to incorporate observations with incomplete variables in regression models are now widely available. We review these routines in the context of a motivating example from a large health services research dataset. While there are still limitations to the current implementations, and additional efforts are required of the analyst, it is feasible to incorporate partially observed values, and these methods should be utilized in practice.},
author = {Horton, Nicholas J and Kleinman, Ken P},
doi = {10.1198/000313007X172556},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Horton, Kleinman - 2007 - Much ado about nothing A comparison of missing data methods and software to fit incomplete data regression mod.pdf:pdf},
isbn = {000313007X},
issn = {0003-1305},
journal = {The American statistician},
keywords = {conditional gaussian,health services research,maximum likelihood,multiple imputation,psychiatric epidemi-},
month = feb,
number = {1},
pages = {79--90},
pmid = {17401454},
title = {{Much ado about nothing: A comparison of missing data methods and software to fit incomplete data regression models.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1839993\&tool=pmcentrez\&rendertype=abstract},
volume = {61},
year = {2007}
}
@article{Scott2009,
author = {Scott, Clayton and Blanchard, Gilles},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Scott, Blanchard - 2009 - Novelty detection Unlabeled data definitely help.pdf:pdf},
pages = {464--471},
title = {{Novelty detection: Unlabeled data definitely help}},
url = {http://eprints.pascal-network.org/archive/00004475/},
volume = {5},
year = {2009}
}
@inproceedings{Ji2012,
author = {Ji, Ming and Yang, Tianbao and Lin, Binbin and Jin, Rong and Han, Jiawei},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ji et al. - 2012 - A simple algorithm for semi-supervised learning with improved generalization error bound.pdf:pdf},
number = {2},
title = {{A simple algorithm for semi-supervised learning with improved generalization error bound}},
url = {http://arxiv.org/abs/1206.6412},
year = {2012}
}
@article{Wang2012b,
author = {Wang, Hua and Nie, Feiping and Huang, Heng},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Nie, Huang - 2012 - High-Order Multi-Task Feature Learning to Identify Longitudinal Phenotypic Markers for Alzheimer's Disease Pro.pdf:pdf},
journal = {Advances in \ldots},
pages = {1--9},
title = {{High-Order Multi-Task Feature Learning to Identify Longitudinal Phenotypic Markers for Alzheimer's Disease Progression Prediction}},
url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/NIPS2012\_0621.pdf},
year = {2012}
}
@article{Huopaniemi2010,
abstract = {Analysis of variance (ANOVA)-type methods are the default tool for the analysis of data with multiple covariates. These tools have been generalized to the multivariate analysis of high-throughput biological datasets, where the main challenge is the problem of small sample size and high dimensionality. However, the existing multi-way analysis methods are not designed for the currently increasingly important experiments where data is obtained from multiple sources. Common examples of such settings include integrated analysis of metabolic and gene expression profiles, or metabolic profiles from several tissues in our case, in a controlled multi-way experimental setup where disease status, medical treatment, gender and time-series are usual covariates.},
author = {Huopaniemi, Ilkka and Suvitaival, Tommi and Nikkil\"{a}, Janne and Oresic, Matej and Kaski, Samuel},
doi = {10.1093/bioinformatics/btq174},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huopaniemi et al. - 2010 - Multivariate multi-way analysis of multi-source data.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Analysis of Variance,Data Collection,Gene Expression Profiling,Gene Expression Profiling: methods,Multivariate Analysis},
month = jun,
number = {12},
pages = {i391--8},
pmid = {20529933},
title = {{Multivariate multi-way analysis of multi-source data.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2881359\&tool=pmcentrez\&rendertype=abstract},
volume = {26},
year = {2010}
}
@inproceedings{Walt2007,
author = {Walt, Christiaan Van Der and Barnard, Etienne},
booktitle = {18th Annual Symposium of the Pattern Recognition Association of South Africa},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Walt, Barnard - 2007 - Measures for the characterisation of pattern-recognition data sets.pdf:pdf},
title = {{Measures for the characterisation of pattern-recognition data sets}},
url = {http://researchspace.csir.co.za/dspace/handle/10204/1979},
year = {2007}
}
@inproceedings{Zhu2003,
author = {Zhu, Xiaojin and Ghahramani, Zoubin and Lafferty, John},
booktitle = {Proceedings of the 20th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhu, Ghahramani, Lafferty - 2003 - Semi-supervised learning using gaussian fields and harmonic functions.pdf:pdf},
pages = {912--919},
title = {{Semi-supervised learning using gaussian fields and harmonic functions}},
url = {http://www.aaai.org/Papers/ICML/2003/ICML03-118.pdf},
year = {2003}
}
@inproceedings{Muandet2012,
author = {Muandet, Krikamol and Fukumizu, K},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Muandet, Fukumizu - 2012 - Learning from distributions via support measure machines.pdf:pdf},
pages = {1--9},
title = {{Learning from distributions via support measure machines}},
url = {http://arxiv.org/abs/1202.6504},
year = {2012}
}
@article{Culp2008,
author = {Culp, Mark and Michailidis, G},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Culp, Michailidis - 2008 - An iterative algorithm for extending learners to a semi-supervised setting.pdf:pdf},
journal = {Journal of Computational and Graphical Statistics},
keywords = {convergence,iterative algorithm,linear smoothers,semi-supervised learning},
number = {3},
pages = {545--571},
title = {{An iterative algorithm for extending learners to a semi-supervised setting}},
url = {http://amstat.tandfonline.com/doi/full/10.1198/106186008X344748},
volume = {17},
year = {2008}
}
@inproceedings{Sa1994,
author = {Sa, Virginia R De},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sa - 1994 - Learning Classification with Unlabeled Data.pdf:pdf},
pages = {112--112},
title = {{Learning Classification with Unlabeled Data}},
year = {1994}
}
@inproceedings{Wager2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1307.1493v2},
author = {Wager, Stefan and Wang, Sida and Liang, Percy},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {arXiv:1307.1493v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wager, Wang, Liang - 2013 - Dropout training as adaptive regularization.pdf:pdf},
pages = {351--359},
title = {{Dropout training as adaptive regularization}},
url = {http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization},
year = {2013}
}
@article{Samworth2012,
author = {Samworth, Richard J.},
doi = {10.1214/12-AOS1049},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Samworth - 2012 - Optimal weighted nearest neighbour classifiers.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = oct,
number = {5},
pages = {2733--2763},
title = {{Optimal weighted nearest neighbour classifiers}},
url = {http://projecteuclid.org/euclid.aos/1359987536},
volume = {40},
year = {2012}
}
@article{Robert2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1006.5366v4},
author = {Robert, Christian P. and Gelman, Andrew},
eprint = {arXiv:1006.5366v4},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Robert, Gelman - 2012 - Not Only Defended But Also Applied The Perceived Absurdity of Bayesian Inference.pdf:pdf},
journal = {arXiv preprint},
keywords = {bayesian,doomsdsay,foundations,frequentist,laplace law of succession},
pages = {1--10},
title = {{"Not Only Defended But Also Applied": The Perceived Absurdity of Bayesian Inference.}},
url = {http://basepub.dauphine.fr/handle/123456789/11069},
year = {2012}
}
@article{Dempster1977,
author = {Dempster, AP and Laird, NM and Rubin, DB},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dempster, Laird, Rubin - 1977 - Maximum likelihood from incomplete data via the EM algorithm.pdf:pdf},
isbn = {0000000779},
journal = {Journal of the Royal Statistical Society. Series B},
keywords = {incomplete,likelihood,maximum},
number = {1},
pages = {1--38},
title = {{Maximum likelihood from incomplete data via the EM algorithm}},
url = {http://www.jstor.org/stable/10.2307/2984875},
volume = {39},
year = {1977}
}
@inproceedings{Matti2006,
author = {Kaariainen, Matti},
booktitle = {International Joint Conference on Neural Networks},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kaariainen - 2006 - Semi-Supervised Model Selection Based on Cross-Validation.pdf:pdf},
number = {510},
title = {{Semi-Supervised Model Selection Based on Cross-Validation}},
year = {2006}
}
@article{Kuncheva2003,
author = {Kuncheva, Ludmila I and Whitaker, Christopher J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kuncheva, Whitaker - 2003 - Measures of Diversity in Classifier Ensembles.pdf:pdf},
journal = {Machine Learning},
keywords = {committee of learners,dependency and diversity,multiple classifiers ensemble,pattern recognition},
pages = {181--207},
title = {{Measures of Diversity in Classifier Ensembles}},
volume = {51},
year = {2003}
}
@inproceedings{Ben-David2011,
author = {Ben-David, Shai and Srebro, Nathan and Urner, R},
booktitle = {Philosophy and Machine Learning - Workshop at NIPS},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-David, Srebro, Urner - 2011 - Universal learning vs. no free lunch results.pdf:pdf},
title = {{Universal learning vs. no free lunch results}},
url = {http://www.dsi.unive.it/PhiMaLe2011/Abstract/Ben-David\_Srebro\_Urner.pdf},
year = {2011}
}
@article{Rifkin2003,
author = {Rifkin, Ryan and Yeo, Gene and Poggio, Tomaso},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rifkin, Yeo, Poggio - 2003 - Regularized least-squares classification.pdf:pdf},
journal = {Nato Science Series Sub Series III Computer and Systems Sciences 190},
title = {{Regularized least-squares classification}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Chapter+7+Classification\#0 http://www.researchgate.net/publication/2833962\_Regularized\_Least-Squares\_Classification/file/32bfe50e3fcab4eb89.pdf},
year = {2003}
}
@article{Gelman2011a,
author = {Gelman, Andrew},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman - 2011 - Ethics and statistics Open data and open methods.pdf:pdf},
journal = {Chance},
pages = {51--53},
title = {{Ethics and statistics: Open data and open methods}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Ethics+and+Statistics+Open+Data+and+Open+Methods\#3},
year = {2011}
}
@article{Burman1989,
author = {Burman, Prabir},
doi = {10.2307/2336116},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Burman - 1989 - A Comparative Study of Ordinary Cross-Validation, v-Fold Cross-Validation and the Repeated Learning-Testing Methods.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
month = sep,
number = {3},
pages = {503},
title = {{A Comparative Study of Ordinary Cross-Validation, v-Fold Cross-Validation and the Repeated Learning-Testing Methods}},
url = {http://www.jstor.org/stable/2336116?origin=crossref},
volume = {76},
year = {1989}
}
@article{Kuncheva2010,
abstract = {Classification of brain images obtained through functional magnetic resonance imaging (fMRI) poses a serious challenge to pattern recognition and machine learning due to the extremely large feature-to-instance ratio. This calls for revision and adaptation of the current state-of-the-art classification methods. We investigate the suitability of the random subspace (RS) ensemble method for fMRI classification. RS samples from the original feature set and builds one (base) classifier on each subset. The ensemble assigns a class label by either majority voting or averaging of output probabilities. Looking for guidelines for setting the two parameters of the method-ensemble size and feature sample size-we introduce three criteria calculated through these parameters: usability of the selected feature sets, coverage of the set of "important" features, and feature set diversity. Optimized together, these criteria work toward producing accurate and diverse individual classifiers. RS was tested on three fMRI datasets from single-subject experiments: the Haxby data (Haxby, 2001.) and two datasets collected in-house. We found that RS with support vector machines (SVM) as the base classifier outperformed single classifiers as well as some of the most widely used classifier ensembles such as bagging, AdaBoost, random forest, and rotation forest. The closest rivals were the single SVM and bagging of SVM classifiers. We use kappa-error diagrams to understand the success of RS.},
author = {Kuncheva, Ludmila I and Rodriguez, Juan J and Plumpton, Catrin O and Linden, David E J and Johnston, Stephen J},
doi = {10.1109/TMI.2009.2037756},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kuncheva et al. - 2010 - Random subspace ensembles for FMRI classification.pdf:pdf},
issn = {1558-254X},
journal = {IEEE transactions on medical imaging},
keywords = {Adult,Algorithms,Brain,Brain: physiology,Computer Simulation,Humans,Magnetic Resonance Imaging,Magnetic Resonance Imaging: methods,Male,Multivariate Analysis,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Reproducibility of Results},
month = feb,
number = {2},
pages = {531--42},
pmid = {20129853},
title = {{Random subspace ensembles for FMRI classification.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20129853},
volume = {29},
year = {2010}
}
@inproceedings{Macia2009,
author = {Macia, Nuria and Orriols-puig, Albert and Bernad\'{o}-Mansilla, Ester},
booktitle = {Hybrid Artificial Intelligence Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Macia, Orriols-puig, Bernad\'{o}-Mansilla - 2009 - Beyond Homemade Artificial Data Sets.pdf:pdf},
keywords = {artificial data sets,data complexity,machine learning},
pages = {605--612},
title = {{Beyond Homemade Artificial Data Sets}},
url = {http://www.springerlink.com/index/N23720WL67U355MV.pdf http://link.springer.com/chapter/10.1007/978-3-642-02319-4\_73},
year = {2009}
}
@inproceedings{Niu2012,
author = {Niu, Gang and Dai, Bo and Yamada, Makoto and Sugiyama, Masashi},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Niu et al. - 2012 - Information-theoretic Semi-supervised Metric Learning via Entropy Regularization.pdf:pdf},
number = {c},
title = {{Information-theoretic Semi-supervised Metric Learning via Entropy Regularization}},
url = {http://arxiv.org/abs/1206.4614},
year = {2012}
}
@inproceedings{Zhang2000,
author = {Zhang, Tong},
booktitle = {Proceedings of the 17th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang - 2000 - The value of unlabeled data for classification problems.pdf:pdf},
pages = {1191--1198},
title = {{The value of unlabeled data for classification problems}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.20.6025\&rep=rep1\&type=pdf},
year = {2000}
}
@book{Zhu2009,
author = {Zhu, Xiaojin and Goldberg, Andrew B.},
booktitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
doi = {10.2200/S00196ED1V01Y200906AIM006},
editor = {Brachman, Ronald J. and Dietterich, Thomas},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhu, Goldberg - 2009 - Introduction to Semi-Supervised Learning.pdf:pdf},
isbn = {9781598295474},
issn = {1939-4608},
month = jan,
number = {1},
pages = {1--130},
publisher = {Morgan \& Claypool},
title = {{Introduction to Semi-Supervised Learning}},
volume = {3},
year = {2009}
}
@book{Kuncheva2004,
author = {Kuncheva, Ludmila I},
booktitle = {Methods and Algorithms. Wiley, Chichester},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kuncheva - 2004 - Combining Pattern Classifers.pdf:pdf},
isbn = {9786468600},
title = {{Combining Pattern Classifers}},
url = {http://www.tandfonline.com/doi/abs/10.1198/tech.2005.s320 http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Combining+Pattern+Classifiers\#3},
year = {2004}
}
@inproceedings{Balcan2006,
author = {Balcan, Maria-Florina and Beygelzimer, Alina and Langford, John},
booktitle = {Proceedings of the 23rd International Conference on Machine learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balcan, Beygelzimer, Langford - 2006 - Agnostic active learning.pdf:pdf},
keywords = {active learning,agnostic setting,linear,sample complexity},
pages = {65--72},
title = {{Agnostic active learning}},
url = {http://www.sciencedirect.com/science/article/pii/S0022000008000652},
year = {2006}
}
@article{Dietterich1997,
author = {Dietterich, Thomas G},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dietterich - 1997 - Machine-Learning Research.pdf:pdf},
number = {4},
pages = {97--136},
title = {{Machine-Learning Research}},
volume = {18},
year = {1997}
}
@article{Buntine1991,
author = {Buntine, Wray},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Buntine - 1991 - Learning Classification Trees.pdf:pdf},
journal = {Artificial Intelligence and Statistics},
title = {{Learning Classification Trees}},
year = {1991}
}
@book{Quinonero-Candela2009,
author = {Quinonero-Candela, Joaquin and Sugiyama, Masashi and Schwaighofer, Anton and Lawrence, Neil D.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Quinonero-Candela et al. - 2009 - Dataset shift in machine learning.pdf:pdf},
isbn = {9780262170055},
title = {{Dataset shift in machine learning}},
url = {http://dl.acm.org/citation.cfm?id=1462129},
year = {2009}
}
@inproceedings{Ogawa2013,
author = {Ogawa, Kohei and Imamura, Motoki and Takeuchi, Ichiro and Sugiyama, Masashi},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ogawa et al. - 2013 - Infinitesimal Annealing for Training Semi-Supervised Support Vector Machines.pdf:pdf},
pages = {897--905},
title = {{Infinitesimal Annealing for Training Semi-Supervised Support Vector Machines}},
url = {http://sugiyama-www.cs.titech.ac.jp/~sugi/2013/ICML2013b.pdf},
year = {2013}
}
@inproceedings{Giraud-Carrier2008,
author = {Giraud-carrier, Christophe},
booktitle = {Tutorial at the 2008 International Conference on Machine Learning and Applications},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Giraud-carrier - 2008 - Metalearning - A Tutorial.pdf:pdf},
number = {December},
title = {{Metalearning - A Tutorial}},
url = {http://dml.cs.byu.edu/~cgc/docs/ICMLA2008Tut/ICMLA 2008.pdf},
year = {2008}
}
@article{Ramakrishnan2013,
address = {New York, New York, USA},
author = {Ramakrishnan, Raghu and Cisl, Team Members},
doi = {10.1145/2487575.2492151},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ramakrishnan, Cisl - 2013 - Scale-out beyond map-reduce.pdf:pdf},
isbn = {9781450321747},
journal = {Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '13},
keywords = {analytics,big data,data science,hadoop,machine learning,map-,reduce,reef,scale-out,sql,yarn},
pages = {1},
publisher = {ACM Press},
title = {{Scale-out beyond map-reduce}},
url = {http://dl.acm.org/citation.cfm?doid=2487575.2492151},
year = {2013}
}
@article{Senn2011,
author = {Senn, Stephen},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Senn - 2011 - You may believe you are a Bayesian but you are probably wrong.pdf:pdf},
journal = {Rationality, Markets and Morals},
pages = {48--66},
title = {{You may believe you are a Bayesian but you are probably wrong}},
url = {http://www.rmm-journal.com/downloads/Article\_Senn.pdf},
volume = {2},
year = {2011}
}
@inproceedings{Singh2008,
author = {Singh, Aarti and Nowak, Robert D. and Zhu, Xiaojin},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Singh, Nowak, Zhu - 2008 - Unlabeled data Now it helps , now it doesn’t.pdf:pdf},
pages = {1513--1520},
title = {{Unlabeled data: Now it helps , now it doesn’t}},
url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/NIPS2008\_0889.pdf},
year = {2008}
}
@article{Shi2011,
author = {Shi, Mingguang and Zhang, Bing},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shi, Zhang - 2011 - Semi-supervised learning improves gene expression-based prediction of cancer recurrence.pdf:pdf},
journal = {Bioinformatics},
number = {21},
pages = {3017--3023},
title = {{Semi-supervised learning improves gene expression-based prediction of cancer recurrence}},
url = {http://bioinformatics.oxfordjournals.org/content/27/21/3017.short},
volume = {27},
year = {2011}
}
@article{Rice2010,
author = {Rice, Kenneth},
doi = {10.1198/tast.2010.09060},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rice - 2010 - A Decision-Theoretic Formulation of Fisher’s Approach to Testing.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
month = nov,
number = {4},
pages = {345--349},
title = {{A Decision-Theoretic Formulation of Fisher’s Approach to Testing}},
url = {http://www.tandfonline.com/doi/abs/10.1198/tast.2010.09060},
volume = {64},
year = {2010}
}
@article{Cortes2012,
author = {Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cortes, Mohri, Rostamizadeh - 2012 - Ensembles of kernel predictors.pdf:pdf},
journal = {arXiv preprint},
title = {{Ensembles of kernel predictors}},
url = {http://arxiv.org/abs/1202.3712},
year = {2012}
}
@inproceedings{Goldman2000,
author = {Goldman, Sally and Zhou, Yan},
booktitle = {Proceedings of the 17th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Goldman, Zhou - 2000 - Enhancing supervised learning with unlabeled data.pdf:pdf},
pages = {327--334},
title = {{Enhancing supervised learning with unlabeled data}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Enhancing+Supervised+Learning+with+Unlabeled+Data\#0},
volume = {3},
year = {2000}
}
@techreport{Zhu2005,
author = {Zhu, Xiaojin},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhu - 2005 - Semi-supervised learning literature survey.pdf:pdf},
institution = {University of Wisconsin - Madison},
pages = {1--59},
title = {{Semi-supervised learning literature survey}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.99.9681\&rep=rep1\&type=pdf},
year = {2005}
}
@article{Azizyan2013,
author = {Azizyan, Martin and Singh, Aarti and Wasserman, Larry},
doi = {10.1214/13-AOS1092},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Azizyan, Singh, Wasserman - 2013 - Density-sensitive semisupervised inference.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = apr,
number = {2},
pages = {751--771},
title = {{Density-sensitive semisupervised inference}},
url = {http://projecteuclid.org/euclid.aos/1368018172},
volume = {41},
year = {2013}
}
@article{Hanselmann2008,
abstract = {Imaging mass spectrometry (IMS) is a promising technology which allows for detailed analysis of spatial distributions of (bio)molecules in organic samples. In many current applications, IMS relies heavily on (semi)automated exploratory data analysis procedures to decompose the data into characteristic component spectra and corresponding abundance maps, visualizing spectral and spatial structure. The most commonly used techniques are principal component analysis (PCA) and independent component analysis (ICA). Both methods operate in an unsupervised manner. However, their decomposition estimates usually feature negative counts and are not amenable to direct physical interpretation. We propose probabilistic latent semantic analysis (pLSA) for non-negative decomposition and the elucidation of interpretable component spectra and abundance maps. We compare this algorithm to PCA, ICA, and non-negative PARAFAC (parallel factors analysis) and show on simulated and real-world data that pLSA and non-negative PARAFAC are superior to PCA or ICA in terms of complementarity of the resulting components and reconstruction accuracy. We further combine pLSA decomposition with a statistical complexity estimation scheme based on the Akaike information criterion (AIC) to automatically estimate the number of components present in a tissue sample data set and show that this results in sensible complexity estimates.},
author = {Hanselmann, Michael and Kirchner, Marc and Renard, Bernhard Y and Amstalden, Erika R and Glunde, Kristine and Heeren, Ron M a and Hamprecht, Fred a},
doi = {10.1021/ac801303x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hanselmann et al. - 2008 - Concise representation of mass spectrometry images by probabilistic latent semantic analysis.pdf:pdf},
issn = {1520-6882},
journal = {Analytical chemistry},
keywords = {Algorithms,Breast Neoplasms,Breast Neoplasms: pathology,Computer Simulation,Female,Humans,Image Processing, Computer-Assisted,Mass Spectrometry,Principal Component Analysis,Signal Processing, Computer-Assisted},
month = dec,
number = {24},
pages = {9649--58},
pmid = {18989936},
title = {{Concise representation of mass spectrometry images by probabilistic latent semantic analysis.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18989936},
volume = {80},
year = {2008}
}
@inproceedings{Druck2010,
author = {Druck, Gregory and McCallum, Andrew Kachites},
booktitle = {Proceedings of the 27th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Druck, McCallum - 2010 - High-performance semi-supervised learning using discriminatively constrained generative models.pdf:pdf},
pages = {319--326},
title = {{High-performance semi-supervised learning using discriminatively constrained generative models}},
url = {http://www.cs.umass.edu/~gdruck/pubs/druck10high.pdf http://machinelearning.wustl.edu/mlpapers/paper\_files/icml2010\_DruckM10.pdf},
year = {2010}
}
@article{Hanczar2010a,
abstract = {The receiver operator characteristic (ROC) curves are commonly used in biomedical applications to judge the performance of a discriminant across varying decision thresholds. The estimated ROC curve depends on the true positive rate (TPR) and false positive rate (FPR), with the key metric being the area under the curve (AUC). With small samples these rates need to be estimated from the training data, so a natural question arises: How well do the estimates of the AUC, TPR and FPR compare with the true metrics?},
author = {Hanczar, Blaise and Hua, Jianping and Sima, Chao and Weinstein, John and Bittner, Michael and Dougherty, Edward R.},
doi = {10.1093/bioinformatics/btq037},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hanczar et al. - 2010 - Small-sample precision of ROC-related estimates.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,False Positive Reactions,Oligonucleotide Array Sequence Analysis,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,ROC Curve},
month = mar,
number = {6},
pages = {822--30},
pmid = {20130029},
title = {{Small-sample precision of ROC-related estimates.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20130029},
volume = {26},
year = {2010}
}
@article{Raftery2014a,
author = {Raftery, Adrian E. and Alkema, Leontine and Gerland, Patrick},
doi = {10.1214/13-STS419},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Raftery, Alkema, Gerland - 2014 - Bayesian Population Projections for the United Nations.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Bayesian hierarchical model, cohort component proj,and phrases,bayesian hierarchical model,cohort component,double logistic function,leslie matrix,life expectancy,projection method,total fertility rate},
month = feb,
number = {1},
pages = {58--68},
title = {{Bayesian Population Projections for the United Nations}},
url = {http://projecteuclid.org/euclid.ss/1399645729},
volume = {29},
year = {2014}
}
@article{Skurichina2000,
author = {Skurichina, Marina and Duin, Robert P.W.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Skurichina, Duin - 2000 - The role of combining rules in bagging and boosting.pdf:pdf},
journal = {Advances in Pattern Recognition},
pages = {631--640},
title = {{The role of combining rules in bagging and boosting}},
url = {http://link.springer.com/chapter/10.1007/3-540-44522-6\_65},
year = {2000}
}
@article{Seaman2013,
author = {Seaman, Shaun and Galati, John and Jackson, Dan and Carlin, John},
doi = {10.1214/13-STS415},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Seaman et al. - 2013 - What Is Meant by “Missing at Random”.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Ignorability, direct-likelihood inference, frequen,and phrases,direct-likelihood inference,frequen-,ignorability,missing completely at random,repeated sampling,tist inference},
month = may,
number = {2},
pages = {257--268},
title = {{What Is Meant by “Missing at Random”?}},
url = {http://projecteuclid.org/euclid.ss/1369147915},
volume = {28},
year = {2013}
}
@article{Haffari2012,
author = {Haffari, Gholamreza and Sarkar, Anoop},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Haffari, Sarkar - 2012 - Analysis of semi-supervised learning with the yarowsky algorithm.pdf:pdf},
journal = {arXiv preprint},
title = {{Analysis of semi-supervised learning with the yarowsky algorithm}},
url = {http://arxiv.org/abs/1206.5240},
year = {2012}
}
@article{Chan1997,
author = {Chan, Philip K. and Stolfo, Salvatore J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chan, Stolfo - 1997 - On the accuracy of meta-learning for scalable data mining.pdf:pdf},
journal = {Journal of Intelligent Information Systems},
title = {{On the accuracy of meta-learning for scalable data mining}},
url = {http://www.springerlink.com/index/M27133K052552242.pdf},
year = {1997}
}
@article{Fayyad1996,
author = {Fayyad, Usama and Piatetsky-shapiro, Gregory and Smyth, Padhraic},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fayyad, Piatetsky-shapiro, Smyth - 1996 - From Data Mining to Knowledge Discovery in.pdf:pdf},
pages = {37--54},
title = {{From Data Mining to Knowledge Discovery in}},
year = {1996}
}
@inproceedings{Chaubey2003,
author = {Chaubey, Yogendra P. and Nebebe, Fassil and Sen, Debaraj},
booktitle = {Joint Statistical Meetings},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chaubey, Nebebe, Sen - 2003 - Estimation of Joint Distribution from Marginal Distributions.pdf:pdf},
keywords = {bayesian prediction,because they require multidimensional,ble,contingency ta- methods,dirichlet prior,however,is preferred as it,merical integration,nu-,of the,readily presents an estimate,the bayesian method},
pages = {883--889},
title = {{Estimation of Joint Distribution from Marginal Distributions}},
url = {http://www.amstat.org/sections/SRMS/Proceedings/y2003/Files/JSM2003-000794.pdf},
year = {2003}
}
@article{Chandrasekaran2013,
abstract = {Modern massive datasets create a fundamental problem at the intersection of the computational and statistical sciences: how to provide guarantees on the quality of statistical inference given bounds on computational resources, such as time or space. Our approach to this problem is to define a notion of "algorithmic weakening," in which a hierarchy of algorithms is ordered by both computational efficiency and statistical efficiency, allowing the growing strength of the data at scale to be traded off against the need for sophisticated processing. We illustrate this approach in the setting of denoising problems, using convex relaxation as the core inferential tool. Hierarchies of convex relaxations have been widely used in theoretical computer science to yield tractable approximation algorithms to many computationally intractable tasks. In the current paper, we show how to endow such hierarchies with a statistical characterization and thereby obtain concrete tradeoffs relating algorithmic runtime to amount of data.},
author = {Chandrasekaran, Venkat and Jordan, Michael I},
doi = {10.1073/pnas.1302293110},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chandrasekaran, Jordan - 2013 - Computational and statistical tradeoffs via convex relaxation.pdf:pdf},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
month = mar,
number = {13},
pages = {E1181--90},
pmid = {23479655},
title = {{Computational and statistical tradeoffs via convex relaxation.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3612621\&tool=pmcentrez\&rendertype=abstract},
volume = {110},
year = {2013}
}
@article{Wang2012a,
abstract = {This paper proposes a method to select a set of genes from a large number of genes with the ability of classifying types of diseases. The proposed gene selection method is designed according to correlation analysis and the concept of 95\% reference range. The method is very simple and uses the information of all genes. We have used the method in leukemia patients and achieved good classification results.},
author = {Wang, Xiaodong and Tian, Jun},
doi = {10.1155/2012/586246},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Tian - 2012 - A gene selection method for cancer classification.pdf:pdf},
issn = {1748-6718},
journal = {Computational and mathematical methods in medicine},
keywords = {array,cancer classification,diagnosis,diagnostic tests,dna micro-,drug discovery,feature selection,gene selection,genomics,proteomics,recursive feature elimination,rna expression,support vector machines},
month = jan,
pages = {586246},
pmid = {23251228},
title = {{A gene selection method for cancer classification.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23289441},
volume = {2012},
year = {2012}
}
@inproceedings{Vandewalle2008,
author = {Vandewalle, Vincent and Biernacki, Christophe and Celeux, Gilles and Govaert, Gerard},
booktitle = {vincent.vandewalle.perso.sfr.fr},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vandewalle et al. - 2008 - Are unlabeled data useful in semi-supervised model-based classification Combining hypothesis testing and mode.pdf:pdf},
title = {{Are unlabeled data useful in semi-supervised model-based classification? Combining hypothesis testing and model choice}},
url = {http://vincent.vandewalle.perso.sfr.fr/documents/recherche/articles/vbcg.pdf},
year = {2008}
}
@inproceedings{Ben-David2006,
author = {Ben-David, Shai and Luxburg, Ulrike Von and P\'{a}l, David},
booktitle = {Proceedings of the 19th Annual Conference on Learning Theory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-David, Luxburg, P\'{a}l - 2006 - A sober look at clustering stability.pdf:pdf},
number = {2002},
pages = {5--19},
title = {{A sober look at clustering stability}},
url = {http://link.springer.com/chapter/10.1007/11776420\_4},
year = {2006}
}
@article{Amini2002,
author = {Amini, Massih-Reza and Gallinari, Patrick},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Amini, Gallinari - 2002 - Semi-supervised logistic regression.pdf:pdf},
journal = {15th European Conference on Artificial Intelligence},
keywords = {a discriminative approach to,classifiers,in this paper,its originality is,machine learning,semi-supervised,semi-supervised algorithm,semi-supervised learning,that it relies on,to cope with additional,unlabeled data,we introduce a new},
pages = {390--394},
title = {{Semi-supervised logistic regression}},
url = {http://books.google.com/books?hl=en\&lr=\&id=5ZuuF0ogxU4C\&oi=fnd\&pg=PA390\&dq=Semi-Supervised+Logistic+Regression\&ots=e\_mibnP3HL\&sig=sCM4blfyZ6f8yagmHuev3IZI3uE http://books.google.com/books?hl=en\&lr=\&id=5ZuuF0ogxU4C\&oi=fnd\&pg=PA390\&dq=Semi-supervised+logistic+regression\&ots=e\_nm9lV0LO\&sig=Ocgf21YHZzcL613kNrVoiJsJnaQ},
year = {2002}
}
@inproceedings{Abney2002,
author = {Abney, Steven},
booktitle = {Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Abney - 2002 - Bootstrapping.pdf:pdf},
number = {July},
pages = {360--367},
title = {{Bootstrapping}},
year = {2002}
}
@article{Todorovski2003,
author = {Todorovski, Ljupco and D\v{z}eroski, Saso},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Todorovski, D\v{z}eroski - 2003 - Combining classifiers with meta decision trees.pdf:pdf},
journal = {Machine learning},
keywords = {combining classifiers,decision trees,ensembles of classifiers,meta-level learning,stacking},
pages = {223--249},
title = {{Combining classifiers with meta decision trees}},
url = {http://link.springer.com/article/10.1023/A:1021709817809},
year = {2003}
}
@article{Efron1977,
author = {Efron, Bradley and Morris, Carl},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Efron, Morris - 1977 - Stein's paradox in statistics.pdf:pdf},
journal = {Scientific American},
pages = {119--127},
title = {{Stein's paradox in statistics}},
url = {https://www.cs.nyu.edu/~roweis/csc2515-2006/readings/stein\_sciam.pdf},
year = {1977}
}
@article{Loogc,
author = {Loog, M. and van Ginneken, B.},
doi = {10.1109/ICPR.2002.1048456},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog, van Ginneken - Unknown - Supervised segmentation by iterated contextual pixel classification.pdf:pdf},
isbn = {0-7695-1695-X},
journal = {Object recognition supported by user interaction for service robots},
pages = {925--928},
publisher = {IEEE Comput. Soc},
title = {{Supervised segmentation by iterated contextual pixel classification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1048456},
volume = {2}
}
@inproceedings{Chaudhuri2010,
author = {Chaudhuri, Kamalika and Dasgupta, Sanjoy},
booktitle = {Advances in Neural Information Processing Systems 23},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chaudhuri, Dasgupta - 2010 - Rates of convergence for the cluster tree.pdf:pdf},
pages = {343--351},
title = {{Rates of convergence for the cluster tree}},
url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/NIPS2010\_0496.pdf},
year = {2010}
}
@article{Liu2013,
author = {Liu, J. and Gelman, a. and Hill, J. and Su, Y.-S. and Kropko, J.},
doi = {10.1093/biomet/ast044},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Liu et al. - 2013 - On the stationary distribution of iterative imputations.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = nov,
number = {1},
pages = {155--173},
title = {{On the stationary distribution of iterative imputations}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/ast044},
volume = {101},
year = {2013}
}
@article{JuliaFlores2013,
author = {{Julia Flores}, M. and G\'{a}mez, Jos\'{e} a. and Mart\'{\i}nez, Ana M.},
doi = {10.1016/j.ins.2013.10.007},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Julia Flores, G\'{a}mez, Mart\'{\i}nez - 2013 - Domains of competence of the semi-naive Bayesian network classifiers.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {domains of competence,semi-naive bayesian network classifiers},
month = oct,
number = {October},
publisher = {Elsevier Inc.},
title = {{Domains of competence of the semi-naive Bayesian network classifiers}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0020025513007226},
year = {2013}
}
@article{Lattimore2011,
archivePrefix = {arXiv},
arxivId = {1111.3846},
author = {Lattimore, Tor and Hutter, Marcus},
eprint = {1111.3846},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lattimore, Hutter - 2011 - No Free Lunch versus Occam's Razor in Supervised Learning.pdf:pdf},
journal = {arXiv preprint},
keywords = {kolmogorov complexity,no free lunch,occam,s razor,supervised learning},
title = {{No Free Lunch versus Occam's Razor in Supervised Learning}},
url = {http://arxiv.org/abs/1111.3846},
year = {2011}
}
@article{Claassen2010,
author = {Claassen, Tom and Heskes, Tom},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Claassen, Heskes - 2010 - Learning causal network structure from multiple (in) dependence models.pdf:pdf},
journal = {Proceedings of the Fifth European Workshop on Probabilistic Graphical Models},
title = {{Learning causal network structure from multiple (in) dependence models}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.173.254\&rep=rep1\&type=pdf\#page=91},
year = {2010}
}
@article{Nigam2000,
author = {Nigam, Kamal and McCallum, Andrew Kachites and Thrun, Sebastian and Mitchell, Tom},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nigam et al. - 2000 - Text classification from labeled and unlabeled documents using EM.pdf:pdf},
journal = {Machine learning},
keywords = {bayesian learning,combining labeled and unlabeled,data,expectation-maximization,integrating supervised and unsuper-,text classification,vised learning},
pages = {1--34},
title = {{Text classification from labeled and unlabeled documents using EM}},
url = {http://link.springer.com/article/10.1023/A:1007692713085},
volume = {34},
year = {2000}
}
@article{Hong2013,
author = {Hong, Yongmiao and Lee, Yoon-Jin},
doi = {10.1214/13-AOS1099},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hong, Lee - 2013 - A loss function approach to model specification testing and its relative efficiency.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = jun,
number = {3},
pages = {1166--1203},
title = {{A loss function approach to model specification testing and its relative efficiency}},
url = {http://projecteuclid.org/euclid.aos/1371150897},
volume = {41},
year = {2013}
}
@inproceedings{Collins1999,
author = {Collins, Michael and Singer, Yoram},
booktitle = {Proceedings of the joint SIGDAT conference on empirical methods in natural language processing and very large corpora},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Collins, Singer - 1999 - Unsupervised models for named entity classification.pdf:pdf},
pages = {189--196},
title = {{Unsupervised models for named entity classification}},
url = {http://acl.ldc.upenn.edu/W/W99/W99-0613.pdf?ref=Sawos.OrgR\%7B.\%EF\%BF\%BD\%EF\%BF\%BD\%EF\%BF\%BD\%EF\%BF\%BD\%C7\%9D\%EF\%BF\%BD\%E2\%80\%A1\%5E\%EF\%BF\%BD\%EF\%BF\%BD\%C3\%A87},
year = {1999}
}
@article{Stigler2013,
author = {Stigler, Stephen M.},
doi = {10.1214/13-STS438},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Stigler - 2013 - The True Title of Bayes’s Essay.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Thomas Bayes, Richard Price, Bayes's theorem, hist,and phrases},
month = aug,
number = {3},
pages = {283--288},
title = {{The True Title of Bayes’s Essay}},
url = {http://projecteuclid.org/euclid.ss/1377696937},
volume = {28},
year = {2013}
}
@inproceedings{Taigman2014,
author = {Taigman, Yaniv and Ranzato, Marc Aurelio and Aviv, Tel and Park, Menlo},
booktitle = {Computer Vision and Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Taigman et al. - 2014 - DeepFace Closing the Gap to Human-Level Performance in Face Verification.pdf:pdf},
title = {{DeepFace : Closing the Gap to Human-Level Performance in Face Verification}},
year = {2014}
}
@inproceedings{Zhang2000a,
author = {Zhang, Tong and Oles, Frank J.},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang, Oles - 2000 - The value of unlabeled data for classification problems.pdf:pdf},
pages = {1191--1198},
title = {{The value of unlabeled data for classification problems}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.20.6025\&rep=rep1\&type=pdf},
year = {2000}
}
@article{Pitt1988,
author = {Pitt, Leonard and Valiant, Leslie G.},
doi = {10.1145/48014.63140},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pitt, Valiant - 1988 - Computational limitations on learning from examples.pdf:pdf},
issn = {00045411},
journal = {Journal of the ACM},
month = oct,
number = {4},
pages = {965--984},
title = {{Computational limitations on learning from examples}},
url = {http://portal.acm.org/citation.cfm?doid=48014.63140},
volume = {35},
year = {1988}
}
@inproceedings{Auger2007,
author = {Auger, Anne and Teytaud, Olivier},
booktitle = {Proceedings of the 9th annual conference on Genetic and evolutionary computation},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Auger, Teytaud - 2007 - Continuous lunches are free!.pdf:pdf},
isbn = {9781595936974},
keywords = {free-lunch,kolmogorov,no-free-lunch,s extension theo-},
pages = {916--922},
title = {{Continuous lunches are free!}},
url = {http://dl.acm.org/citation.cfm?id=1277145},
year = {2007}
}
@article{Stahlecker1996,
author = {Stahlecker, Peter and Knautz, Henning and Trenkler, Gotz},
doi = {10.1007/BF00046994},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Stahlecker, Knautz, Trenkler - 1996 - Minimax adjustment technique in a parameter restricted linear model.pdf:pdf},
issn = {0167-8019},
journal = {Acta Applicandae Mathematicae},
keywords = {linear regression,minimax adjustment,projection estimator},
month = apr,
number = {1},
pages = {139--144},
title = {{Minimax adjustment technique in a parameter restricted linear model}},
url = {http://link.springer.com/10.1007/BF00046994},
volume = {43},
year = {1996}
}
@inproceedings{Caticha2006,
archivePrefix = {arXiv},
arxivId = {arXiv:physics/0608185v1},
author = {Caticha, Ariel and Giffin, Adom},
booktitle = {Bayesian Inference and Maximum Entropy Methods In Science and Engineering},
eprint = {0608185v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Caticha, Giffin - 2006 - Updating Probabilities.pdf:pdf},
pages = {31--42},
primaryClass = {arXiv:physics},
title = {{Updating Probabilities}},
url = {http://arxiv.org/abs/physics/0608185},
volume = {872},
year = {2006}
}
@article{Wang2007,
author = {Wang, Junhui and Shen, Xiaotong},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Shen - 2007 - Large margin semi-supervised learning.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {generalization,grouping,sequential quadratic programming,support vectors},
pages = {1867--1891},
title = {{Large margin semi-supervised learning}},
url = {http://jmlr.csail.mit.edu/papers/volume8/wang07a/wang07a.pdf},
volume = {8},
year = {2007}
}
@article{Lin2012,
address = {New York, New York, USA},
author = {Lin, Jimmy and Kolcz, Alek},
doi = {10.1145/2213836.2213958},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lin, Kolcz - 2012 - Large-scale machine learning at twitter.pdf:pdf},
isbn = {9781450312479},
journal = {Proceedings of the 2012 international conference on Management of Data - SIGMOD '12},
keywords = {en-,logistic regression,online learning,sembles,stochastic gradient descent},
pages = {793},
publisher = {ACM Press},
title = {{Large-scale machine learning at twitter}},
url = {http://dl.acm.org/citation.cfm?doid=2213836.2213958},
year = {2012}
}
@phdthesis{Lu2009,
author = {Lu, Tyler},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lu - 2009 - Fundamental Limitations of Semi-Supervised Learning.pdf:pdf},
title = {{Fundamental Limitations of Semi-Supervised Learning}},
year = {2009}
}
@inproceedings{Duchi2008,
author = {Duchi, John and Shalev-Shwartz, Shai},
booktitle = {Proceedings of the 25th international conference on Machine learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Duchi, Shalev-Shwartz - 2008 - Efficient Projections onto the ℓ1-Ball for Learning in High Dimensions.pdf:pdf},
pages = {272--279},
title = {{Efficient Projections onto the ℓ1-Ball for Learning in High Dimensions}},
url = {http://dl.acm.org/citation.cfm?id=1390191},
year = {2008}
}
@article{Shafer2008,
author = {Shafer, Glenn and Vovk, Vladimir},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shafer, Vovk - 2008 - A tutorial on conformal prediction.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {confidence,on-line compression modeling,on-line learning,prediction regions},
pages = {371--421},
title = {{A tutorial on conformal prediction}},
url = {http://dl.acm.org/citation.cfm?id=1390693},
volume = {9},
year = {2008}
}
@inproceedings{Fan2008,
author = {Fan, Bin and Lei, Zhen and Li, Stan Z.},
booktitle = {8th IEEE International Conference on Automatic Face \& Gesture Recognition},
doi = {10.1109/AFGR.2008.4813329},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fan, Lei, Li - 2008 - Normalized LDA for semi-supervised learning.pdf:pdf},
isbn = {978-1-4244-2153-4},
month = sep,
pages = {1--6},
title = {{Normalized LDA for semi-supervised learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4813329},
year = {2008}
}
@inproceedings{Ho2008,
author = {Ho, Tin Kam},
booktitle = {Proceedings of the 2008 Joint IAPR International Workshop on Structural, Syntactic, and Statistical Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho - 2008 - Data complexity analysis Linkage between context and solution in classification.pdf:pdf},
pages = {986--995},
title = {{Data complexity analysis: Linkage between context and solution in classification}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-89689-0\_102},
year = {2008}
}
@article{VonHippel2007,
author = {von Hippel, Paul T.},
doi = {10.1111/j.1467-9531.2007.00180.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/von Hippel - 2007 - Regression With Missing Ys an Improved Strategy for Analyzing Multiply Imputed Data.pdf:pdf},
issn = {0081-1750},
journal = {Sociological Methodology},
month = dec,
number = {1},
pages = {83--117},
title = {{Regression With Missing Ys: an Improved Strategy for Analyzing Multiply Imputed Data}},
url = {http://smx.sagepub.com/lookup/doi/10.1111/j.1467-9531.2007.00180.x},
volume = {37},
year = {2007}
}
@inproceedings{Suzuki2008,
author = {Suzuki, Jun and Isozaki, Hideki},
booktitle = {ACL},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Suzuki, Isozaki - 2008 - Semi-Supervised Sequential Labeling and Segmentation Using Giga-Word Scale Unlabeled Data.pdf:pdf},
pages = {665--673},
title = {{Semi-Supervised Sequential Labeling and Segmentation Using Giga-Word Scale Unlabeled Data.}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.164.5597\&rep=rep1\&type=pdf},
year = {2008}
}
@article{Wang2012,
abstract = {MOTIVATION: Epistasis or gene-gene interaction has gained increasing attention in studies of complex diseases. Its presence as an ubiquitous component of genetic architecture of common human diseases has been contemplated. However, the detection of gene-gene interaction is difficult due to combinatorial explosion. RESULTS: We present a novel feature selection method incorporating variable interaction. Three gene expression datasets are analyzed to illustrate our method, although it can also be applied to other types of high-dimensional data. The quality of variables selected is evaluated in two ways: first by classification error rates, then by functional relevance assessed using biological knowledge. We show that the classification error rates can be significantly reduced by considering interactions. Secondly, a sizable portion of genes identified by our method for breast cancer metastasis overlaps with those reported in gene-to-system breast cancer (G2SBC) database as disease associated and some of them have interesting biological implication. In summary, interaction-based methods may lead to substantial gain in biological insights as well as more accurate prediction.},
author = {Wang, Haitian and Lo, Shaw-Hwa and Zheng, Tian and Hu, Inchi},
doi = {10.1093/bioinformatics/bts531},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang et al. - 2012 - Interaction-based feature selection and classification for high-dimensional biological data.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
month = nov,
number = {21},
pages = {2834--42},
pmid = {22945786},
title = {{Interaction-based feature selection and classification for high-dimensional biological data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22945786},
volume = {28},
year = {2012}
}
@inproceedings{Airola2010,
author = {Airola, Antti and Pahikkala, Tapio and Boberg, Jorma and Salakoski, Tapio},
booktitle = {Ninth International Conference on Machine Learning and Applications},
doi = {10.1109/ICMLA.2010.158},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Airola et al. - 2010 - Applying Permutation Tests for Assessing the Statistical Significance of Wrapper Based Feature Selection.pdf:pdf},
isbn = {978-1-4244-9211-4},
month = dec,
pages = {989--994},
publisher = {Ieee},
title = {{Applying Permutation Tests for Assessing the Statistical Significance of Wrapper Based Feature Selection}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5708982},
year = {2010}
}
@article{Wasserman2009,
author = {Wasserman, Larry and Roeder, Kathryn},
doi = {10.1214/08-AOS646},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wasserman, Roeder - 2009 - High dimensional variable selection.pdf:pdf},
journal = {Annals of statistics},
number = {5},
pages = {2178--2201},
title = {{High dimensional variable selection}},
url = {http://www.ncbi.nlm.nih.gov/pmc/articles/pmc2752029/},
volume = {37},
year = {2009}
}
@article{Johnson2013,
author = {Johnson, Valen E.},
doi = {10.1214/13-AOS1123},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Johnson - 2013 - Uniformly most powerful Bayesian tests.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {bayes factor,higgs,ily model,jeffreys-lindley paradox,neyman-pearson lemma,non-local prior density,objective bayes,one-parameter exponential fam-,uniformly most powerful test},
month = aug,
number = {4},
pages = {1716--1741},
title = {{Uniformly most powerful Bayesian tests}},
url = {http://projecteuclid.org/euclid.aos/1378386237},
volume = {41},
year = {2013}
}
@inproceedings{Lawrence2004,
author = {Lawrence, Neil D. and Jordan, Michael I.},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lawrence, Jordan - 2004 - Semi-supervised learning via Gaussian processes.pdf:pdf},
pages = {753--760},
title = {{Semi-supervised learning via Gaussian processes}},
url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/NIPS2005\_257.pdf},
year = {2004}
}
@inproceedings{Bernad2004,
author = {Bernad\'{o}-Mansilla, Ester and Ho, Tin Kam},
booktitle = {Proceedings of the 17th International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bernad\'{o}-Mansilla, Ho - 2004 - On classifier domains of competence.pdf:pdf},
title = {{On classifier domains of competence}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1334026},
year = {2004}
}
@inproceedings{Klinkenberg2001,
author = {Klinkenberg, Ralf},
booktitle = {Workshop notes of the IJCAI-01 Workshop on Learning from Temporal and Spatial Data},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Klinkenberg - 2001 - Using labeled and unlabeled data to learn drifting concepts.pdf:pdf},
pages = {16--24},
title = {{Using labeled and unlabeled data to learn drifting concepts}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.23.1798\&rep=rep1\&type=pdf},
year = {2001}
}
@techreport{Krijthe2013,
author = {Krijthe, Jesse H and Loog, Marco},
booktitle = {Under review},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Krijthe, Loog - 2013 - Implicitly Constrained Semi-Supervised Least Squares Classification.pdf:pdf},
keywords = {constrained,least squares classification,semi-supervised learning},
title = {{Implicitly Constrained Semi-Supervised Least Squares Classification}},
url = {www.jessekrijthe.com/papers/krijthe2013.pdf},
year = {2013}
}
@article{Gelman2012a,
author = {Gelman, Andrew and Hill, Jennifer and Yajima, Masanao},
doi = {10.1080/19345747.2011.618213},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Hill, Yajima - 2012 - Why We (Usually) Don't Have to Worry About Multiple Comparisons.pdf:pdf},
issn = {1934-5747},
journal = {Journal of Research on Educational Effectiveness},
keywords = {bayesian inference,hierarchical modeling,multiple comparisons,statis-,type s error},
month = apr,
number = {2},
pages = {189--211},
title = {{Why We (Usually) Don't Have to Worry About Multiple Comparisons}},
url = {http://www.tandfonline.com/doi/abs/10.1080/19345747.2011.618213},
volume = {5},
year = {2012}
}
@article{Tibshirani2001,
author = {Tibshirani, Robert and Walther, Guenther and Hastie, Trevor},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tibshirani, Walther, Hastie - 2001 - Estimating the number of clusters in a data set via the gap statistic.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B, Statistical methodology},
pages = {411--423},
title = {{Estimating the number of clusters in a data set via the gap statistic}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/1467-9868.00293/abstract},
volume = {63},
year = {2001}
}
@article{Ben-david2011,
author = {Ben-david, Shai and Srebro, Nati and Urner, Ruth},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-david, Srebro, Urner - 2011 - Is learning possible without Prior Knowledge Do Universal Learners exist High level view of ( Statis.pdf:pdf},
title = {{Is learning possible without Prior Knowledge ? Do Universal Learners exist ? High level view of ( Statistical ) Machine Learning}},
year = {2011}
}
@article{Shevade2003,
author = {Shevade, S. K. and Keerthi, S. S.},
doi = {10.1093/bioinformatics/btg308},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shevade, Keerthi - 2003 - A simple and efficient algorithm for gene selection using sparse logistic regression.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
month = nov,
number = {17},
pages = {2246--2253},
title = {{A simple and efficient algorithm for gene selection using sparse logistic regression}},
url = {http://bioinformatics.oxfordjournals.org/cgi/doi/10.1093/bioinformatics/btg308},
volume = {19},
year = {2003}
}
@article{Brazdil2003a,
author = {Brazdil, Pavel B. and Soares, Carlos and Costa, JP Da},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brazdil, Soares, Costa - 2003 - Ranking learning algorithms Using IBL and meta-learning on accuracy and time results.pdf:pdf},
journal = {Machine Learning},
keywords = {algorithm recommendation,data characterization,meta-learning,ranking},
pages = {251--277},
title = {{Ranking learning algorithms: Using IBL and meta-learning on accuracy and time results}},
url = {http://link.springer.com/article/10.1023/A:1021713901879},
volume = {50},
year = {2003}
}
@article{Xiao2012,
author = {Xiao, Yuanhui and Song, Ruiguang and Chen, Mi and Hall, H. Irene},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Xiao et al. - 2012 - Direct and Unbiased Multiple Imputation Methods for Missing Values of Categorical Variables.pdf:pdf},
journal = {Journal of Data Science},
keywords = {bias,categorical variable,hiv,missing values,multiple impu-},
pages = {465--481},
title = {{Direct and Unbiased Multiple Imputation Methods for Missing Values of Categorical Variables}},
url = {http://www.jdsruc.org/upload/7(2012-07-01170715).pdf},
volume = {10},
year = {2012}
}
@article{Yarowsky1995,
address = {Morristown, NJ, USA},
author = {Yarowsky, David},
doi = {10.3115/981658.981684},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yarowsky - 1995 - Unsupervised word sense disambiguation rivaling supervised methods.pdf:pdf},
journal = {Proceedings of the 33rd annual meeting on Association for Computational Linguistics},
pages = {189--196},
publisher = {Association for Computational Linguistics},
title = {{Unsupervised word sense disambiguation rivaling supervised methods}},
url = {http://portal.acm.org/citation.cfm?doid=981658.981684},
year = {1995}
}
@article{VanderMaaten2007,
author = {{Van der Maaten}, Laurens},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Van der Maaten - 2007 - An Introduction to Dimensionality Reduction Using Matlab.pdf:pdf},
number = {July},
title = {{An Introduction to Dimensionality Reduction Using Matlab}},
year = {2007}
}
@article{Uh2012,
author = {Uh, H and Mertens, B J A and Beekman, M.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Uh, Mertens, Beekman - 2012 - Search for multiple biomarkers predicting longevity in families weighted penalized logistic regression.pdf:pdf},
pages = {1--22},
title = {{Search for multiple biomarkers predicting longevity in families : weighted penalized logistic regression}},
year = {2012}
}
@article{Spirtes2010,
author = {Pearl, Judea},
doi = {10.2202/1557-4679.1203},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pearl - 2010 - Introduction to causal inference.pdf:pdf},
issn = {1557-4679},
journal = {Journal of Machine Learning Research},
month = jan,
pages = {1643--1662},
pmid = {20305706},
title = {{Introduction to causal inference}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2836213\&tool=pmcentrez\&rendertype=abstract},
volume = {11},
year = {2010}
}
@article{Giraud-carrier2004,
author = {Giraud-carrier, Christophe and Vilalta, Ricardo and Brazdil, Pavel B.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Giraud-carrier, Vilalta, Brazdil - 2004 - Introduction to the special issue on meta-learning.pdf:pdf},
journal = {Machine learning},
keywords = {dynamic bias selection,inductive bias,meta-knowledge,meta-learning},
pages = {187--193},
title = {{Introduction to the special issue on meta-learning}},
url = {http://link.springer.com/article/10.1023/B:MACH.0000015878.60765.42},
volume = {54},
year = {2004}
}
@article{Guo2010,
author = {Guo, Yuanyuan and Niu, Xiaoda and Zhang, Harry},
doi = {10.1109/ICDM.2010.66},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Guo, Niu, Zhang - 2010 - An Extensive Empirical Study on Semi-supervised Learning.pdf:pdf},
isbn = {978-1-4244-9131-5},
journal = {IEEE International Conference on Data Mining},
keywords = {-semi-supervised learning,bayesian classifiers},
month = dec,
pages = {186--195},
publisher = {Ieee},
title = {{An Extensive Empirical Study on Semi-supervised Learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5693972},
year = {2010}
}
@article{Blanchard2010,
author = {Blanchard, Gilles and Lee, Gyemin and Scott, Clayton},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Blanchard, Lee, Scott - 2010 - Semi-supervised novelty detection.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {2973--3009},
title = {{Semi-supervised novelty detection}},
url = {http://dl.acm.org/citation.cfm?id=1953028},
volume = {11},
year = {2010}
}
@article{Sun2013,
author = {Sun, Quan and Pfahringer, Bernhard},
doi = {10.1007/s10994-013-5387-y},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sun, Pfahringer - 2013 - Pairwise meta-rules for better meta-learning-based algorithm ranking.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {algorithm ranking,ensemble learning,meta-learning,ranking trees},
month = jul,
number = {1},
pages = {141--161},
title = {{Pairwise meta-rules for better meta-learning-based algorithm ranking}},
url = {http://link.springer.com/10.1007/s10994-013-5387-y},
volume = {93},
year = {2013}
}
@article{Domingos2012,
author = {Domingos, Pedro},
doi = {10.1145/2347736.2347755},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Domingos - 2012 - A few useful things to know about machine learning.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
month = oct,
number = {10},
pages = {78},
title = {{A few useful things to know about machine learning}},
url = {http://dl.acm.org/citation.cfm?doid=2347736.2347755},
volume = {55},
year = {2012}
}
@article{Homrighausen2014,
author = {Homrighausen, Darren and McDonald, Daniel J.},
doi = {10.1007/s10994-014-5438-z},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Homrighausen, McDonald - 2014 - Leave-one-out cross-validation is risk consistent for lasso.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {persistence,stochastic equicontinuity,uniform convergence},
month = mar,
number = {March 2013},
title = {{Leave-one-out cross-validation is risk consistent for lasso}},
url = {http://link.springer.com/10.1007/s10994-014-5438-z},
year = {2014}
}
@article{Douc2012,
author = {Douc, Randal and Moulines, Eric},
doi = {10.1214/12-AOS1047},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Douc, Moulines - 2012 - Asymptotic properties of the maximum likelihood estimation in misspecified hidden Markov models.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {and phrases,hidden markov models,mator,maximum likelihood esti-,misspecified models,state space models,strong consistency},
month = oct,
number = {5},
pages = {2697--2732},
title = {{Asymptotic properties of the maximum likelihood estimation in misspecified hidden Markov models}},
url = {http://projecteuclid.org/euclid.aos/1359987535},
volume = {40},
year = {2012}
}
@inproceedings{Niu2013,
author = {Niu, G and Jitkrittum, W and Dai, Bo and Hachiya, H and Sugiyama, M},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Niu et al. - 2013 - Squared-loss Mutual Information Regularization A Novel Information-theoretic Approach to Semi-supervised Learning.pdf:pdf},
pages = {10--18},
title = {{Squared-loss Mutual Information Regularization: A Novel Information-theoretic Approach to Semi-supervised Learning}},
url = {http://sugiyama-www.cs.titech.ac.jp/~gang/paper/niu\_icml13.pdf},
year = {2013}
}
@article{Browner1987,
abstract = {Just as diagnostic tests are most helpful in light of the clinical presentation, statistical tests are most useful in the context of scientific knowledge. Knowing the specificity and sensitivity of a diagnostic test is necessary, but insufficient: the clinician must also estimate the prior probability of the disease. In the same way, knowing the P value and power, or the confidence interval, for the results of a research study is necessary but insufficient: the reader must estimate the prior probability that the research hypothesis is true. Just as a positive diagnostic test does not mean that a patient has the disease, especially if the clinical picture suggests otherwise, a significant P value does not mean that a research hypothesis is correct, especially if it is inconsistent with current knowledge. Powerful studies are like sensitive tests in that they can be especially useful when the results are negative. Very low P values are like very specific tests; both result in few false-positive results due to chance. This Bayesian approach can clarify much of the confusion surrounding the use and interpretation of statistical tests.},
author = {Browner, W S and Newman, T B},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Browner, Newman - 1987 - Are all significant P values created equal The analogy between diagnostic tests and clinical research.pdf:pdf},
issn = {0098-7484},
journal = {JAMA : the journal of the American Medical Association},
keywords = {Bayes Theorem,Predictive Value of Tests,Research,Statistics as Topic},
month = may,
number = {18},
pages = {2459--63},
pmid = {3573245},
title = {{Are all significant P values created equal? The analogy between diagnostic tests and clinical research.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/3573245},
volume = {257},
year = {1987}
}
@inproceedings{Ho2000,
author = {Ho, Tin Kam},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho - 2000 - Complexity of Classification Problems and Comparative Advantages of Combined Classifiers.pdf:pdf},
pages = {97--106},
title = {{Complexity of Classification Problems and Comparative Advantages of Combined Classifiers}},
year = {2000}
}
@article{Kalousis1999,
author = {Kalousis, Alexis and Theoharis, T},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kalousis, Theoharis - 1999 - NOEMON An intelligent Assistant for Classifier Selection.pdf:pdf},
journal = {Intelligent Data Analysis},
keywords = {classifier comparison,classifier selection,dataset morphology,multidimensional metrics},
number = {5},
pages = {319--337},
title = {{NOEMON: An intelligent Assistant for Classifier Selection}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.38.7762},
volume = {3},
year = {1999}
}
@article{Bengio2007,
author = {Bengio, Yoshua and LeCun, Yann},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bengio, LeCun - 2007 - Scaling Learning Algorithms towards AI.pdf:pdf},
journal = {Large-Scale Kernel Machines},
number = {1},
pages = {1--41},
title = {{Scaling Learning Algorithms towards AI}},
url = {http://www.iro.umontreal.ca/~lisa/bib/pub\_subject/language/pointeurs/bengio+lecun-chapter2007.pdf},
year = {2007}
}
@misc{Czepiel2002,
author = {Czepiel, SA},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Czepiel - 2002 - Maximum likelihood estimation of logistic regression models theory and implementation.pdf:pdf},
title = {{Maximum likelihood estimation of logistic regression models: theory and implementation}},
url = {http://www.czep.net/stat/mlelr.pdf},
year = {2002}
}
@article{Taylor1977,
author = {Taylor, Publisher and Mclachlan, Geoffrey John},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Taylor, Mclachlan - 1977 - Estimating the Linear Discriminant Function from Initial Samples Containing a Small Number of Unclassified Ob.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {completely classified,conditional and expected error,initial samples in-,m j f j,mi,n,rates,sample discriminant functions},
number = {358},
pages = {403--406},
title = {{Estimating the Linear Discriminant Function from Initial Samples Containing a Small Number of Unclassified Observations}},
volume = {72},
year = {1977}
}
@article{Ting1997,
author = {Ting, Kai Ming and Witten, Ian H.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ting, Witten - 1997 - Stacked Generalization when does it work.pdf:pdf},
title = {{Stacked Generalization: when does it work?}},
url = {http://researchcommons.waikato.ac.nz/handle/10289/1066},
year = {1997}
}
@article{DeRidder2004,
author = {de Ridder, D. and Loog, Marco and Reinders, M.J.T.},
doi = {10.1109/ICPR.2004.1334176},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ridder, Loog, Reinders - 2004 - Local Fisher embedding.pdf:pdf},
isbn = {0-7695-2128-2},
journal = {Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.},
number = {3},
pages = {295--298 Vol.2},
publisher = {Ieee},
title = {{Local Fisher embedding}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1334176},
year = {2004}
}
@article{Talwalkar2013,
author = {Talwalkar, Ameet and Kumar, Sanjiv and Mohri, Mehryar and Rowley, Henry},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Talwalkar et al. - 2013 - Large-scale SVD and manifold learning.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {3129--3152},
title = {{Large-scale SVD and manifold learning}},
url = {http://dl.acm.org/citation.cfm?id=2567761},
volume = {14},
year = {2013}
}
@inproceedings{Carroll2007,
author = {Carroll, James L. and Seppi, Kevin D.},
booktitle = {IJCNN Workshop on Meta-Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Carroll, Seppi - 2007 - No-free-lunch and Bayesian optimality.pdf:pdf},
title = {{No-free-lunch and Bayesian optimality}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.142.7564\&rep=rep1\&type=pdf},
year = {2007}
}
@inproceedings{Pranckeviciene2006,
author = {Pranckeviciene, Erinija and Ho, Tin Kam and Somorjai, Ray},
booktitle = {International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pranckeviciene, Ho, Somorjai - 2006 - Class separability in spaces reduced by feature selection.pdf:pdf},
title = {{Class separability in spaces reduced by feature selection}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1699514},
year = {2006}
}
@inproceedings{Joachims2003,
author = {Joachims, Thorsten},
booktitle = {Proceedings of the 20th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Joachims - 2003 - Transductive learning via spectral graph partitioning.pdf:pdf},
pages = {290--297},
title = {{Transductive learning via spectral graph partitioning}},
url = {http://www.aaai.org/Papers/ICML/2003/ICML03-040.pdf},
year = {2003}
}
@inproceedings{Grandvalet2005,
address = {Cambridge, MA},
author = {Grandvalet, Yves and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems 17},
editor = {Saul, L. K. and Weiss, Y. and Bottou, L.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Grandvalet, Bengio - 2005 - Semi-supervised learning by entropy minimization.pdf:pdf},
pages = {529--536},
publisher = {MIT Press},
title = {{Semi-supervised learning by entropy minimization}},
url = {http://eprints.pascal-network.org/archive/00001978/},
year = {2005}
}
@article{Susko2013,
author = {Susko, E.},
doi = {10.1093/biomet/ast032},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Susko - 2013 - Likelihood ratio tests with boundary constraints using data-dependent degrees of freedom.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = aug,
number = {4},
pages = {1019--1023},
title = {{Likelihood ratio tests with boundary constraints using data-dependent degrees of freedom}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/ast032},
volume = {100},
year = {2013}
}
@inproceedings{Latinne2001,
author = {Latinne, Patrice and Debeir, Olivier and Decaestecker, Christine},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Latinne, Debeir, Decaestecker - 2001 - Limiting the Number of Trees in Random Forests.pdf:pdf},
title = {{Limiting the Number of Trees in Random Forests}},
year = {2001}
}
@inproceedings{Zhou2007a,
author = {Zhou, Zhi-hua and Xu, Jun-Ming},
booktitle = {Proceedings of the 24th International Conference on Machine learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Xu - 2007 - On the relation between multi-instance learning and semi-supervised learning.pdf:pdf},
number = {1997},
title = {{On the relation between multi-instance learning and semi-supervised learning}},
url = {http://dl.acm.org/citation.cfm?id=1273643},
year = {2007}
}
@article{Molinaro2005,
abstract = {In genomic studies, thousands of features are collected on relatively few samples. One of the goals of these studies is to build classifiers to predict the outcome of future observations. There are three inherent steps to this process: feature selection, model selection and prediction assessment. With a focus on prediction assessment, we compare several methods for estimating the 'true' prediction error of a prediction model in the presence of feature selection.},
author = {Molinaro, Annette M and Simon, Richard and Pfeiffer, Ruth M},
doi = {10.1093/bioinformatics/bti499},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Molinaro, Simon, Pfeiffer - 2005 - Prediction error estimation a comparison of resampling methods.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Computer Simulation,Data Interpretation, Statistical,Gene Expression Profiling,Gene Expression Profiling: methods,Models, Genetic,Models, Statistical,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Reproducibility of Results,Sample Size,Sensitivity and Specificity,Software},
month = aug,
number = {15},
pages = {3301--7},
pmid = {15905277},
title = {{Prediction error estimation: a comparison of resampling methods.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15905277},
volume = {21},
year = {2005}
}
@article{Yan2013,
author = {Yan, Yan and Rosales, R\'{o}mer and Fung, Glenn and Subramanian, Ramanathan and Dy, Jennifer},
doi = {10.1007/s10994-013-5412-1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yan et al. - 2013 - Learning from multiple annotators with varying expertise.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = oct,
title = {{Learning from multiple annotators with varying expertise}},
url = {http://link.springer.com/10.1007/s10994-013-5412-1},
year = {2013}
}
@misc{Tibshirani,
author = {Tibshirani, Robert},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tibshirani - Unknown - Machine Learning vs. Statistics.pdf:pdf},
title = {{Machine Learning vs. Statistics}}
}
@inproceedings{Jordan2002,
abstract = {We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widely held belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better. This stems from the observation - which is borne out in repeated experiments - that while discriminative learning has lower asymptotic error, a generative classifier may also approach its (higher) asymptotic error much faster.},
author = {Jordan, Michael I. and Ng, Andrew Y},
booktitle = {Advances in neural information processing systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jordan, Ng - 2002 - On Discriminative vs. Generative classifiers comparison of logistic regression and naive Bayes.pdf:pdf},
number = {14},
pages = {841--848},
title = {{On Discriminative vs. Generative classifiers: comparison of logistic regression and naive Bayes}},
url = {http://books.google.com/books?hl=en\&lr=\&id=GbC8cqxGR7YC\&oi=fnd\&pg=PA841\&dq=On+Discriminative+vs.+Generative+classifiers:+comparison+of+logistic+regression+and+naive+Bayes\&ots=ZvO0F2\_vx9\&sig=0nMLd-CWMsb8-jyrI6YetIH6ZZU},
volume = {2},
year = {2002}
}
@article{Weston2005,
abstract = {MOTIVATION: Building an accurate protein classification system depends critically upon choosing a good representation of the input sequences of amino acids. Recent work using string kernels for protein data has achieved state-of-the-art classification performance. However, such representations are based only on labeled data--examples with known 3D structures, organized into structural classes--whereas in practice, unlabeled data are far more plentiful. RESULTS: In this work, we develop simple and scalable cluster kernel techniques for incorporating unlabeled data into the representation of protein sequences. We show that our methods greatly improve the classification performance of string kernels and outperform standard approaches for using unlabeled data, such as adding close homologs of the positive examples to the training data. We achieve equal or superior performance to previously presented cluster kernel methods and at the same time achieving far greater computational efficiency. AVAILABILITY: Source code is available at www.kyb.tuebingen.mpg.de/bs/people/weston/semiprot. The Spider matlab package is available at www.kyb.tuebingen.mpg.de/bs/people/spider. SUPPLEMENTARY INFORMATION: www.kyb.tuebingen.mpg.de/bs/people/weston/semiprot.},
author = {Weston, Jason and Leslie, Christina and Ie, Eugene and Zhou, Dengyong and Elisseeff, Andre and Noble, William Stafford},
doi = {10.1093/bioinformatics/bti497},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Weston et al. - 2005 - Semi-supervised protein classification using cluster kernels.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Cluster Analysis,Pattern Recognition,Protein,Protein: methods,Proteins,Proteins: analysis,Proteins: chemistry,Proteins: classification,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis,Software},
month = aug,
number = {15},
pages = {3241--7},
pmid = {15905279},
title = {{Semi-supervised protein classification using cluster kernels.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15905279},
volume = {21},
year = {2005}
}
@article{Han2013,
author = {Han, Fang and Zhao, Tuo and Liu, Han},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Han, Zhao, Liu - 2013 - CODA High Dimensional Copula Discriminant Analysis.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {gaussian copula,high dimensional statistics,nonparanormal distribution,rank-based statistics,sparse nonlinear discriminant analysis},
pages = {629--671},
title = {{CODA: High Dimensional Copula Discriminant Analysis}},
volume = {14},
year = {2013}
}
@unpublished{Loog2013,
author = {Loog, Marco},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - 2013 - Conservative Transductive and Semi-Supervised Empirical Risk Minimization.pdf:pdf},
pages = {1--9},
title = {{Conservative Transductive and Semi-Supervised Empirical Risk Minimization}},
year = {2013}
}
@inproceedings{Sokolovska2008,
address = {Helsinki, Finland},
author = {Sokolovska, Nataliya and Capp\'{e}, Olivier and Yvon, Francois},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
editor = {Cohen, William W. and McCallum, Andrew and Roweis, Sam T.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sokolovska, Capp\'{e}, Yvon - 2008 - The asymptotics of semi-supervised learning in discriminative probabilistic models.pdf:pdf},
pages = {984--991},
publisher = {ACM Press},
title = {{The asymptotics of semi-supervised learning in discriminative probabilistic models}},
url = {http://dl.acm.org/citation.cfm?id=1390280},
year = {2008}
}
@article{Bengio2010,
author = {Bengio, Yoshua and Delalleau, Olivier and Simard, Clarence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bengio, Delalleau, Simard - 2010 - Decision trees do not generalize to new variations.pdf:pdf},
journal = {Computational Intelligence},
number = {4},
pages = {449--467},
title = {{Decision trees do not generalize to new variations}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8640.2010.00366.x/full},
volume = {26},
year = {2010}
}
@unpublished{Blum2001,
author = {Blum, Avrim and Chawla, S},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Blum, Chawla - 2001 - Learning from labeled and unlabeled data using graph mincuts.pdf:pdf},
institution = {Carnegie Mellon University, Computer Science Department},
title = {{Learning from labeled and unlabeled data using graph mincuts}},
url = {http://repository.cmu.edu/compsci/163/?utm\_source=repository.cmu.edu\%2Fcompsci\%2F163\&utm\_medium=PDF\&utm\_campaign=PDFCoverPages},
year = {2001}
}
@article{Raudys1998,
author = {Raudys, Sarunas and Duin, Robert P.W.},
doi = {10.1016/S0167-8655(98)00016-6},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Raudys, Duin - 1998 - Expected classification error of the Fisher linear classifier with pseudo-inverse covariance matrix.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Dimensionality,fisher linear discriminant,generalization error,pseudo-inversion,sample size,scissors effect,statistical classification},
month = apr,
number = {5-6},
pages = {385--392},
publisher = {Elsevier},
title = {{Expected classification error of the Fisher linear classifier with pseudo-inverse covariance matrix}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865598000166},
volume = {19},
year = {1998}
}
@article{Moore2001,
author = {Moore, Andrew W},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Moore - 2001 - VC-dimension for characterizing classifiers.pdf:pdf},
pages = {1--20},
title = {{VC-dimension for characterizing classifiers}},
year = {2001}
}
@article{Sotoca2006,
author = {Sotoca, J M and Mollineda, R A and Sanchez, J.S.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sotoca, Mollineda, Sanchez - 2006 - A meta-learning framework for pattern classification by means of data complexity measures.pdf:pdf},
journal = {Inteligencia artificial: Revista Iberoamericana de Inteligencia Artificial},
keywords = {classification,data complexity,feature selection,meta-learning,prototype selection},
number = {29},
pages = {31--38},
title = {{A meta-learning framework for pattern classification by means of data complexity measures}},
volume = {10},
year = {2006}
}
@article{Bousquet2004,
author = {Bousquet, Olivier and Boucheron, Stephane and Lugosi, Gabor},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bousquet, Boucheron, Lugosi - 2004 - Introduction to statistical learning theory.pdf:pdf},
journal = {Lecture Notes in Computer Science},
pages = {169--207},
title = {{Introduction to statistical learning theory}},
url = {http://www.springerlink.com/index/CGW0K6W5W1W1WR9B.pdf},
volume = {3176},
year = {2004}
}
@inproceedings{Sindhwani2006,
address = {New York, New York, USA},
author = {Sindhwani, Vikas and Keerthi, S. S.},
booktitle = {Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sindhwani, Keerthi - 2006 - Large scale semi-supervised linear SVMs.pdf:pdf},
isbn = {1595933697},
keywords = {global optimiza-,support vector machines,text categorization,tion,unlabeled data},
pages = {477},
publisher = {ACM Press},
title = {{Large scale semi-supervised linear SVMs}},
year = {2006}
}
@inproceedings{Blum1998,
author = {Blum, Avrim and Mitchell, Tom},
booktitle = {Proceedings of the 11th Annual Conference on Computational Learning Theory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Blum, Mitchell - 1998 - Combining labeled and unlabeled data with co-training.pdf:pdf},
pages = {92--100},
title = {{Combining labeled and unlabeled data with co-training}},
url = {http://dl.acm.org/citation.cfm?id=279962},
year = {1998}
}
@article{Wang,
author = {Wang, Jianyong},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang - Unknown - FAMER Making Multi-Instance Learning Better and Faster.pdf:pdf},
pages = {594--605},
title = {{FAMER : Making Multi-Instance Learning Better and Faster}}
}
@article{Huang2009,
abstract = {This letter discusses the robustness issue of kernel principal component analysis. A class of new robust procedures is proposed based on eigenvalue decomposition of weighted covariance. The proposed procedures will place less weight on deviant patterns and thus be more resistant to data contamination and model deviation. Theoretical influence functions are derived, and numerical examples are presented as well. Both theoretical and numerical results indicate that the proposed robust method outperforms the conventional approach in the sense of being less sensitive to outliers. Our robust method and results also apply to functional principal component analysis.},
author = {Huang, Su-Yun and Yeh, Yi-Ren and Eguchi, Shinto},
doi = {10.1162/neco.2009.02-08-706},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huang, Yeh, Eguchi - 2009 - Robust kernel principal component analysis.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Humans,Linear Models,Models, Statistical,Principal Component Analysis,Principal Component Analysis: methods,Reproducibility of Results},
month = nov,
number = {11},
pages = {3179--213},
pmid = {19686071},
title = {{Robust kernel principal component analysis.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22481823},
volume = {21},
year = {2009}
}
@article{Leistner2009,
author = {Leistner, Christian and Saffari, Amir and Santner, Jakob and Bischof, Horst},
doi = {10.1109/ICCV.2009.5459198},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Leistner et al. - 2009 - Semi-Supervised Random Forests.pdf:pdf},
isbn = {978-1-4244-4420-5},
journal = {2009 IEEE 12th International Conference on Computer Vision},
month = sep,
pages = {506--513},
publisher = {Ieee},
title = {{Semi-Supervised Random Forests}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5459198},
year = {2009}
}
@article{Dietterich1998,
author = {Dietterich, Thomas G},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dietterich - 1998 - Approximate statistical tests for comparing supervised classification learning algorithms.pdf:pdf},
journal = {Neural computation},
title = {{Approximate statistical tests for comparing supervised classification learning algorithms}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/089976698300017197},
year = {1998}
}
@article{Lin2008,
author = {Lin, Chih-jen and Weng, Ruby C},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lin, Weng - 2008 - Trust Region Newton Method for Large-Scale Logistic Regression.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {conjugate gradient,logistic regression,newton method,support,trust region},
pages = {627--650},
title = {{Trust Region Newton Method for Large-Scale Logistic Regression}},
volume = {9},
year = {2008}
}
@article{Varma2006,
abstract = {Cross-validation (CV) is an effective method for estimating the prediction error of a classifier. Some recent articles have proposed methods for optimizing classifiers by choosing classifier parameter values that minimize the CV error estimate. We have evaluated the validity of using the CV error estimate of the optimized classifier as an estimate of the true error expected on independent data.},
author = {Varma, Sudhir and Simon, Richard},
doi = {10.1186/1471-2105-7-91},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Varma, Simon - 2006 - Bias in error estimation when using cross-validation for model selection.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Artificial Intelligence,Bias (Epidemiology),Computer Simulation,Data Interpretation, Statistical,Gene Expression Profiling,Gene Expression Profiling: methods,Models, Genetic,Models, Statistical,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Reproducibility of Results,Sensitivity and Specificity},
month = jan,
pages = {91},
pmid = {16504092},
title = {{Bias in error estimation when using cross-validation for model selection.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1397873\&tool=pmcentrez\&rendertype=abstract},
volume = {7},
year = {2006}
}
@article{Sun2010,
author = {Sun, Shiliang and Shawe-taylor, John},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sun, Shawe-taylor - 2010 - Sparse Semi-supervised Learning Using Conjugate Functions.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {fenchel-legendre conjugate,multi-,representer theorem,semi-supervised learning,statistical learning theory,support vector machine,view regularization},
pages = {2423--2455},
title = {{Sparse Semi-supervised Learning Using Conjugate Functions}},
volume = {11},
year = {2010}
}
@article{Wang2009b,
author = {Wang, Junhui and Shen, Xiaotong and Pan, Wei},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Shen, Pan - 2009 - On efficient large margin semisupervised learning Method and theory.pdf:pdf},
journal = {The Journal of Machine Learning Research},
keywords = {classification,difference convex programming,nonconvex minimization,regulariza-,support vectors,tion},
pages = {719--742},
title = {{On efficient large margin semisupervised learning: Method and theory}},
url = {http://dl.acm.org/citation.cfm?id=1577094},
volume = {10},
year = {2009}
}
@article{Dimitroff2014,
author = {Dimitroff, Georgi and Georgiev, Georgi and Toloşi, Laura and Popov, Borislav},
doi = {10.1007/s10994-014-5439-y},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dimitroff et al. - 2014 - Efficient \$\$F\$\$ F measure maximization via weighted maximum likelihood.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = apr,
title = {{Efficient \$\$F\$\$ F measure maximization via weighted maximum likelihood}},
url = {http://link.springer.com/10.1007/s10994-014-5439-y},
year = {2014}
}
@book{Barber2012,
author = {Barber, David},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Barber - 2012 - Bayesian reasoning and machine learning.pdf:pdf},
title = {{Bayesian reasoning and machine learning}},
url = {http://books.google.com/books?hl=en\&lr=\&id=yxZtddB\_Ob0C\&oi=fnd\&pg=PR5\&dq=Bayesian+Reasoning+and+Machine+Learning\&ots=A\_VEJ7bMBo\&sig=rls310\_gHhPhip1Nfc8KLIkMnfE},
year = {2012}
}
@article{King1995,
author = {King, R.D. and Feng, C and Sutherland, A},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/King, Feng, Sutherland - 1995 - Statlog comparison of classification algorithms on large real-world problems.pdf:pdf},
journal = {Applied Artificial Intelligence an International Journal},
number = {3},
pages = {289--333},
title = {{Statlog: comparison of classification algorithms on large real-world problems}},
url = {http://www.tandfonline.com/doi/abs/10.1080/08839519508945477},
volume = {9},
year = {1995}
}
@article{Geyer1994,
author = {Geyer, Charles J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Geyer - 1994 - On the asymptotics of constrained M-estimation.pdf:pdf},
journal = {The Annals of Statistics},
number = {4},
pages = {1993--2010},
title = {{On the asymptotics of constrained M-estimation}},
url = {http://www.jstor.org/stable/2242495},
volume = {22},
year = {1994}
}
@article{Mordelet2010,
archivePrefix = {arXiv},
arxivId = {arXiv:1010.0772v1},
author = {Mordelet, Fantine and Vert, Jean-Philippe},
eprint = {arXiv:1010.0772v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mordelet, Vert - 2010 - A bagging SVM to learn from positive and unlabeled examples.pdf:pdf},
journal = {arXiv preprint},
pages = {1--15},
title = {{A bagging SVM to learn from positive and unlabeled examples}},
url = {http://arxiv.org/abs/1010.0772},
year = {2010}
}
@article{Mann2007,
author = {Mann, Gideon S. and McCallum, Andrew Kachites},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mann, McCallum - 2007 - Efficient computation of entropy gradient for semi-supervised conditional random fields.pdf:pdf},
journal = {Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics},
title = {{Efficient computation of entropy gradient for semi-supervised conditional random fields}},
url = {http://dl.acm.org/citation.cfm?id=1614136},
year = {2007}
}
@article{Hennig2012,
author = {Hennig, Philipp and Kiefel, Martin},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hennig, Kiefel - 2012 - Quasi-Newton Methods A New Direction.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {gaussian processes,numerical analysis,optimization,probability},
pages = {843--865},
title = {{Quasi-Newton Methods: A New Direction}},
url = {http://arxiv.org/abs/1206.4602},
volume = {14},
year = {2012}
}
@article{Doksum2007,
author = {Doksum, Kjell and Ozeki, Akichika and Kim, Jihoon and {Chaibub Neto}, Elias},
doi = {10.1016/j.spl.2007.03.005},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Doksum et al. - 2007 - Thinking outside the box Statistical inference based on Kullback–Leibler empirical projections.pdf:pdf},
issn = {01677152},
journal = {Statistics \& Probability Letters},
keywords = {bootstrap,box-cox transformation,classification,covariate,k-l divergence,klep,outside the box,sandwich formula},
month = jul,
number = {12},
pages = {1201--1213},
title = {{Thinking outside the box: Statistical inference based on Kullback–Leibler empirical projections}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167715207000843},
volume = {77},
year = {2007}
}
@article{Cesa-Bianchi2007,
author = {Cesa-Bianchi, Nicol\`{o}},
doi = {10.1016/j.tcs.2007.03.053},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cesa-Bianchi - 2007 - Applications of regularized least squares to pattern classification.pdf:pdf},
issn = {03043975},
journal = {Theoretical Computer Science},
keywords = {on-line learning,perceptron,ridge regression,selective sampling},
month = sep,
number = {3},
pages = {221--231},
title = {{Applications of regularized least squares to pattern classification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S030439750700237X},
volume = {382},
year = {2007}
}
@inproceedings{Steck2003,
author = {Steck, Harald and Jaakkola, Tommi S.},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Steck, Jaakkola - 2003 - Bias-corrected bootstrap and model uncertainty.pdf:pdf},
title = {{Bias-corrected bootstrap and model uncertainty}},
url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/NIPS2003\_AA66.pdf},
year = {2003}
}
@article{Huang1998,
author = {Huang, Jianhua Z.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huang - 1998 - Projection estimation in multiple regression with application to functional ANOVA models.pdf:pdf},
journal = {The Annals of Statistics},
keywords = {and phrases,anova,curse of dimensionality,finite elements,interaction,least},
number = {1},
pages = {242--272},
title = {{Projection estimation in multiple regression with application to functional ANOVA models}},
url = {http://projecteuclid.org/euclid.aos/1030563984},
volume = {26},
year = {1998}
}
@article{Gelman2014,
author = {Gelman, Andrew},
doi = {10.1214/13-STS458},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman - 2014 - How Bayesian Analysis Cracked the Red-State, Blue-State Problem.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Multilevel regression and poststratification (MRP),and phrases,mrp,multilevel regression and poststratification,political science,sample surveys,sparse data,voting},
month = feb,
number = {1},
pages = {26--35},
title = {{How Bayesian Analysis Cracked the Red-State, Blue-State Problem}},
url = {http://projecteuclid.org/euclid.ss/1399645725},
volume = {29},
year = {2014}
}
@article{Hartley1968a,
author = {Hartley, HO and Rao, JNK},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hartley, Rao - 1968 - Classification and estimation in analysis of variance problems(2).pdf:pdf},
journal = {Review of the International Statistical Institute},
number = {2},
pages = {141--147},
title = {{Classification and estimation in analysis of variance problems}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Classification+and+Estimation+in+Analysis+of+Variance+Problems\#1 http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Classification+and+estimation+in+analysis+of+variance+problems\#1},
volume = {36},
year = {1968}
}
@inproceedings{Plessis2012,
author = {du Plessis, Marthinus Christoffel and Sugiyama, Masashi},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Plessis, Sugiyama - 2012 - Semi-supervised learning of class balance under class-prior change by distribution matching.pdf:pdf},
title = {{Semi-supervised learning of class balance under class-prior change by distribution matching}},
url = {http://arxiv.org/abs/1206.4677},
year = {2012}
}
@article{Arlot2010,
author = {Arlot, Sylvain and Celisse, Alain},
doi = {10.1214/09-SS054},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Arlot, Celisse - 2010 - A survey of cross-validation procedures for model selection.pdf:pdf},
issn = {1935-7516},
journal = {Statistics Surveys},
keywords = {and phrases,cross-validation,leave-one-out,model selection},
pages = {40--79},
title = {{A survey of cross-validation procedures for model selection}},
url = {http://projecteuclid.org/euclid.ssu/1268143839},
volume = {4},
year = {2010}
}
@article{Sch,
archivePrefix = {arXiv},
arxivId = {arXiv:1112.2738v1},
author = {Sch\"{o}lkopf, Bernhard and Janzing, Dominik and Peters, Jonas and Zhang, Kun},
eprint = {arXiv:1112.2738v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sch\"{o}lkopf et al. - Unknown - Robust Learning via Cause-Effect Models.pdf:pdf},
pages = {1--15},
title = {{Robust Learning via Cause-Effect Models}}
}
@article{Gelman2013,
abstract = {A substantial school in the philosophy of science identifies Bayesian inference with inductive inference and even rationality as such, and seems to be strengthened by the rise and practical success of Bayesian statistics. We argue that the most successful forms of Bayesian statistics do not actually support that particular philosophy but rather accord much better with sophisticated forms of hypothetico-deductivism. We examine the actual role played by prior distributions in Bayesian models, and the crucial aspects of model checking and model revision, which fall outside the scope of Bayesian confirmation theory. We draw on the literature on the consistency of Bayesian updating and also on our experience of applied work in social science. Clarity about these matters should benefit not just philosophy of science, but also statistical practice. At best, the inductivist view has encouraged researchers to fit and compare models without checking them; at worst, theorists have actively discouraged practitioners from performing model checking because it does not fit into their framework.},
author = {Gelman, Andrew and Shalizi, Cosma Rohilla},
doi = {10.1111/j.2044-8317.2011.02037.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Shalizi - 2013 - Philosophy and the practice of Bayesian statistics.pdf:pdf},
issn = {2044-8317},
journal = {The British journal of mathematical and statistical psychology},
month = feb,
number = {1},
pages = {8--38},
pmid = {22364575},
title = {{Philosophy and the practice of Bayesian statistics.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22364575},
volume = {66},
year = {2013}
}
@inproceedings{Cortes2010,
author = {Cortes, Corinna and Mansour, Yishay and Mohri, Mehryar},
booktitle = {Advances in Neural Information Processing Systems 23},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cortes, Mansour, Mohri - 2010 - Learning bounds for importance weighting.pdf:pdf},
pages = {442--450},
title = {{Learning bounds for importance weighting}},
url = {http://www.cs.nyu.edu/~mohri/pub/importance.pdf},
year = {2010}
}
@inproceedings{Sindhwani2005,
author = {Sindhwani, Vikas and Niyogi, Partha and Belkin, Mikhail},
booktitle = {Proceedings of the 22nd international conference on Machine learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sindhwani, Niyogi, Belkin - 2005 - Beyond the point cloud from transductive to semi-supervised learning.pdf:pdf},
number = {0},
pages = {824--831},
title = {{Beyond the point cloud: from transductive to semi-supervised learning}},
url = {http://dl.acm.org/citation.cfm?id=1102455},
year = {2005}
}
@book{Basu2006,
author = {Basu, Mitra and Ho, Tin Kam},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Basu, Ho - 2006 - Data complexity in pattern recognition.pdf:pdf},
isbn = {9781846281716},
title = {{Data complexity in pattern recognition}},
url = {http://books.google.com/books?hl=en\&lr=\&id=GflBKbzym9oC\&oi=fnd\&pg=PR11\&dq=Data+Complexity+in+Pattern+Recognition\&ots=igbI3IXn6d\&sig=-7L3L4iU5lzLaNaCVoEux\_GbVn4},
year = {2006}
}
@inproceedings{Elworthy1994,
archivePrefix = {arXiv},
arxivId = {arXiv:cmp-lg/9410012v2},
author = {Elworthy, David},
booktitle = {Proceedings of the fourth conference on Applied natural language processing},
eprint = {9410012v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Elworthy - 1994 - Does Baum-Welch re-estimation help taggers.pdf:pdf},
pages = {53--58},
primaryClass = {arXiv:cmp-lg},
title = {{Does Baum-Welch re-estimation help taggers?}},
url = {http://dl.acm.org/citation.cfm?id=974371},
year = {1994}
}
@article{McLachlan1982,
author = {McLachlan, Geoffrey J. and Ganesalingam, S.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/McLachlan, Ganesalingam - 1982 - Updating a discriminant function on the basis of unclassified data.pdf:pdf},
journal = {Communication in Statistics- Simulation and Computation},
title = {{Updating a discriminant function on the basis of unclassified data}},
url = {http://www.tandfonline.com/doi/full/10.1080/03610918208812293},
year = {1982}
}
@inproceedings{Corduneanu2002,
author = {Corduneanu, Adrian and Jaakkola, Tommi},
booktitle = {Proceedings of the 19th conference on Uncertainty in Artificial Intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Corduneanu, Jaakkola - 2002 - On information regularization.pdf:pdf},
pages = {151--158},
title = {{On information regularization}},
url = {http://dl.acm.org/citation.cfm?id=2100602},
year = {2002}
}
@article{Caticha2011,
author = {Caticha, Ariel and Mohammad-Djafari, Ali and Bercher, Jean-François and Bessiére, Pierre},
doi = {10.1063/1.3573619},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Caticha et al. - 2011 - Entropic Inference.pdf:pdf},
isbn = {9780735408609},
keywords = {bayes rule,entropy,information,maximum entropy},
number = {1},
pages = {20--29},
title = {{Entropic Inference}},
url = {http://link.aip.org/link/APCPCS/v1305/i1/p20/s1\&Agg=doi},
volume = {20},
year = {2011}
}
@article{Kuncheva2002,
author = {Kuncheva, Ludmila I},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kuncheva - 2002 - A Theoretical Study on Six Classifier Fusion Strategies.pdf:pdf},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {2},
pages = {281--286},
title = {{A Theoretical Study on Six Classifier Fusion Strategies}},
volume = {24},
year = {2002}
}
@article{Bartlett2003a,
author = {Bartlett, Peter L and Mendelson, S},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bartlett, Mendelson - 2003 - Rademacher and Gaussian complexities Risk bounds and structural results.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {data-dependent complexity,error bounds,maxi-,rademacher averages},
pages = {463--482},
title = {{Rademacher and Gaussian complexities: Risk bounds and structural results}},
url = {http://dl.acm.org/citation.cfm?id=944944},
volume = {3},
year = {2003}
}
@article{Zhang2006,
abstract = {With the development of DNA microarray technology, scientists can now measure the expression levels of thousands of genes simultaneously in one single experiment. One current difficulty in interpreting microarray data comes from their innate nature of 'high-dimensional low sample size'. Therefore, robust and accurate gene selection methods are required to identify differentially expressed group of genes across different samples, e.g. between cancerous and normal cells. Successful gene selection will help to classify different cancer types, lead to a better understanding of genetic signatures in cancers and improve treatment strategies. Although gene selection and cancer classification are two closely related problems, most existing approaches handle them separately by selecting genes prior to classification. We provide a unified procedure for simultaneous gene selection and cancer classification, achieving high accuracy in both aspects.},
author = {Zhang, Hao Helen and Ahn, Jeongyoun and Lin, Xiaodong and Park, Cheolwoo},
doi = {10.1093/bioinformatics/bti736},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang et al. - 2006 - Gene selection using support vector machines with non-convex penalty.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Bayes Theorem,Cluster Analysis,Databases, Genetic,Humans,Models, Genetic,Models, Statistical,Neoplasms,Neoplasms: genetics,Neoplasms: metabolism,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Pattern Recognition, Automated,Sequence Analysis, DNA,Software,Time Factors},
month = jan,
number = {1},
pages = {88--95},
pmid = {16249260},
title = {{Gene selection using support vector machines with non-convex penalty.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16249260},
volume = {22},
year = {2006}
}
@techreport{Welling,
author = {Welling, Max},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Welling - Unknown - Kernel ridge Regression.pdf:pdf},
number = {3},
pages = {3--5},
title = {{Kernel ridge Regression}}
}
@article{Keogh2005,
author = {Keogh, Eamonn and Lin, Jessica},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Keogh, Lin - 2005 - Clustering of time-series subsequences is meaningless implications for previous and future research.pdf:pdf},
journal = {Knowledge and information systems},
number = {2},
pages = {154--177},
title = {{Clustering of time-series subsequences is meaningless: implications for previous and future research}},
url = {http://link.springer.com/article/10.1007/s10115-004-0172-7},
volume = {8},
year = {2005}
}
@article{Xiong2013,
author = {Xiong, S.},
doi = {10.1093/biomet/ast041},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Xiong - 2013 - Better subset regression.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = nov,
number = {November 2013},
pages = {71--84},
title = {{Better subset regression}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/ast041},
year = {2013}
}
@article{Nemenman2001,
author = {Nemenman, Ilya and Shafee, Fariel and Bialek, William},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nemenman, Shafee, Bialek - 2001 - Entropy and Inference, Revisited.pdf:pdf},
journal = {arXiv preprint},
title = {{Entropy and Inference, Revisited}},
url = {http://arxiv.org/abs/physics/0108025},
year = {2001}
}
@article{Baraniuk2007,
author = {Baraniuk, Richard G.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Baraniuk - 2007 - Compressive sensing.pdf:pdf},
journal = {IEEE Signal Processing Magazine},
number = {July},
pages = {118--121},
title = {{Compressive sensing}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4286571 http://omni.isr.ist.utl.pt/~aguiar/CS\_notes.pdf},
year = {2007}
}
@article{Martella2011,
abstract = {In healthy aging research, typically multiple health outcomes are measured, representing health status. The aim of this paper was to develop a model-based clustering approach to identify homogeneous sibling pairs according to their health status. Model-based clustering approaches will be considered on the basis of linear mixed effect model for the mixture components. Class memberships of siblings within pairs are allowed to be correlated, and within a class the correlation between siblings is modeled using random sibling pair effects. We propose an expectation-maximization algorithm for maximum likelihood estimation. Model performance is evaluated via simulations in terms of estimating the correct parameters, degree of agreement, and the ability to detect the correct number of clusters. The performance of our model is compared with the performance of standard model-based clustering approaches. The methods are used to classify sibling pairs from the Leiden Longevity Study according to their health status. Our results suggest that homogeneous healthy sibling pairs are associated with a longer life span. Software is available for fitting the new models.},
author = {Martella, F. and Vermunt, J.K. and Beekman, M. and Westendorp, R.G.J. and Slagboom, P.E. and Houwing-Duistermaat, J.J.},
doi = {10.1002/sim.4365},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Martella et al. - 2011 - A mixture model with random-effects components for classifying sibling pairs.pdf:pdf},
issn = {1097-0258},
journal = {Statistics in medicine},
keywords = {80 and over,Aged,Aging,Aging: physiology,Cluster Analysis,Computer Simulation,Female,Health,Humans,Longevity,Longevity: physiology,Male,Models,Siblings,Statistical},
month = nov,
number = {27},
pages = {3252--64},
pmid = {21905068},
title = {{A mixture model with random-effects components for classifying sibling pairs.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21905068},
volume = {30},
year = {2011}
}
@article{Jenssen2006,
author = {Jenssen, Robert and Eltoft, Torbj\o rn and Erdogmus, Deniz and Principe, Jose C.},
doi = {10.1007/s11265-006-9771-8},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jenssen et al. - 2006 - Some Equivalences between Kernel Methods and Information Theoretic Methods.pdf:pdf},
issn = {0922-5773},
journal = {The Journal of VLSI Signal Processing Systems for Signal, Image, and Video Technology},
month = dec,
number = {1-2},
pages = {49--65},
title = {{Some Equivalences between Kernel Methods and Information Theoretic Methods}},
url = {http://link.springer.com/10.1007/s11265-006-9771-8},
volume = {45},
year = {2006}
}
@article{Leemis2008,
author = {Leemis, Lawrence M and McQueston, Jacquelyn T},
doi = {10.1198/000313008X270448},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Leemis, McQueston - 2008 - Univariate Distribution Relationships.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {asymptotic relationships,distribution proper-,limiting distributions,stochastic parameters,ties,transforma-},
month = feb,
number = {1},
pages = {45--53},
title = {{Univariate Distribution Relationships}},
url = {http://www.tandfonline.com/doi/abs/10.1198/000313008X270448},
volume = {62},
year = {2008}
}
@inproceedings{Krijthe2012b,
author = {Krijthe, Jesse Hendrik and Ho, Tin Kam and Loog, Marco},
booktitle = {Proceedings of the 21st International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Krijthe, Ho, Loog - 2012 - Improving cross-validation based classifier selection using meta-learning.pdf:pdf},
number = {1},
pages = {2873--2876},
title = {{Improving cross-validation based classifier selection using meta-learning}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6460765},
year = {2012}
}
@inproceedings{Joachims1999,
author = {Joachims, Thorsten},
booktitle = {Proceedings of the 16th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Joachims - 1999 - Transductive inference for text classification using support vector machines.pdf:pdf},
pages = {200--209},
publisher = {Morgan Kaufmann Publishers},
title = {{Transductive inference for text classification using support vector machines}},
year = {1999}
}
@phdthesis{Krijthe2012,
abstract = {In order to choose from the large number of classification methods available for use, cross-validation error estimates are often employed. We present this cross-validation selection strategy in the framework of meta-learning and show that conceptually, meta- learning techniques could provide better classifier selections than traditional cross-validation selection. Using various simulation studies we illustrate and discuss this possibility. Through a collection of datasets resembling real-world data, we investigate whether these improvements could possibly exist in the real-world as well. Although the approach presented here currently requires signifi- cant investment when applied to practical applications, the concept of being able to outperform cross-validation selection opens the door to new classifier selection strategies.},
author = {Krijthe, Jesse Hendrik},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Krijthe - 2012 - Improving Cross-Validation Classifier Selection Accuracy through Meta-Learning.pdf:pdf},
keywords = {Classifier Selection,Error estimation,Meta-Learning},
mendeley-tags = {Classifier Selection,Error estimation,Meta-Learning},
school = {Delft University of Technology},
title = {{Improving Cross-Validation Classifier Selection Accuracy through Meta-Learning}},
year = {2012}
}
@inproceedings{Sokolovska2011,
address = {Greece},
author = {Sokolovska, Nataliya},
booktitle = {ECML PKDD},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sokolovska - 2011 - Aspects of semi-supervised and active learning in conditional random fields.pdf:pdf},
keywords = {active learn-,conditional random fields,ing,probability of observations,semi-supervised learning},
title = {{Aspects of semi-supervised and active learning in conditional random fields}},
url = {http://www.springerlink.com/index/3308764R6251J70P.pdf},
year = {2011}
}
@article{He2010,
abstract = {Feature selection techniques have been used as the workhorse in biomarker discovery applications for a long time. Surprisingly, the stability of feature selection with respect to sampling variations has long been under-considered. It is only until recently that this issue has received more and more attention. In this article, we review existing stable feature selection methods for biomarker discovery using a generic hierarchical framework. We have two objectives: (1) providing an overview on this new yet fast growing topic for a convenient reference; (2) categorizing existing methods under an expandable framework for future research and development.},
author = {He, Zengyou and Yu, Weichuan},
doi = {10.1016/j.compbiolchem.2010.07.002},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/He, Yu - 2010 - Stable feature selection for biomarker discovery.pdf:pdf},
issn = {1476-928X},
journal = {Computational biology and chemistry},
keywords = {Animals,Artificial Intelligence,Biological Markers,Biological Markers: analysis,Biological Markers: metabolism,Computational Biology,Computational Biology: methods,Humans},
month = aug,
number = {4},
pages = {215--25},
pmid = {20702140},
publisher = {Elsevier Ltd},
title = {{Stable feature selection for biomarker discovery.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20702140},
volume = {34},
year = {2010}
}
@article{Schmidt1996,
author = {Schmidt, Karsten},
doi = {10.1007/BF00046993},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schmidt - 1996 - A comparison of minimax and least squares estimators in linear regression with polyhedral prior information.pdf:pdf},
issn = {0167-8019},
journal = {Acta Applicandae Mathematicae},
keywords = {1,3,average performance,estimator,inequality restricted least squares,minimax estima-,n vector of,parameter restrictions,polyhedral prior information in,projection estimators,regression model y,the linear regression model,tion,u,we consider the linear,where y is an,x},
month = apr,
number = {1},
pages = {127--138},
title = {{A comparison of minimax and least squares estimators in linear regression with polyhedral prior information}},
url = {http://link.springer.com/10.1007/BF00046993},
volume = {43},
year = {1996}
}
@phdthesis{Mika2002,
author = {Mika, Sebastian},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mika - 2002 - Kernel fisher discriminants.pdf:pdf},
title = {{Kernel fisher discriminants}},
url = {http://opus.kobv.de/tuberlin/volltexte/2003/477/},
year = {2002}
}
@article{Hyvarinen2000,
abstract = {A fundamental problem in neural network research, as well as in many other disciplines, is finding a suitable representation of multivariate data, i.e. random vectors. For reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation of the original data. In other words, each component of the representation is a linear combination of the original variables. Well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. Independent component analysis (ICA) is a recently developed method in which the goal is to find a linear representation of non-Gaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. In this paper, we present the basic theory and applications of ICA, and our recent work on the subject.},
author = {Hyv\"{a}rinen, Aapo and Oja, Erkki},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hyv\"{a}rinen, Oja - 2000 - Independent component analysis algorithms and applications.pdf:pdf},
issn = {0893-6080},
journal = {Neural networks : the official journal of the International Neural Network Society},
keywords = {Algorithms,Artifacts,Brain,Brain: physiology,Humans,Magnetoencephalography,Neural Networks (Computer),Normal Distribution},
number = {4-5},
pages = {411--30},
pmid = {10946390},
title = {{Independent component analysis: algorithms and applications.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10946390},
volume = {13},
year = {2000}
}
@inproceedings{Balcan2013,
author = {Balcan, Maria-Florina and Berlind, Christopher and Ehrlich, Steven and Liang, Yingyu},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balcan et al. - 2013 - Efficient Semi-supervised and Active Learning of Disjunctions.pdf:pdf},
pages = {633--641},
title = {{Efficient Semi-supervised and Active Learning of Disjunctions}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/ICML2013\_balcan13},
year = {2013}
}
@inproceedings{Collobert2006a,
author = {Collobert, Ronan and Sinz, Fabian and Weston, Jason and Bottou, L},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Collobert et al. - 2006 - Trading convexity for scalability.pdf:pdf},
pages = {201--208},
title = {{Trading convexity for scalability}},
url = {http://dl.acm.org/citation.cfm?id=1143870},
year = {2006}
}
@article{Buhlmann2014,
author = {B\"{u}hlmann, Peter},
doi = {10.1214/13-STS460},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/B\"{u}hlmann - 2014 - Discussion of Big Bayes Stories and BayesBag.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
month = feb,
number = {1},
pages = {91--94},
title = {{Discussion of Big Bayes Stories and BayesBag}},
url = {http://projecteuclid.org/euclid.ss/1399645732},
volume = {29},
year = {2014}
}
@article{Jones2011,
abstract = {MALDI mass spectrometry can generate profiles that contain hundreds of biomolecular ions directly from tissue. Spatially-correlated analysis, MALDI imaging MS, can simultaneously reveal how each of these biomolecular ions varies in clinical tissue samples. The use of statistical data analysis tools to identify regions containing correlated mass spectrometry profiles is referred to as imaging MS-based molecular histology because of its ability to annotate tissues solely on the basis of the imaging MS data. Several reports have indicated that imaging MS-based molecular histology may be able to complement established histological and histochemical techniques by distinguishing between pathologies with overlapping/identical morphologies and revealing biomolecular intratumor heterogeneity. A data analysis pipeline that identifies regions of imaging MS datasets with correlated mass spectrometry profiles could lead to the development of novel methods for improved diagnosis (differentiating subgroups within distinct histological groups) and annotating the spatio-chemical makeup of tumors. Here it is demonstrated that highlighting the regions within imaging MS datasets whose mass spectrometry profiles were found to be correlated by five independent multivariate methods provides a consistently accurate summary of the spatio-chemical heterogeneity. The corroboration provided by using multiple multivariate methods, efficiently applied in an automated routine, provides assurance that the identified regions are indeed characterized by distinct mass spectrometry profiles, a crucial requirement for its development as a complementary histological tool. When simultaneously applied to imaging MS datasets from multiple patient samples of intermediate-grade myxofibrosarcoma, a heterogeneous soft tissue sarcoma, nodules with mass spectrometry profiles found to be distinct by five different multivariate methods were detected within morphologically identical regions of all patient tissue samples. To aid the further development of imaging MS based molecular histology as a complementary histological tool the Matlab code of the agreement analysis, instructions and a reduced dataset are included as supporting information.},
author = {Jones, Emrys a and van Remoortere, Alexandra and van Zeijl, Ren\'{e} J M and Hogendoorn, Pancras C W and Bov\'{e}e, Judith V M G and Deelder, Andr\'{e} M and McDonnell, Liam a},
doi = {10.1371/journal.pone.0024913},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jones et al. - 2011 - Multiple statistical analysis techniques corroborate intratumor heterogeneity in imaging mass spectrometry dataset.pdf:pdf},
issn = {1932-6203},
journal = {PloS one},
keywords = {Algorithms,Databases, Factual,Diagnostic Imaging,Diagnostic Imaging: methods,Fibroma,Fibroma: metabolism,Fibrosarcoma,Fibrosarcoma: metabolism,Gene Expression Regulation, Neoplastic,Humans,Ions,Ions: chemistry,Models, Statistical,Molecular Imaging,Molecular Imaging: methods,Multivariate Analysis,Peptides,Peptides: chemistry,Proteins,Proteins: chemistry,Sarcoma,Sarcoma: metabolism,Software,Spectrometry, Mass, Matrix-Assisted Laser Desorpti},
month = jan,
number = {9},
pages = {e24913},
pmid = {21980364},
title = {{Multiple statistical analysis techniques corroborate intratumor heterogeneity in imaging mass spectrometry datasets of myxofibrosarcoma.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3183001\&tool=pmcentrez\&rendertype=abstract},
volume = {6},
year = {2011}
}
@article{Hanczar2010,
author = {Hanczar, Blaise and Dougherty, Edward R.},
doi = {10.2174/157489310790596376},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hanczar, Dougherty - 2010 - On the Comparison of Classifiers for Microarray Data.pdf:pdf},
issn = {15748936},
journal = {Current Bioinformatics},
keywords = {classifier comparison,error estimation,microarray classification,variance study},
month = mar,
number = {1},
pages = {29--39},
title = {{On the Comparison of Classifiers for Microarray Data}},
url = {http://openurl.ingenta.com/content/xref?genre=article\&issn=1574-8936\&volume=5\&issue=1\&spage=29},
volume = {5},
year = {2010}
}
@article{Lindner1999,
author = {Lindner, Guido and Studer, Rudi},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lindner, Studer - 1999 - AST Support for algorithm selection with a CBR approach.pdf:pdf},
journal = {Principles of Data Mining and Knowledge Discovery},
title = {{AST: Support for algorithm selection with a CBR approach}},
url = {http://www.springerlink.com/index/QBDF3R2GKVW57LUF.pdf http://link.springer.com/chapter/10.1007/978-3-540-48247-5\_52},
year = {1999}
}
@article{Gelman2013d,
author = {Gelman, Andrew and Robert, Christian P},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Robert - 2013 - “Not only defended but also applied” The perceived absurdity of Bayesian inference.pdf:pdf},
journal = {The American Statistician},
keywords = {bayesian,bogosity,doomsdsay argument,foundations,frequentist,laplace law of succession},
number = {1},
title = {{“Not only defended but also applied”: The perceived absurdity of Bayesian inference}},
volume = {67},
year = {2013}
}
@article{Poggio2003,
author = {Poggio, Tomaso and Smale, Steve},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Poggio, Smale - 2003 - The Mathematics of Learning Dealing with Data.pdf:pdf},
journal = {Notices of the AMS},
pages = {537--544},
title = {{The Mathematics of Learning: Dealing with Data}},
url = {http://www.ams.org/notices/200305/fea-smale.pdf},
year = {2003}
}
@article{Pearl2009,
author = {Pearl, Judea},
doi = {10.1214/09-SS057},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pearl - 2009 - Causal inference in statistics An overview.pdf:pdf},
issn = {1935-7516},
journal = {Statistics Surveys},
keywords = {Structural equation models, confounding, graphical,and phrases,causal effects,causes of effects,confounding,counterfactuals,graph-,ical methods,mediation,policy evaluation,potential-outcome,received september 2009,structural equation models},
number = {September},
pages = {96--146},
title = {{Causal inference in statistics: An overview}},
url = {http://projecteuclid.org/euclid.ssu/1255440554},
volume = {3},
year = {2009}
}
@article{Hilario2006,
abstract = {Among the many applications of mass spectrometry, biomarker pattern discovery from protein mass spectra has aroused considerable interest in the past few years. While research efforts have raised hopes of early and less invasive diagnosis, they have also brought to light the many issues to be tackled before mass-spectra-based proteomic patterns become routine clinical tools. Known issues cover the entire pipeline leading from sample collection through mass spectrometry analytics to biomarker pattern extraction, validation, and interpretation. This study focuses on the data-analytical phase, which takes as input mass spectra of biological specimens and discovers patterns of peak masses and intensities that discriminate between different pathological states. We survey current work and investigate computational issues concerning the different stages of the knowledge discovery process: exploratory analysis, quality control, and diverse transforms of mass spectra, followed by further dimensionality reduction, classification, and model evaluation. We conclude after a brief discussion of the critical biomedical task of analyzing discovered discriminatory patterns to identify their component proteins as well as interpret and validate their biological implications.},
author = {Hilario, Melanie and Kalousis, Alexandros and Pellegrini, Christian and M\"{u}ller, Markus},
doi = {10.1002/mas.20072},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hilario et al. - 2006 - Processing and classification of protein mass spectra.pdf:pdf},
issn = {0277-7037},
journal = {Mass spectrometry reviews},
keywords = {Algorithms,Animals,Biological Markers,Computational Biology,Humans,Mass Spectrometry,Mass Spectrometry: classification,Mass Spectrometry: methods,Models, Chemical,Peptide Mapping,Proteins,Proteins: analysis,Proteomics},
number = {3},
pages = {409--49},
pmid = {16463283},
title = {{Processing and classification of protein mass spectra.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16463283},
volume = {25},
year = {2006}
}
@inproceedings{Wolpert2002,
author = {Wolpert, David H},
booktitle = {Soft Computing and Industry},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wolpert - 2002 - The Supervised Learning No-Free-Lunch Theorems.pdf:pdf},
pages = {25--42},
title = {{The Supervised Learning No-Free-Lunch Theorems}},
url = {http://link.springer.com/chapter/10.1007/978-1-4471-0123-9\_3},
year = {2002}
}
@article{Jordan2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1309.7804v1},
author = {Jordan, Michael I.},
doi = {10.3150/12-BEJSP17},
eprint = {arXiv:1309.7804v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jordan - 2013 - On statistics, computation and scalability.pdf:pdf},
issn = {1350-7265},
journal = {Bernoulli},
keywords = {()},
month = sep,
number = {4},
pages = {1378--1390},
title = {{On statistics, computation and scalability}},
url = {http://projecteuclid.org/euclid.bj/1377612856},
volume = {19},
year = {2013}
}
@misc{Gelman2013b,
author = {Gelman, Andrew},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman - 2013 - Choices in statistical graphics My stories.pdf:pdf},
title = {{Choices in statistical graphics : My stories}},
year = {2013}
}
@article{Cooper1992,
author = {Cooper, Gregory F. and Herskovits, Edward},
doi = {10.1007/BF00994110},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cooper, Herskovits - 1992 - A Bayesian method for the induction of probabilistic networks from data.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {bayesian belief networks,induction,machine learning,probabilistic networks},
month = oct,
number = {4},
pages = {309--347},
title = {{A Bayesian method for the induction of probabilistic networks from data}},
url = {http://link.springer.com/10.1007/BF00994110},
volume = {9},
year = {1992}
}
@article{Clarke2012,
author = {Clarke, Bertrand and Clarke, Jennifer},
doi = {10.1214/12-SS100},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Clarke, Clarke - 2012 - Prediction in several conventional contexts.pdf:pdf},
issn = {1935-7516},
journal = {Statistics Surveys},
keywords = {62M20Prediction, prequential, IID data, time serie,and phrases,iid data,longitudinal data,prediction,prequential,received march 2012,survival analysis,time series},
pages = {1--73},
title = {{Prediction in several conventional contexts}},
url = {http://projecteuclid.org/euclid.ssu/1336481369},
volume = {6},
year = {2012}
}
@misc{Klein2004,
author = {Klein, Dan},
booktitle = {University of California at Berkeley, Computer Science \ldots},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Klein - 2004 - Lagrange Multipliers without Permanent Scarring.pdf:pdf},
title = {{Lagrange Multipliers without Permanent Scarring}},
url = {http://www.ee.columbia.edu/~vittorio/LagrangeMultipliers-Klein.pdf},
year = {2004}
}
@article{Ho2002,
author = {Ho, Tin Kam and Basu, Mitra},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho, Basu - 2002 - Complexity Measures of Supervised Classification Problems.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {3},
pages = {289--300},
title = {{Complexity Measures of Supervised Classification Problems}},
volume = {24},
year = {2002}
}
@inproceedings{Weinberger2009,
archivePrefix = {arXiv},
arxivId = {arXiv:0902.2206v5},
author = {Weinberger, Kilian and Dasgupta, Anirban and Attenberg, Josh and Langford, John and Smola, Alex},
booktitle = {International Conference on Machine Learning},
eprint = {arXiv:0902.2206v5},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Weinberger et al. - 2009 - Feature hashing for large scale multitask learning.pdf:pdf},
keywords = {classifier personalization,concentration inequalities,document classification,kernels,multitask learning},
number = {Icml},
pages = {1113--1120},
title = {{Feature hashing for large scale multitask learning}},
url = {http://dl.acm.org/citation.cfm?id=1553516},
year = {2009}
}
@techreport{Wainwright,
author = {Wainwright, Martin J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wainwright - Unknown - Graphical models and message-passing algorithms Some introductory lectures.pdf:pdf},
pages = {1--55},
title = {{Graphical models and message-passing algorithms : Some introductory lectures}}
}
@inproceedings{Aha1992,
author = {Aha, David W.},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Aha - 1992 - Generalizing from case studies A case study.pdf:pdf},
title = {{Generalizing from case studies: A case study}},
url = {http://www.denizyuret.com/ref/aha/aha92generalizing.pdf},
year = {1992}
}
@article{Marchand2004,
author = {Marchand, E and Strawderman, WE},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Marchand, Strawderman - 2004 - Estimation in Restricted Parameter Spaces A Review.pdf:pdf},
journal = {Institute of Mathematical Statistics Lecture Notes-Monograph Series},
number = {2004},
pages = {21--44},
title = {{Estimation in Restricted Parameter Spaces: A Review}},
url = {http://www.jstor.org/stable/10.2307/4356296},
volume = {45},
year = {2004}
}
@article{Doornik2008,
author = {Doornik, Jurgen A. and Hansen, Henrik},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Doornik, Hansen - 2008 - An Omnibus Test for Univariate and Multivariate Normality.pdf:pdf},
journal = {Oxford Bulletin of Economics and Statistics},
keywords = {johnson system,kurtosis,multivariate normality test,ness,skew-,univariate normality test,wilson-hilferty transformation},
number = {1},
pages = {927--939},
title = {{An Omnibus Test for Univariate and Multivariate Normality}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1468-0084.2008.00537.x/full},
volume = {70},
year = {2008}
}
@article{Mirowski2008,
author = {Mirowski, Piotr W. and LeCun, Yann and Madhavan, Deepak and Kuzniecky, Ruben},
doi = {10.1109/MLSP.2008.4685487},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mirowski et al. - 2008 - Comparing SVM and convolutional networks for epileptic seizure prediction from intracranial EEG.pdf:pdf},
isbn = {978-1-4244-2375-0},
journal = {2008 IEEE Workshop on Machine Learning for Signal Processing},
month = oct,
pages = {244--249},
publisher = {Ieee},
title = {{Comparing SVM and convolutional networks for epileptic seizure prediction from intracranial EEG}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4685487},
year = {2008}
}
@book{Seeger2004,
abstract = {Gaussian processes (GPs) are natural generalisations of multivariate Gaussian random variables to infinite (countably or continuous) index sets. GPs have been applied in a large number of fields to a diverse range of ends, and very many deep theoretical analyses of various properties are available. This paper gives an introduction to Gaussian processes on a fairly elementary level with special emphasis on characteristics relevant in machine learning. It draws explicit connections to branches such as spline smoothing models and support vector machines in which similar ideas have been investigated. Gaussian process models are routinely used to solve hard machine learning problems. They are attractive because of their flexible non-parametric nature and computational simplicity. Treated within a Bayesian framework, very powerful statistical methods can be implemented which offer valid estimates of uncertainties in our predictions and generic model selection procedures cast as nonlinear optimization problems. Their main drawback of heavy computational scaling has recently been alleviated by the introduction of generic sparse approximations.13,78,31 The mathematical literature on GPs is large and often uses deep concepts which are not required to fully understand most machine learning applications. In this tutorial paper, we aim to present characteristics of GPs relevant to machine learning and to show up precise connections to other "kernel machines" popular in the community. Our focus is on a simple presentation, but references to more detailed sources are provided.},
author = {Seeger, Matthias},
booktitle = {International journal of neural systems},
doi = {10.1142/S0129065704001899},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Seeger - 2004 - Gaussian processes for machine learning.pdf:pdf},
isbn = {026218253X},
issn = {0129-0657},
keywords = {Algorithms,Artificial Intelligence,Bayes Theorem,Entropy,Linear Models,Models, Statistical,Normal Distribution,Regression Analysis,Statistics, Nonparametric},
month = apr,
number = {2},
pages = {69--106},
pmid = {15112367},
title = {{Gaussian processes for machine learning.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15112367},
volume = {14},
year = {2004}
}
@article{Gelman2013a,
author = {Gelman, Andrew},
doi = {10.1097/EDE.0b013e31827886f7},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman - 2013 - P values and statistical practice.pdf:pdf},
issn = {1531-5487},
journal = {Epidemiology},
month = jan,
number = {1},
pages = {69--72},
pmid = {23232612},
title = {{P values and statistical practice.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23232612},
volume = {24},
year = {2013}
}
@article{Nguyen2009,
author = {Nguyen, XuanLong and Wainwright, Martin J. and Jordan, Michael I.},
doi = {10.1214/08-AOS595},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nguyen, Wainwright, Jordan - 2009 - On surrogate loss functions and f -divergences.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = apr,
number = {2},
pages = {876--904},
title = {{On surrogate loss functions and f -divergences}},
url = {http://projecteuclid.org/euclid.aos/1236693153},
volume = {37},
year = {2009}
}
@article{Herrero-lopez,
author = {Herrero-lopez, Sergio and Williams, John R and Sanchez, Abel},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Herrero-lopez, Williams, Sanchez - Unknown - Parallel Multiclass Classification using SVMs on GPUs.pdf:pdf},
isbn = {9781605589350},
journal = {Memory},
keywords = {gpu,support vector machine},
pages = {2--11},
title = {{Parallel Multiclass Classification using SVMs on GPUs}}
}
@inproceedings{Zhou2005a,
author = {Zhou, Zhi-hua and Li, Ming},
booktitle = {International Joint Conferences on Artificial Intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Li - 2005 - Semi-Supervised Regression with Co-Training.pdf:pdf},
title = {{Semi-Supervised Regression with Co-Training.}},
url = {http://ijcai.org/Past Proceedings/IJCAI-05/PDF/0689.pdf},
year = {2005}
}
@article{Chapelle2006a,
address = {New York, New York, USA},
author = {Chapelle, Olivier and Chi, Mingmin and Zien, Alexander},
doi = {10.1145/1143844.1143868},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chapelle, Chi, Zien - 2006 - A continuation method for semi-supervised SVMs.pdf:pdf},
isbn = {1595933832},
journal = {Proceedings of the 23rd international conference on Machine learning - ICML '06},
pages = {185--192},
publisher = {ACM Press},
title = {{A continuation method for semi-supervised SVMs}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143868},
year = {2006}
}
@article{Kalousis2004,
author = {Kalousis, Alexandros and Gama, Joao and Hilario, Melanie},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kalousis, Gama, Hilario - 2004 - On data and algorithms Understanding inductive performance.pdf:pdf},
journal = {Machine Learning},
number = {3},
pages = {275--312},
title = {{On data and algorithms: Understanding inductive performance}},
url = {http://link.springer.com/article/10.1023/B:MACH.0000015882.38031.85},
volume = {54},
year = {2004}
}
@article{Bottou2013,
author = {Bottou, L\'{e}on},
doi = {10.1007/s10994-013-5335-x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bottou - 2013 - From machine learning to machine reasoning.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {machine learning,reasoning,recursive networks},
month = apr,
pages = {133--149},
title = {{From machine learning to machine reasoning}},
url = {http://link.springer.com/10.1007/s10994-013-5335-x},
year = {2013}
}
@book{Brazdil2010,
author = {Brazdil, Pavel B. and Bernstein, Abraham},
editor = {Brazdil, Pavel and Bernstein, Abraham},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brazdil, Bernstein - 2010 - Proceedings of 3rd Planning to Learn Workshop at ECAI 2010.pdf:pdf},
number = {Ecai},
title = {{Proceedings of 3rd Planning to Learn Workshop at ECAI 2010}},
year = {2010}
}
@inproceedings{White2012,
author = {White, Martha and Schuurmans, Dale},
booktitle = {International Conference on Artificial Intelligence and Statistics},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/White, Schuurmans - 2012 - Generalized optimal reverse prediction.pdf:pdf},
pages = {1305--1313},
title = {{Generalized optimal reverse prediction}},
url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/AISTATS2012\_WhiteS12.pdf},
year = {2012}
}
@article{White1982,
author = {White, Halbert},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/White - 1982 - Maximum Likelihood Estimation of Misspecified Models.pdf:pdf},
journal = {Econometrica},
number = {1},
pages = {1--25},
title = {{Maximum Likelihood Estimation of Misspecified Models}},
url = {http://www.jstor.org/stable/10.2307/1912526},
volume = {50},
year = {1982}
}
@inproceedings{Kuncheva2001,
author = {Kuncheva, Ludmila I and Roli, Fabio and Marcialis, Gian Luca and Shipp, Catherine A.},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kuncheva et al. - 2001 - Complexity of Data Subsets Generated by the Random Subspace Method An Experimental Investigation.pdf:pdf},
pages = {349--358},
title = {{Complexity of Data Subsets Generated by the Random Subspace Method: An Experimental Investigation}},
year = {2001}
}
@article{VanRooden2010,
abstract = {The clinical variability between patients with Parkinson's disease (PD) may point at the existence of subtypes of the disease. Identification of subtypes is important, since a focus on homogeneous groups may enhance the chance of success of research on mechanisms of disease and may also lead to tailored treatment strategies. Cluster analysis (CA) is an objective method to classify patients into subtypes. We systematically reviewed the methodology and results of CA studies in PD to gain a better understanding of the robustness of identified subtypes. We found seven studies that fulfilled the inclusion criteria. Studies were limited by incomplete reporting and methodological limitations. Differences between studies rendered comparisons of the results difficult. However, it appeared that studies which applied a comparable design identified similar subtypes. The cluster profiles "old age-at-onset and rapid disease progression" and "young age-at-onset and slow disease progression" emerged from the majority of studies. Other cluster profiles were less consistent across studies. Future studies with a rigorous study design that is standardized with respect to the included variables, data processing, and CA technique may advance the knowledge on subtypes in PD.},
author = {van Rooden, Stephanie M and Heiser, Willem J and Kok, Joost N and Verbaan, Dagmar and van Hilten, Jacobus J and Marinus, Johan},
doi = {10.1002/mds.23116},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/van Rooden et al. - 2010 - The identification of Parkinson's disease subtypes using cluster analysis a systematic review.pdf:pdf},
issn = {1531-8257},
journal = {Movement disorders : official journal of the Movement Disorder Society},
keywords = {Algorithms,Cluster Analysis,Humans,Parkinson Disease,Parkinson Disease: classification,PubMed,PubMed: statistics \& numerical data},
month = jun,
number = {8},
pages = {969--78},
pmid = {20535823},
title = {{The identification of Parkinson's disease subtypes using cluster analysis: a systematic review.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20535823},
volume = {25},
year = {2010}
}
@phdthesis{Eslick,
author = {Eslick, Ian Scott and Lieberman, Henry},
file = {::},
title = {{Personalized health experiments to optimize well-being and enable scientific discovery by}}
}
@article{Lupton,
author = {Lupton, Deborah},
file = {::},
journal = {Critical Public Health},
keywords = {digital health,health promotion,mHealth,quantified self,sociology of health},
title = {{Quantifying the body: monitoring and measuring health in the age of mHealth technologies}},
url = {http://www.tandfonline.com/doi/full/10.1080/09581596.2013.794931\#.UbZvNZN--V4}
}
@article{Larson2010,
author = {Larson, Eric B},
doi = {10.1007/s11606-010-1440-8},
file = {::},
issn = {1525-1497},
journal = {Journal of general internal medicine},
keywords = {Chronic Disease,Chronic Disease: drug therapy,Evidence-Based Medicine,Evidence-Based Medicine: methods,Evidence-Based Medicine: standards,Humans,Individualized Medicine,Randomized Controlled Trials as Topic,Randomized Controlled Trials as Topic: methods,Randomized Controlled Trials as Topic: standards,Randomized Controlled Trials as Topic: utilization},
month = sep,
number = {9},
pages = {891--2},
pmid = {20632123},
title = {{N-of-1 trials: a new future?}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2917674\&tool=pmcentrez\&rendertype=abstract},
volume = {25},
year = {2010}
}
@book{Perry2012,
author = {Perry, Bruce W.},
publisher = {O'Reilly Media, Inc.,},
title = {{Fitness for Geeks: Real Science, Great Nutrition, and Good Health.}},
year = {2012}
}
@inproceedings{Calimport2013,
abstract = {
Quantification of the processes that lead to a post-human state is key to the optimisation, scaling, distribution of post-human development. 

By making abstract and qualitative claims about post-humanism this fundamentally hinders progress to a post-human state and may lead to state changes that do not progress us towards a post-human state or particular post-human state of interest.

Quantified self and self-tracking methods and tools that utilise big data and personalised analytics show promise for performing experiments that lead to a post-human state. In addition, quantified self and self-tracking methods allow for states and qualities previously and widely considered qualitative to be quantified. States such as emotional states, elegance of form, brain states, interpersonal interaction and lifestyles can now, for some parameters, be measured quantitatively. 

By outlining clear milestones to reach and a mutual goal of achieving post-humanity this would allow focused work to optimise systems and innovate to reach a shared target. Quantification of ourselves and environments with self-tracking technologies and ubiquitous computing we can both see what processes, objects and states lead to characteristics we ascribe to the post-human, use the data for biofeedback and education, and engineer post-human futures.

A preliminary, longitudinal self-tracking project has been undertaken to track individual post-human development and the processes, objects or entities that may lead to post-human states. The proof-of-principle experiment tracks multi-level changes on the molecular, behavourial, cognitive, social, material and psychological level as an attempt is made to reach a post-human state with particular quantitative criteria. In this study the individual is attempting to achieve post-humanity as an intermediate state, with the core goal being an infinite lifespan through a systematic and comprehensive re-engineering of the self and universe.

The primary creation of the longitudinal project was a database of categorised  processes, states and objects that may lead us towards or away from an infinite post-human existence. This may be the first categorised database of its kind that seeks to catalog processes, states and objects that we could enrich for or remove from existence in order to attain an infinite post-human existence. This cataloging will enable milestones to be created, experiments to be performed, actions to be quantified and goals to be reached. 

By self-tracking and biofeedback the individual can modify their thoughts and behaviour in order to assist the development of post-human states. Self-tracking also allows sharing knowledge on how to develop into a post-human, and utilization of crowdsourcing becoming post-human. 

During the proof-of-principle experiment multiple other potentially quantifiable processes were uncovered that could be tracked via self-tracking technologies and big data such as totals and growth of peaceful entities, global loss of negative human morphological and behavioural traits, tracking and removal of non-aesthetic items that are not conducive to longevity and the post-human state.
},
address = {Roma, Italy},
author = {Calimport, Stuart Richard Gilbert},
booktitle = {Proceedings of the 5th Beyond Humanism Conference, The Posthuman: Differences, Embodiments, Performativity},
title = {{Quantification of Post-human Development}},
year = {2013}
}
@article{Neuhauser2011,
abstract = {Healthcare managers, clinical researchers and individual patients (and their physicians) manage variation differently to achieve different ends. First, managers are primarily concerned with the performance of care processes over time. Their time horizon is relatively short, and the improvements they are concerned with are pragmatic and 'holistic.' Their goal is to create processes that are stable and effective. The analytical techniques of statistical process control effectively reflect these concerns. Second, clinical and health-services researchers are interested in the effectiveness of care and the generalisability of findings. They seek to control variation by their study design methods. Their primary question is: 'Does A cause B, everything else being equal?' Consequently, randomised controlled trials and regression models are the research methods of choice. The focus of this reductionist approach is on the 'average patient' in the group being observed rather than the individual patient working with the individual care provider. Third, individual patients are primarily concerned with the nature and quality of their own care and clinical outcomes. They and their care providers are not primarily seeking to generalise beyond the unique individual. We propose that the gold standard for helping individual patients with chronic conditions should be longitudinal factorial design of trials with individual patients. Understanding how these three groups deal differently with variation can help appreciate these three approaches.},
annote = {        From Duplicate 1 (                           The meaning of variation to healthcare managers, clinical and health-services researchers, and individual patients.                         - Neuhauser, Duncan; Provost, Lloyd; Bergman, Bo )
                
        
        
      },
author = {Neuhauser, Duncan and Provost, Lloyd and Bergman, Bo},
doi = {10.1136/bmjqs.2010.046334},
file = {::},
issn = {2044-5423},
journal = {BMJ quality \& safety},
month = apr,
number = {Suppl 1},
pages = {i36--i40},
pmid = {21450768},
title = {{The meaning of variation to healthcare managers, clinical and health-services researchers, and individual patients.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21450768},
volume = {20 Suppl 1},
year = {2011}
}
@article{Roberts2004,
abstract = {Little is known about how to generate plausible new scientific ideas. So it is noteworthy that 12 years of self-experimentation led to the discovery of several surprising cause-effect relationships and suggested a new theory of weight control, an unusually high rate of new ideas. The cause-effect relationships were: (1) Seeing faces in the morning on television decreased mood in the evening (> 10 hrs later) and improved mood the next day (> 24 hrs later), yet had no detectable effect before that (0-10 hrs later). The effect was strongest if the faces were life-sized and at a conversational distance. Travel across time zones reduced the effect for a few weeks. (2) Standing 8 hours per day reduced early awakening and made sleep more restorative, even though more standing was associated with less sleep. (3) Morning light (1 hr/day) reduced early awakening and made sleep more restorative. (4) Breakfast increased early awakening. (5) Standing and morning light together eliminated colds (upper respiratory tract infections) for more than 5 years. (6) Drinking lots of water, eating low-glycemic-index foods, and eating sushi each caused a modest weight loss. (7) Drinking unflavored fructose water caused a large weight loss that has lasted more than 1 year. While losing weight, hunger was much less than usual. Unflavored sucrose water had a similar effect. The new theory of weight control, which helped discover this effect, assumes that flavors associated with calories raise the body-fat set point: The stronger the association, the greater the increase. Between meals the set point declines. Self-experimentation lasting months or years seems to be a good way to generate plausible new ideas.},
author = {Roberts, Seth},
file = {::},
issn = {0140-525X},
journal = {The Behavioral and brain sciences},
keywords = {Affect,Attitude to Health,Autoexperimentation,Body Weight,Face,Humans,Research Design,Self-Assessment,Sleep,Wakefulness,Weight Loss},
month = apr,
number = {2},
pages = {227--62; discussion 262--87},
pmid = {15595236},
title = {{Self-experimentation as a source of new ideas: ten examples about sleep, mood, health, and weight.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15595236},
volume = {27},
year = {2004}
}
@article{Chen2012,
abstract = {Personalized medicine is expected to benefit from combining genomic information with regular monitoring of physiological states by multiple high-throughput methods. Here, we present an integrative personal omics profile (iPOP), an analysis that combines genomic, transcriptomic, proteomic, metabolomic, and autoantibody profiles from a single individual over a 14 month period. Our iPOP analysis revealed various medical risks, including type 2 diabetes. It also uncovered extensive, dynamic changes in diverse molecular components and biological pathways across healthy and diseased conditions. Extremely high-coverage genomic and transcriptomic data, which provide the basis of our iPOP, revealed extensive heteroallelic changes during healthy and diseased states and an unexpected RNA editing mechanism. This study demonstrates that longitudinal iPOP can be used to interpret healthy and diseased states by connecting genomic information with additional dynamic omics activity.},
author = {Chen, Rui and Mias, George I and Li-Pook-Than, Jennifer and Jiang, Lihua and Lam, Hugo Y K and Chen, Rong and Miriami, Elana and Karczewski, Konrad J and Hariharan, Manoj and Dewey, Frederick E and Cheng, Yong and Clark, Michael J and Im, Hogune and Habegger, Lukas and Balasubramanian, Suganthi and O'Huallachain, Maeve and Dudley, Joel T and Hillenmeyer, Sara and Haraksingh, Rajini and Sharon, Donald and Euskirchen, Ghia and Lacroute, Phil and Bettinger, Keith and Boyle, Alan P and Kasowski, Maya and Grubert, Fabian and Seki, Scott and Garcia, Marco and Whirl-Carrillo, Michelle and Gallardo, Mercedes and Blasco, Maria a and Greenberg, Peter L and Snyder, Phyllis and Klein, Teri E and Altman, Russ B and Butte, Atul J and Ashley, Euan a and Gerstein, Mark and Nadeau, Kari C and Tang, Hua and Snyder, Michael},
doi = {10.1016/j.cell.2012.02.009},
file = {::},
issn = {1097-4172},
journal = {Cell},
keywords = {Diabetes Mellitus, Type 2,Diabetes Mellitus, Type 2: genetics,Female,Gene Expression Profiling,Genome, Human,Genomics,Humans,Individualized Medicine,Male,Metabolomics,Middle Aged,Mutation,Proteomics,Respiratory Syncytial Viruses,Respiratory Syncytial Viruses: isolation \& purific,Rhinovirus,Rhinovirus: isolation \& purification},
month = mar,
number = {6},
pages = {1293--307},
pmid = {22424236},
publisher = {Elsevier Inc.},
title = {{Personal omics profiling reveals dynamic molecular and medical phenotypes.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3341616\&tool=pmcentrez\&rendertype=abstract},
volume = {148},
year = {2012}
}
@article{Goldberger2013,
author = {Goldberger, Jeffrey J.},
issn = {0098-7484},
journal = {JAMA},
month = may,
pages = {1},
title = {{Personalized Medicine vs Guideline-Based Medicine<alt-title>Personalized Medicine vs Guideline-based Medicine</alt-title>}},
url = {http://jama.jamanetwork.com/article.aspx?articleid=1691756},
year = {2013}
}
@article{Olsson2005,
annote = {        From Duplicate 5 (                           The One-Person Randomized Controlled Trial MANAGEMENT OF INDIVIDUAL                         - Olsson, Jesper; Terris, Darcey; Elg, Matthias; Lundberg, Jonas )
                
        
        
      },
author = {Olsson, Jesper and Terris, Darcey and Elg, Matthias and Lundberg, Jonas},
file = {::},
journal = {Response},
number = {4},
pages = {206--216},
title = {{The One-Person Randomized Controlled Trial MANAGEMENT OF INDIVIDUAL}},
volume = {14},
year = {2005}
}
@inproceedings{Calimport2013a,
abstract = {Socio-cultural markers such as attitudes, behaviours, ideas, aspirations and interests were collected and correlated to previously identified social, psychological and quantitative predictors of lifespan. Attitudes to whether a person wanted to live for as long as possible and if so in what condition were also surveyed. The aim of the study was to identify trends, correlations and multivariate, interdisciplinary markers of health that contain word and phrase based socio-cultural markers as well as other known longevity markers. This would lead to enhanced accuracy in predictions of current health states and longevity predictions. Text-data analysis was chosen to find novel longevity markers, as text-data is easily surveyed, available, digitized and sequence-based. As more than an analogy to The Human Genome Project, The Human Memome Project sets out to record personalised text-sequence data for individuals to be able to correlate particular text-sequences to specific health markers associated with longevity. Data was gathered from over 300 participants, 25 countries, 6 continents, in an 18-70-age range. We have attempted to use varied statistical analysis and computational modeling techniques to build optimal sets of attitudes, behaviours and interests that correlate the most with longevity and find text-sequences that are correlated the most or least with longevity. This project presents a proof-of-principle experiment that could be developed into advanced technology platforms, big data analysis, predictive analytics and personal health tools for the healthcare, preventative medicine and global, local or personal, real-time longevity risk analysis. The Human Memome Project also presents a key engagement with the Crowd Sourcing, Citizen Science and Quantified Self communities for research into longevity.},
author = {Calimport, Stuart Richard Gilbert. and Bentley, Barry},
file = {::},
publisher = {Rejuvenation Research},
title = {{The Human Memome Project: Text-data analytics to find socio-cultural predictors of longevity utilising the Quantified Self, crowdsourcing and citizen science communities}},
year = {2013}
}
@phdthesis{Watson2013,
author = {Watson, Sara M.},
keywords = {Big Data,conceptual metaphors,data lifecycle,data self,everyday life,personal data,quantified self,self-tracking,use},
pages = {48},
school = {Oxford University},
title = {{Living with Data: Personal Data Uses of the Quantified Self}},
url = {http://www.scribd.com/doc/172418320/Living-With-Data-Personal-Data-Uses-of-the-Quantified-Self},
year = {2013}
}
