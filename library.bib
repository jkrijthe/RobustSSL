Automatically generated by Mendeley Desktop 1.17.10
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@misc{Krijthe2016limits,
archivePrefix = {arXiv},
arxivId = {1612.08875},
author = {Krijthe, Jesse Hendrik and Loog, Marco},
eprint = {1612.08875},
title = {{The Pessimistic Limits of Margin-based Losses in Semi-supervised Learning}},
year = {2016}
}
@article{Masnadi-shirazi2015,
author = {Masnadi-shirazi, Hamed},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Masnadi-shirazi - 2015 - A View of Margin Losses as Regularizers of Probability Estimates.pdf:pdf},
keywords = {boosting,classification,margin losses,probability elicitation,regularization},
pages = {2751--2795},
title = {{A View of Margin Losses as Regularizers of Probability Estimates}},
volume = {16},
year = {2015}
}
@article{Zhou2005,
author = {Zhou, Zhi-hua and Li, Ming},
doi = {10.1109/TKDE.2005.186},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Li - 2005 - Tri-training exploiting unlabeled data using three classifiers.pdf:pdf},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
month = {nov},
number = {11},
pages = {1529--1541},
title = {{Tri-training: exploiting unlabeled data using three classifiers}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1512038},
volume = {17},
year = {2005}
}
@article{Hosmer1973,
abstract = {The iterative maximum likelihood (ML) estimates proposed by Hasselblad [1966; 1967; 1969] are considered under each of three different types of sample. One sample type is the case where only data from the mixture is available. The other two types have a small sample from each component and a larger sample from the mixture. A small Monte Carlo study shows that when the known samples represent as little as 10{\%} of the total sample considerable gains in efficiency are possible over the estimates computed from the no known data sample. An example using the lengths of male and female halibut is presented.},
author = {Hosmer, David W. Jr},
doi = {10.2307/2529141},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/2529141.pdf:pdf},
issn = {0006-341X},
journal = {Biometrics},
number = {4},
pages = {761--770},
title = {{A Comparison of Iterative Maximum Likelihood Estimates of the Parameters of a Mixture of Two Normal Distributions Under Three Different Types of Sample}},
url = {http://www.jstor.org/stable/2529141{\%}5Cnpapers://28b4d2b8-358f-48a3-a2ff-2ddb6af66501/Paper/p5910},
volume = {29},
year = {1973}
}
@article{Tulabandhula2015,
author = {Tulabandhula, Theja and Rudin, Cynthia},
doi = {10.1007/s10994-014-5478-4},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tulabandhula, Rudin - 2015 - Generalization Bounds for Learning with Linear and Quadratic Side Knowledge.pdf:pdf},
journal = {Machine Learning},
pages = {183--216},
title = {{Generalization Bounds for Learning with Linear and Quadratic Side Knowledge}},
volume = {100},
year = {2015}
}
@book{Chapelle2006,
address = {Cambridge, MA},
author = {Chapelle, Olivier and Sch{\"{o}}lkopf, Bernhard and Zien, Alexander},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chapelle, Sch{\"{o}}lkopf, Zien - 2006 - Semi-supervised learning.pdf:pdf},
isbn = {9780262033589},
publisher = {MIT press},
title = {{Semi-supervised learning}},
year = {2006}
}
@article{Alaa2017,
abstract = {We consider the problem of obtaining individualized estimates for the effect of a certain treatment given observational data. The problem differs fundamentally from classical supervised learning since for each individual subject, we either observe the response with or without the treatment but never both. Hence, estimating the effect of a treatment entails a causal inference task in which we need to estimate counterfactual outcomes. To address this problem, we propose a novel multi-task learning framework in which the individuals' responses with and without the treatment are modeled as a vector-valued function that belongs to a reproducing kernel Hilbert space. Unlike previous methods for causal inference that use the G-computation formula, our approach does not obtain separate estimates for the treatment and control response surfaces, but rather obtains a joint estimate that ensures data efficiency in scenarios where the selection bias is strong. In order to be able to provide individualized measures of uncertainty in our estimates, we adopt a Bayesian approach for learning this vector-valued function using a multi-task Gaussian process prior; uncertainty is quantified via posterior credible intervals. We develop a novel risk based empirical Bayes approach for calibrating the Gaussian process hyper-parameters in a data-driven fashion based on gradient descent in which the update rule is itself learned from the data using a recurrent neural network. Experiments conducted on semi-synthetic data show that our algorithm significantly outperforms state-of-the-art causal inference methods.},
archivePrefix = {arXiv},
arxivId = {1704.02801},
author = {Alaa, Ahmed M. and van der Schaar, Mihaela},
eprint = {1704.02801},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/1704.02801.pdf:pdf},
journal = {arXiv},
number = {Nips},
pages = {1--11},
title = {{Bayesian Inference of Individualized Treatment Effects using Multi-task Gaussian Processes}},
url = {http://arxiv.org/abs/1704.02801},
year = {2017}
}
@article{Rubin1972,
author = {Rubin, Donald B.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rubin - 1972 - A Non-Iterative Algorithm for Least Squares Estimation of Missing Values in Any Analysis of Variance Design.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
number = {2},
pages = {136--141},
title = {{A Non-Iterative Algorithm for Least Squares Estimation of Missing Values in Any Analysis of Variance Design}},
volume = {21},
year = {1972}
}
@inproceedings{Scholkopf2012,
author = {Sch{\"{o}}lkopf, Bernhard and Janzing, Dominik and Peters, Jonas},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sch{\"{o}}lkopf, Janzing, Peters - 2012 - On Causal and Anticausal Learning.pdf:pdf},
pages = {1255--1262},
title = {{On Causal and Anticausal Learning}},
url = {http://arxiv.org/abs/1206.6471},
year = {2012}
}
@inproceedings{Bottou2010,
author = {Bottou, Leon},
booktitle = {Proceedings of the 21st International Conference on Computational Statistics (COMPSTAT)},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bottou - 2010 - Large-scale machine learning with stochastic gradient descent.pdf:pdf},
keywords = {efficiency,online learning,stochastic gradient descent},
pages = {177--186},
publisher = {Springer},
title = {{Large-scale machine learning with stochastic gradient descent}},
year = {2010}
}
@techreport{Wolpert1996,
author = {Wolpert, David H and Macready, W},
booktitle = {Santa Fe Institute Technical Report},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wolpert, Macready - 1996 - Combining Stacking With Bagging To Improve A Learning Algorithm.pdf:pdf},
pages = {1--28},
title = {{Combining Stacking With Bagging To Improve A Learning Algorithm}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.53.9933{\&}rep=rep1{\&}type=pdf},
year = {1996}
}
@inproceedings{Sa1994,
author = {Sa, Virginia R De},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sa - 1994 - Learning Classification with Unlabeled Data.pdf:pdf},
pages = {112--112},
title = {{Learning Classification with Unlabeled Data}},
year = {1994}
}
@unpublished{Savov,
author = {Savov, Ivan},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Savov - Unknown - Linear algebra explained in four pages.pdf:pdf},
pages = {1--4},
title = {{Linear algebra explained in four pages}}
}
@article{Brodley1995,
author = {Brodley, Carla E.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brodley - 1995 - Recursive automatic bias selection for classifier construction.pdf:pdf},
journal = {Machine Learning},
keywords = {automatic algorithm selection,decision trees,hybrid classifiers,inductive bias,learning from},
pages = {63--94},
title = {{Recursive automatic bias selection for classifier construction}},
url = {http://link.springer.com/article/10.1023/A:1022686102325},
volume = {94},
year = {1995}
}
@article{Hanselmann2013,
abstract = {Digital staining for the automated annotation of mass spectrometry imaging (MSI) data has previously been achieved using state-of-the-art classifiers such as random forests or support vector machines (SVMs). However, the training of such classifiers requires an expert to label exemplary data in advance. This process is time-consuming and hence costly, especially if the tissue is heterogeneous. In theory, it may be sufficient to only label a few highly representative pixels of an MS image, but it is not known a priori which pixels to select. This motivates active learning strategies in which the algorithm itself queries the expert by automatically suggesting promising candidate pixels of an MS image for labeling. Given a suitable querying strategy, the number of required training labels can be significantly reduced while maintaining classification accuracy. In this work, we propose active learning for convenient annotation of MSI data. We generalize a recently proposed active learning method to the multiclass case and combine it with the random forest classifier. Its superior performance over random sampling is demonstrated on secondary ion mass spectrometry data, making it an interesting approach for the classification of MS images.},
author = {Hanselmann, Michael and R{\"{o}}der, Jens and K{\"{o}}the, Ullrich and Renard, Bernhard Y and Heeren, Ron M.A. and Hamprecht, Fred A.},
doi = {10.1021/ac3023313},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hanselmann et al. - 2013 - Active learning for convenient annotation and classification of secondary ion mass spectrometry images.pdf:pdf},
issn = {1520-6882},
journal = {Analytical Chemistry},
month = {jan},
number = {1},
pages = {147--55},
pmid = {23157438},
title = {{Active learning for convenient annotation and classification of secondary ion mass spectrometry images.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23157438},
volume = {85},
year = {2013}
}
@book{Quinonero-Candela2009,
author = {Quinonero-Candela, Joaquin and Sugiyama, Masashi and Schwaighofer, Anton and Lawrence, Neil D.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Quinonero-Candela et al. - 2009 - Dataset Shift in Machine Learning.pdf:pdf},
isbn = {9780262170055},
title = {{Dataset Shift in Machine Learning}},
url = {http://dl.acm.org/citation.cfm?id=1462129},
year = {2009}
}
@article{Hunt1990,
author = {Hunt, D. N. and Bell, M. J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hunt, Bell - 1990 - Semi-iterative missing value estimation.pdf:pdf},
journal = {Journal of Applied Statistics},
number = {3},
pages = {389--396},
title = {{Semi-iterative missing value estimation}},
volume = {17},
year = {1990}
}
@article{Arlot2010,
author = {Arlot, Sylvain and Celisse, Alain},
doi = {10.1214/09-SS054},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Arlot, Celisse - 2010 - A survey of cross-validation procedures for model selection.pdf:pdf},
issn = {1935-7516},
journal = {Statistics Surveys},
keywords = {and phrases,cross-validation,leave-one-out,model selection},
pages = {40--79},
title = {{A survey of cross-validation procedures for model selection}},
url = {http://projecteuclid.org/euclid.ssu/1268143839},
volume = {4},
year = {2010}
}
@article{Wang2009,
author = {Wang, Xiaozhe and Smith-Miles, Kate and Hyndman, Rob},
doi = {10.1016/j.neucom.2008.10.017},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Smith-Miles, Hyndman - 2009 - Rule induction for forecasting method selection Meta-learning the characteristics of univariate time.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
month = {jun},
number = {10-12},
pages = {2581--2594},
title = {{Rule induction for forecasting method selection: Meta-learning the characteristics of univariate time series}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231208005134},
volume = {72},
year = {2009}
}
@incollection{Opper2001,
author = {Opper, Manfred},
booktitle = {Frontiers of Life: Intelligent systems},
editor = {Baltimore, David},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Opper - 2001 - Learning to generalize.pdf:pdf},
isbn = {0120773430},
pages = {763--775},
publisher = {Academic Press},
title = {{Learning to generalize}},
year = {2001}
}
@article{Senn2011,
author = {Senn, Stephen},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Senn - 2011 - You may believe you are a Bayesian but you are probably wrong.pdf:pdf},
journal = {Rationality, Markets and Morals},
pages = {48--66},
title = {{You may believe you are a Bayesian but you are probably wrong}},
url = {http://www.rmm-journal.com/downloads/Article{\_}Senn.pdf},
volume = {2},
year = {2011}
}
@article{Yoder2016,
abstract = {Traditionally, practitioners initialize the {\{}$\backslash$tt k-means{\}} algorithm with centers chosen uniformly at random. Randomized initialization with uneven weights ({\{}$\backslash$tt k-means++{\}}) has recently been used to improve the performance over this strategy in cost and run-time. We consider the k-means problem with semi-supervised information, where some of the data are pre-labeled, and we seek to label the rest according to the minimum cost solution. By extending the {\{}$\backslash$tt k-means++{\}} algorithm and analysis to account for the labels, we derive an improved theoretical bound on expected cost and observe improved performance in simulated and real data examples. This analysis provides theoretical justification for a roughly linear semi-supervised clustering algorithm.},
archivePrefix = {arXiv},
arxivId = {1602.00360},
author = {Yoder, Jordan and Priebe, Carey E.},
eprint = {1602.00360},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yoder, Priebe - 2016 - Semi-supervised K-means.pdf:pdf},
keywords = {approximation,clustering,kmeans,partially labeled,semi-supervised},
number = {1},
pages = {1--16},
title = {{Semi-supervised K-means++}},
url = {http://arxiv.org/abs/1602.00360},
year = {2016}
}
@techreport{Freeman2013,
abstract = {The jump from problem sets to research can be hard. We sometimes see students who ace their classes struggle with their research. In little bites, here is what I think is important for succeeding in research as a graduate student. • The first advice can go on a bumper sticker: " Slow down to speed up " . In classes, the world is rigged. There's a simple correct answer and the problem is structured to let you come to that answer. You get feedback with the correct answer within a day after you submit anything. Research is different. No one tells you the right answer, we don't know if there is a right answer. We don't know if something doesn't work because there's a silly mistake in the program or because a broad set of assumptions is flawed. How do you deal with that? Take things slowly. Verify your assumptions. Understand the thing, whatever it is–the program, the algorithm, or the proof. As you do experiments, only change one thing at a time, so you know what the outcome of the experiment means. It may feel like you're going slowly, but you'll be making much more progress than if you flail around, trying different things, but not understanding what's going on. Figure 1: Research advice for a bumper sticker. • Please don't tell me " it doesn't work " . Of course it doesn't work. If there's a single mistake in the chain, the whole thing won't work, and how could you possibly go through all those steps without making a mistake somewhere? What I want to hear instead is something like, " I've narrowed down the problem to step B. Until step A, you can see that it works, because you put in X and you get Y out, as we expect. You can see how it fails here at B. I've ruled out W and Z as the cause. " • " This sounds like hard work. " Yes. It's no longer about being smart. By now, everyone around you is smart. In graduate school, it's the hard workers who pull ahead. This happens in sports, too. You always read stories about how hard the great players work, being the first ones out to practice, the last ones to leave, etc.},
author = {Freeman, Bill},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Freeman - 2013 - How to Do Research.pdf:pdf},
pages = {3},
title = {{How to Do Research}},
year = {2013}
}
@article{Liu2013,
author = {Liu, J. and Gelman, Andrew and Hill, J. and Su, Y.-S. and Kropko, J.},
doi = {10.1093/biomet/ast044},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Liu et al. - 2013 - On the stationary distribution of iterative imputations.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = {nov},
number = {1},
pages = {155--173},
title = {{On the stationary distribution of iterative imputations}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/ast044},
volume = {101},
year = {2013}
}
@article{Betancourt2015a,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02273v1},
author = {Betancourt, Michael},
eprint = {arXiv:1506.02273v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Betancourt - 2015 - A Unified Treatment of Predictive Model Comparison.pdf:pdf},
pages = {1--20},
title = {{A Unified Treatment of Predictive Model Comparison}},
year = {2015}
}
@inproceedings{Ogawa2013,
author = {Ogawa, Kohei and Imamura, Motoki and Takeuchi, Ichiro and Sugiyama, Masashi},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ogawa et al. - 2013 - Infinitesimal Annealing for Training Semi-Supervised Support Vector Machines.pdf:pdf},
pages = {897--905},
title = {{Infinitesimal Annealing for Training Semi-Supervised Support Vector Machines}},
url = {http://sugiyama-www.cs.titech.ac.jp/{~}sugi/2013/ICML2013b.pdf},
year = {2013}
}
@unpublished{Grunwald2014,
archivePrefix = {arXiv},
arxivId = {1412.3730},
author = {Gr{\"{u}}nwald, Peter and van Ommen, Thijs},
eprint = {1412.3730},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gr{\"{u}}nwald, Ommen - 2014 - Inconsistency of Bayesian Inference for Misspecified Linear Models , and a Proposal for Repairing It.pdf:pdf},
pages = {1--70},
title = {{Inconsistency of Bayesian Inference for Misspecified Linear Models , and a Proposal for Repairing It}},
year = {2014}
}
@article{Liu2017,
abstract = {Background: Cognitive decline is a debilitating manifestation of disease progression in Parkinson's disease. We aimed to develop a clinical-genetic score to predict global cognitive impairment in patients with the disease. Methods: In this longitudinal analysis, we built a prediction algorithm for global cognitive impairment (defined as Mini Mental State Examination [MMSE] ≤25) using data from nine cohorts of patients with Parkinson's disease from North America and Europe assessed between 1986 and 2016. Candidate predictors of cognitive decline were selected through a backward eliminated Cox's proportional hazards analysis using the Akaike's information criterion. These were used to compute the multivariable predictor on the basis of data from six cohorts included in a discovery population. Independent replication was attained in patients from a further three independent longitudinal cohorts. The predictive score was rebuilt and retested in 10 000 training and test sets randomly generated from the entire study population. Findings: 3200 patients with Parkinson's disease who were longitudinally assessed with 27 022 study visits between 1986 and 2016 in nine cohorts from North America and Europe were assessed for eligibility. 235 patients with MMSE ≤25 at baseline and 135 whose first study visit occurred more than 12 years from disease onset were excluded. The discovery population comprised 1350 patients (after further exclusion of 334 with missing covariates) from six longitudinal cohorts with 5165 longitudinal visits over 12{\textperiodcentered}8 years (median 2{\textperiodcentered}8, IQR 1{\textperiodcentered}6-4{\textperiodcentered}6). Age at onset, baseline MMSE, years of education, motor exam score, sex, depression, and $\beta$-glucocerebrosidase (GBA) mutation status were included in the prediction model. The replication population comprised 1132 patients (further excluding 14 patients with missing covariates) from three longitudinal cohorts with 19 127 follow-up visits over 8{\textperiodcentered}6 years (median 6{\textperiodcentered}5, IQR 4{\textperiodcentered}1-7{\textperiodcentered}2). The cognitive risk score predicted cognitive impairment within 10 years of disease onset with an area under the curve (AUC) of more than 0{\textperiodcentered}85 in both the discovery (95{\%} CI 0{\textperiodcentered}82-0{\textperiodcentered}90) and replication (95{\%} CI 0{\textperiodcentered}78-0{\textperiodcentered}91) populations. Patients scoring in the highest quartile for cognitive risk score had an increased hazard for global cognitive impairment compared with those in the lowest quartile (hazard ratio 18{\textperiodcentered}4 [95{\%} CI 9{\textperiodcentered}4-36{\textperiodcentered}1]). Dementia or disabling cognitive impairment was predicted with an AUC of 0{\textperiodcentered}88 (95{\%} CI 0{\textperiodcentered}79-0{\textperiodcentered}94) and a negative predictive value of 0{\textperiodcentered}92 (95{\%} 0{\textperiodcentered}88-0{\textperiodcentered}95) at the predefined cutoff of 0{\textperiodcentered}196. Performance was stable in 10 000 randomly resampled subsets. Interpretation: Our predictive algorithm provides a potential test for future cognitive health or impairment in patients with Parkinson's disease. This model could improve trials of cognitive interventions and inform on prognosis. Funding: National Institutes of Health, US Department of Defense.},
author = {Liu, Ganqiang and Locascio, Joseph J. and Corvol, Jean Christophe and Boot, Brendon and Liao, Zhixiang and Page, Kara and Franco, Daly and Burke, Kyle and Jansen, Iris E. and Trisini-Lipsanopoulos, Ana and Winder-Rhodes, Sophie and Tanner, Caroline M. and Lang, Anthony E. and Eberly, Shirley and Elbaz, Alexis and Brice, Alexis and Mangone, Graziella and Ravina, Bernard and Shoulson, Ira and Cormier-Dequaire, Florence and Heutink, Peter and van Hilten, Jacobus J. and Barker, Roger A. and Williams-Gray, Caroline H. and Marinus, Johan and Scherzer, Clemens R.},
doi = {10.1016/S1474-4422(17)30122-9},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Liu{\_}LancetNeurol (2017) Prediction of cognition in PD - analyses of nine cohorts.pdf:pdf},
issn = {14744465},
journal = {The Lancet Neurology},
number = {August},
pmid = {28629879},
title = {{Prediction of cognition in Parkinson's disease with a clinical-genetic score: A longitudinal analysis of nine cohorts}},
volume = {16},
year = {2017}
}
@inproceedings{Liu2014,
author = {Liu, Anqi and Ziebart, Brian D},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Liu, Ziebart - 2014 - Robust Classification Under Sample Selection Bias.pdf:pdf},
pages = {37--45},
title = {{Robust Classification Under Sample Selection Bias}},
year = {2014}
}
@article{Salimans2013,
abstract = {We propose a general algorithm for approximating nonstandard Bayesian posterior distributions. The algorithm minimizes the Kullback-Leibler divergence of an approximating distribution to the intractable posterior distribution. Our method can be used to approximate any posterior distribution, provided that it is given in closed form up to the proportionality constant. The approximation can be any distribution in the exponential family or any mixture of such distributions, which means that it can be made arbitrarily precise. Several examples illustrate the speed and accuracy of our approximation method in practice.},
archivePrefix = {arXiv},
arxivId = {1206.6679},
author = {Salimans, Tim and Knowles, David A.},
doi = {10.1214/13-BA858},
eprint = {1206.6679},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/euclid.ba.1386166315.pdf:pdf},
isbn = {1401.2135},
issn = {19360975},
journal = {Bayesian Analysis},
keywords = {Approximate inference,Stochastic approximation,Variational bayes},
number = {4},
pages = {837--882},
title = {{Fixed-form variational posterior approximation through stochastic linear regression}},
volume = {8},
year = {2013}
}
@article{Janzing2013,
author = {Janzing, Dominik and Balduzzi, David and Grosse-Wentrup, Moritz and Sch{\"{o}}lkopf, Bernhard},
doi = {10.1214/13-AOS1145},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Janzing et al. - 2013 - Quantifying causal influences.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {oct},
number = {5},
pages = {2324--2358},
title = {{Quantifying causal influences}},
url = {http://projecteuclid.org/euclid.aos/1383661266},
volume = {41},
year = {2013}
}
@article{Johnson,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.01255v1},
author = {Johnson, Rie},
eprint = {arXiv:1504.01255v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Johnson - Unknown - Semi-Supervised Learning with Multi-View Embedding Theory and Application with Convolutional Neural Networks.pdf:pdf},
pages = {1--16},
title = {{Semi-Supervised Learning with Multi-View Embedding : Theory and Application with Convolutional Neural Networks}}
}
@inproceedings{Nadler2009,
abstract = {We study the behavior of the popular Laplacian Regularization method for Semi-Supervised Learning at the regime of a fixed number of labeled points but a large number of unlabeled points. We show that in ℝ d, d ≥ 2, the method is actually not well-posed, and as the number of unlabeled points increases the solution degenerates to a noninformative function. We also contrast the method with the Laplacian Eigenvector method, and discuss the "smoothness" assumptions associated with this alternate method.},
author = {Nadler, Boaz and Srebro, Nathan and Zhou, Xueyuan},
booktitle = {Advances in Neural Information Processing Systems 22},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nadler, Srebro, Zhou - 2009 - Semi-supervised learning with the graph laplacian The limit of infinite unlabelled data.pdf:pdf},
isbn = {9781615679119},
pages = {1331--1338},
title = {{Semi-supervised learning with the graph laplacian: The limit of infinite unlabelled data}},
year = {2009}
}
@article{McLachlan1982,
author = {McLachlan, Geoffrey J. and Ganesalingam, S.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/McLachlan, Ganesalingam - 1982 - Updating a discriminant function on the basis of unclassified data.pdf:pdf},
journal = {Communication in Statistics- Simulation and Computation},
number = {6},
pages = {753--767},
title = {{Updating a discriminant function on the basis of unclassified data}},
url = {http://www.tandfonline.com/doi/full/10.1080/03610918208812293},
volume = {11},
year = {1982}
}
@inproceedings{Hoi2006,
abstract = {The goal of active learning is to select the most informative examples for manual labeling. Most of the previous studies in active learning have focused on selecting a single unlabeled example in each iteration. This could be ineﬃcient since the classiﬁcation model has to be retrained for every labeled example. In this paper, we present a framework for “batch mode active learning” that applies the Fisher information matrix to select a number of informative examples simultaneously. The key computational challenge is how to eﬃciently identify the subset of unlabeled examples that can result in the largest reduction in the Fisher information. To resolve this challenge, we propose an eﬃcient greedy algorithm that is based on the property of submodular functions. Our empirical studies with ﬁve UCI datasets and one realworld medical image classiﬁcation show that the proposed batch mode active learning algorithm is more eﬀective than the state-ofthe-art algorithms for active learning},
annote = {Badly written},
author = {Hoi, Steven C H and Jin, Rong and Zhu, Jianke and Lyu, Michael R},
booktitle = {Proceedings of the 23rd International Conference on Machine learning},
doi = {10.1145/1143844.1143897},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hoi et al. - 2006 - Batch mode active learning and its application to medical image classification.pdf:pdf},
isbn = {1595933832},
pages = {417--424},
title = {{Batch mode active learning and its application to medical image classification}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143897},
year = {2006}
}
@article{Li2009a,
abstract = {Semi-Supervised Support Vector Machines (S3VMs) typically directly$\backslash$nestimate the label assignments for the unlabeled instances. This$\backslash$nis often inefficient even with recent advances in the efficient training$\backslash$nof the (supervised) SVM. In this paper, we show that S3VMs, with$\backslash$nknowledge of the means of the class labels of the unlabeled data,$\backslash$nis closely related to the supervised SVM with known labels on all$\backslash$nthe unlabeled data. This motivates us to first estimate the label$\backslash$nmeans of the unlabeled data. Two versions of the meanS3VM, which$\backslash$nwork by maximizing the margin between the label means, are proposed.$\backslash$nThe first one is based on multiple kernel learning, while the second$\backslash$none is based on alternating optimization. Experiments show that both$\backslash$nof the proposed algorithms achieve highly competitive and sometimes$\backslash$neven the best performance as compared to the state-of-the-art semi-supervised$\backslash$nlearners. Moreover, they are more efficient than existing S3VMs.},
author = {Li, Yu-Feng and Kwok, James T. and Zhou, Zhi-hua},
doi = {10.1145/1553374.1553456},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li, Kwok, Zhou - 2009 - Semi-supervised learning using label mean.pdf:pdf},
isbn = {9781605585161},
journal = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {633--640},
title = {{Semi-supervised learning using label mean}},
url = {http://dl.acm.org/citation.cfm?id=1553456},
year = {2009}
}
@inproceedings{Goldberg2007,
author = {Goldberg, Andrew B. and Zhu, Xiaojin and Wright, Stephen},
booktitle = {International Conference on Artificial Intelligence and Statistics},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Goldberg, Zhu, Wright - 2007 - Dissimilarity in graph-based semi-supervised classification.pdf:pdf},
number = {1},
pages = {55--162},
title = {{Dissimilarity in graph-based semi-supervised classification}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS07{\_}GoldbergZW.pdf},
year = {2007}
}
@article{Kakade2008,
abstract = {This work characterizes the generalization ability of algorithms whose predictions are linear in the input vector. To this end, we provide sharp bounds for Rademacher and Gaussian complexities of (constrained) linear classes, which directly lead to a number of generalization bounds. This derivation provides simpliﬁed proofs of a number of corollaries including: risk bounds for linear prediction (including settings where the weight vectors are constrained by either L2 or L1 constraints), margin bounds (including both L2 and L1 margins, along with more general notions based on relative entropy), a proof of the PAC-Bayes theorem, and upper bounds on L2 covering numbers (with Lp norm constraints and relative entropy constraints). In addition to providing a uniﬁed analysis, the results herein provide some of the sharpest risk and margin bounds. Interestingly, our results show that the uniform convergence rates of empirical risk minimization algorithms tightly match the regret bounds of online learning algorithms for linear prediction, up to a constant factor of 2.},
author = {Kakade, Sham M. and Sridharan, Karthik and Tewari, Ambuj},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kakade, Sridharan, Tewari - 2008 - On the Complexity of Linear Prediction risk bounds, margin bounds, and regularization.pdf:pdf},
isbn = {9781605609492},
journal = {Advances in Neural Information Processing Systems},
pages = {1--11},
title = {{On the Complexity of Linear Prediction : risk bounds, margin bounds, and regularization}},
url = {https://papers.nips.cc/paper/3510-on-the-complexity-of-linear-prediction-risk-bounds-margin-bounds-and-regularization.pdf},
year = {2008}
}
@inproceedings{Macia2010,
author = {Macia, Nuria and Ho, Tin Kam and Orriols-puig, Albert and Bernad{\'{o}}-Mansilla, Ester},
booktitle = {Proceedings of the 20th International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Macia et al. - 2010 - The Landscape Contest at ICPR 2010.pdf:pdf},
pages = {29--45},
title = {{The Landscape Contest at ICPR 2010}},
year = {2010}
}
@article{Gelman2011a,
author = {Gelman, Andrew},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman - 2011 - Ethics and statistics Open data and open methods.pdf:pdf},
journal = {Chance},
pages = {51--53},
title = {{Ethics and statistics: Open data and open methods}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Ethics+and+Statistics+Open+Data+and+Open+Methods{\#}3},
year = {2011}
}
@article{Bottou2012,
author = {Bottou, L{\'{e}}on},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bottou - 2012 - Stochastic Gradient Descent Tricks.pdf:pdf},
journal = {Neural Networks: Tricks of the Trade},
number = {1},
pages = {1--16},
title = {{Stochastic Gradient Descent Tricks}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-35289-8{\_}25},
year = {2012}
}
@techreport{Welling,
author = {Welling, Max},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Welling - Unknown - Kernel ridge Regression.pdf:pdf},
number = {3},
pages = {3--5},
title = {{Kernel ridge Regression}}
}
@article{Fourure,
author = {Fourure, Damien and Fromont, Elisa and Muselet, Damien and Tr, Alain and Wolf, Christian},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fourure et al. - Unknown - Semantic Segmentation via Multi-task , Multi-domain Learning.pdf:pdf},
keywords = {convolutional neural networks,deep learning,domain adaptation,multi-task learning,segmentation,semantic},
title = {{Semantic Segmentation via Multi-task , Multi-domain Learning}}
}
@article{Soares2004,
author = {Soares, Carlos and Brazdil, Pavel B. and Kuba, Petr},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Soares, Brazdil, Kuba - 2004 - A Meta-Learning Method to Select the KernelWidth in Support Vector Regression.pdf:pdf},
journal = {Machine learning},
keywords = {gaussian kernel,learning rankings,meta-learning,parameter setting,support vector machines},
pages = {195--209},
title = {{A Meta-Learning Method to Select the KernelWidth in Support Vector Regression}},
url = {http://link.springer.com/article/10.1023/b:mach.0000015879.28004.9b},
volume = {54},
year = {2004}
}
@book{Grunwald2007,
author = {Grunwald, Peter D.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Grunwald - 2007 - The minimum description length principle.pdf:pdf},
isbn = {9780262072816},
publisher = {MIT Press},
title = {{The minimum description length principle}},
year = {2007}
}
@article{Kalousis2004,
author = {Kalousis, Alexandros and Gama, Joao and Hilario, Melanie},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kalousis, Gama, Hilario - 2004 - On data and algorithms Understanding inductive performance.pdf:pdf},
journal = {Machine Learning},
number = {3},
pages = {275--312},
title = {{On data and algorithms: Understanding inductive performance}},
url = {http://link.springer.com/article/10.1023/B:MACH.0000015882.38031.85},
volume = {54},
year = {2004}
}
@article{Ganesalingam1978,
author = {Ganesalingam, S. and Mclachlan, Geoffrey John},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ganesalingam, Mclachlan - 1978 - The efficiency of a linear disriminant function based on unclassified initial samples.pdf:pdf},
journal = {Biometrica},
number = {3},
pages = {658--665},
title = {{The efficiency of a linear disriminant function based on unclassified initial samples}},
volume = {65},
year = {1978}
}
@inproceedings{VanOmmen2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1406.6200v1},
author = {van Ommen, Thijs},
booktitle = {Proceedings of the 30th Conference Annual Conference on Uncertainty in Artificial Intelligence},
eprint = {arXiv:1406.6200v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/van Ommen - 2014 - Combining predictions from linear models when training and test inputs differ.pdf:pdf},
pages = {653--662},
title = {{Combining predictions from linear models when training and test inputs differ}},
year = {2014}
}
@article{Shannon1948,
author = {Shannon, Claude Elwood},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shannon - 1948 - A mathematical theory of communication.pdf:pdf},
journal = {The Bell System Technical Journal},
number = {J},
pages = {379--423},
title = {{A mathematical theory of communication}},
url = {http://dl.acm.org/citation.cfm?id=584093},
volume = {27},
year = {1948}
}
@article{Moshkbar-bakhshayesh2016,
author = {Moshkbar-bakhshayesh, Khalil and Ghofrani, Mohammad B},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Moshkbar-bakhshayesh, Ghofrani - 2016 - Combining Supervised and Semi-Supervised Learning in the Design of a New Identifier for NPPs Tra.pdf:pdf},
journal = {IEEE Transactions on Nuclear Science},
number = {3},
pages = {1882--1888},
title = {{Combining Supervised and Semi-Supervised Learning in the Design of a New Identifier for NPPs Transients}},
volume = {63},
year = {2016}
}
@article{Weston2006,
address = {New York, New York, USA},
author = {Weston, Jason and Collobert, Ronan and Sinz, Fabian and Bottou, L{\'{e}}on and Vapnik, Vladimir},
doi = {10.1145/1143844.1143971},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Weston et al. - 2006 - Inference with the Universum.pdf:pdf},
isbn = {1595933832},
journal = {Proceedings of the 23rd international conference on Machine learning - ICML '06},
pages = {1009--1016},
publisher = {ACM Press},
title = {{Inference with the Universum}},
year = {2006}
}
@article{Krijthe2017projection,
archivePrefix = {arXiv},
arxivId = {1602.07865},
author = {Krijthe, Jesse Hendrik and Loog, Marco},
doi = {10.1007/s10994-017-5626-8},
eprint = {1602.07865},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/10.1007{\_}s10994-017-5626-8.pdf:pdf},
journal = {Machine Learning},
number = {7},
pages = {993--1008},
title = {{Projected Estimators for Robust Semi-supervised Classification}},
url = {http://arxiv.org/abs/1602.07865},
volume = {106},
year = {2017}
}
@incollection{Opper1995,
address = {New York},
author = {Opper, Manfred and Kinzel, Wolfgang},
booktitle = {Physics of Neural Networks III},
editor = {Domany, Eytan and Hemmen, J. Leo and Schulten, Klaus},
pages = {151--209},
publisher = {Springer-Verlag},
title = {{Statistical Mechanics of Generalization}},
year = {1995}
}
@article{Patel2015a,
author = {Patel, Nihir and Wang, Jason T L},
doi = {10.1007/s12038-015-9558-9},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Patel, Wang - 2015 - Semi-supervised prediction of gene regulatory networks using machine learning algorithms.pdf:pdf},
issn = {0250-5991},
journal = {Journal of Biosciences},
number = {4},
pages = {731--740},
title = {{Semi-supervised prediction of gene regulatory networks using machine learning algorithms}},
url = {http://link.springer.com/10.1007/s12038-015-9558-9},
volume = {40},
year = {2015}
}
@article{Bynum2007,
abstract = {OBJECTIVE: To develop a method for assigning Medicare enrollees and the physicians who serve them to individual hospitals with adequate validity to allow population-based assessments of provider specific costs and quality of care.$\backslash$n$\backslash$nDATA SOURCES/STUDY SETTING: The study populations consist of a 20 percent sample of Medicare fee-for-service enrollees and all physicians submitting claims for Medicare services from 1998 to 2000. Data were obtained from Medicare claims and enrollment files, Medicare's MPIER file, and from the American Hospital Association Annual Survey.$\backslash$n$\backslash$nSTUDY DESIGN: Cross-sectional analysis of the characteristics of hospitals, their extended medical staffs (EMSs) and the utilization patterns of their assigned Medicare enrollees.$\backslash$n$\backslash$nDATA COLLECTION METHODS: Medicare enrollees were assigned to their predominant ambulatory physician and then to the hospital where that physician provided inpatient services or where a plurality of that physician's patient panel had medical admissions. Each beneficiary was linked to a physician and a hospital regardless of whether the patient was hospitalized creating Ambulatory Provider Specific Cohorts (APSCs).$\backslash$n$\backslash$nPRINCIPAL FINDINGS: Ninety-six percent of eligible Medicare enrollees who had an index physician visit in 1998 were assigned to a specific provider. Two-thirds of the medical admissions during a 2-year period occurred at the assigned hospital and two-thirds of evaluation and management services were billed by the assigned hospital's EMS. The empirically derived EMS for hospitals had reasonable face and discriminant validity in terms of number and type of physicians practicing at different sized and type hospitals. Estimates of risk-adjusted costs across physician groups in year one are highly predictive of costs in a subsequent year (r=0.87, p{\textless}.0001 and weighted kappa=0.65, p{\textless}.0001).$\backslash$n$\backslash$nCONCLUSIONS: Medicare claims data can be used to assign virtually all Medicare enrollees to empirically defined care systems comprised of hospitals and the physicians who practice at these hospitals. Studies of patterns of practice, costs and outcomes of care experienced by these APSCs could complement other methods of monitoring provider performance.},
author = {Bynum, Julie P W and Bernal-Delgado, Enrique and Gottlieb, Daniel and Fisher, Elliott},
doi = {10.1111/j.1475-6773.2006.00633.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bynum{\_}et{\_}al-2007-Health{\_}Services{\_}Research.pdf:pdf},
isbn = {0017-9124},
issn = {00179124},
journal = {Health Services Research},
keywords = {Hospital,Performance measures,Small area analysis},
number = {1 I},
pages = {45--62},
pmid = {17355581},
title = {{Assigning ambulatory patients and their physicians to hospitals: A method for obtaining population-based provider performance measurements}},
volume = {42},
year = {2007}
}
@article{Sotoca2006,
author = {Sotoca, J M and Mollineda, R A and Sanchez, J.S.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sotoca, Mollineda, Sanchez - 2006 - A meta-learning framework for pattern classification by means of data complexity measures.pdf:pdf},
journal = {Inteligencia artificial: Revista Iberoamericana de Inteligencia Artificial},
keywords = {classification,data complexity,feature selection,meta-learning,prototype selection},
number = {29},
pages = {31--38},
title = {{A meta-learning framework for pattern classification by means of data complexity measures}},
volume = {10},
year = {2006}
}
@inproceedings{Rao1995,
author = {Rao, R. Bharat and Gordon, Diana and Spears, William},
booktitle = {Proceedings of the 12th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rao, Gordon, Spears - 1995 - For Every Generalization Action, Is there really an Equal and Opposite Reaction Analysis of the conservatio.pdf:pdf},
pages = {471--479},
title = {{For Every Generalization Action, Is there really an Equal and Opposite Reaction? Analysis of the conservation Law for Generalization Performance}},
url = {http://www.researchgate.net/publication/2516136{\_}For{\_}Every{\_}Generalization{\_}Action{\_}Is{\_}There{\_}Really{\_}An{\_}Equal{\_}And{\_}Opposite{\_}Reaction{\_}Analysis{\_}of{\_}the{\_}Conservation{\_}Law{\_}for{\_}Generalization{\_}Performance/file/79e4150b866697f897.pdf},
year = {1995}
}
@article{Ascherio2016,
abstract = {Since 2006, several longitudinal studies have assessed environmental or behavioural factors that seem to modify the risk of developing Parkinson's disease. Increased risk of Parkinson's disease has been associated with exposure to pesticides, consumption of dairy products, history of melanoma, and traumatic brain injury, whereas a reduced risk has been reported in association with smoking, caffeine consumption, higher serum urate concentrations, physical activity, and use of ibuprofen and other common medications. Randomised trials are investigating the possibility that some of the negative risk factors might be neuroprotective and thus beneficial in individuals with early Parkinson's disease, particularly with respect to smoking (nicotine), caffeine, and urate. In the future, it might be possible to identify Parkinson's disease in its prodromal phase and to promote neuroprotective interventions before the onset of motor symptoms. At this time, however, the only intervention that seems justifiable for the primary prevention of Parkinson's disease is the promotion of physical activity, which is likely to be beneficial for the prevention of several chronic diseases.},
author = {Ascherio, Alberto and Schwarzschild, Michael A.},
doi = {10.1016/S1474-4422(16)30230-7},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ascherio{\_}Lancet Neurol (2016) Epidemiology of PD - risk factors and prevention.pdf:pdf},
isbn = {1474-4422},
issn = {14744465},
journal = {The Lancet Neurology},
number = {12},
pages = {1257--1272},
pmid = {27751556},
publisher = {Elsevier Ltd},
title = {{The epidemiology of Parkinson's disease: risk factors and prevention}},
url = {http://dx.doi.org/10.1016/S1474-4422(16)30230-7},
volume = {15},
year = {2016}
}
@article{Pitt1988,
author = {Pitt, Leonard and Valiant, Leslie G.},
doi = {10.1145/48014.63140},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pitt, Valiant - 1988 - Computational limitations on learning from examples.pdf:pdf},
issn = {00045411},
journal = {Journal of the ACM},
month = {oct},
number = {4},
pages = {965--984},
title = {{Computational limitations on learning from examples}},
url = {http://portal.acm.org/citation.cfm?doid=48014.63140},
volume = {35},
year = {1988}
}
@article{Kucukelbir2015,
abstract = {Variational inference is a scalable technique for approximate Bayesian inference. Deriving variational inference algorithms requires tedious model-specific calculations; this makes it difficult to automate. We propose an automatic variational inference algorithm, automatic differentiation variational inference (ADVI). The user only provides a Bayesian model and a dataset; nothing else. We make no conjugacy assumptions and support a broad class of models. The algorithm automatically determines an appropriate variational family and optimizes the variational objective. We implement ADVI in Stan (code available now), a probabilistic programming framework. We compare ADVI to MCMC sampling across hierarchical generalized linear models, nonconjugate matrix factorization, and a mixture model. We train the mixture model on a quarter million images. With ADVI we can use variational inference on any model we write in Stan.},
archivePrefix = {arXiv},
arxivId = {1506.03431},
author = {Kucukelbir, Alp and Ranganath, Rajesh and Gelman, Andrew and Blei, David M.},
eprint = {1506.03431},
pages = {1--22},
title = {{Automatic Variational Inference in Stan}},
url = {http://arxiv.org/abs/1506.03431},
year = {2015}
}
@inproceedings{Aha1992,
author = {Aha, David W.},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Aha - 1992 - Generalizing from case studies A case study.pdf:pdf},
title = {{Generalizing from case studies: A case study}},
year = {1992}
}
@article{Stahlecker1996,
author = {Stahlecker, Peter and Knautz, Henning and Trenkler, Gotz},
doi = {10.1007/BF00046994},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Stahlecker, Knautz, Trenkler - 1996 - Minimax adjustment technique in a parameter restricted linear model.pdf:pdf},
issn = {0167-8019},
journal = {Acta Applicandae Mathematicae},
keywords = {linear regression,minimax adjustment,projection estimator},
month = {apr},
number = {1},
pages = {139--144},
title = {{Minimax adjustment technique in a parameter restricted linear model}},
url = {http://link.springer.com/10.1007/BF00046994},
volume = {43},
year = {1996}
}
@article{Isaksson2008,
author = {Isaksson, Anders and Wallman, M. and Goransson, H. and Gustafsson, Mats G},
doi = {10.1016/j.patrec.2008.06.018},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Isaksson et al. - 2008 - Cross-validation and bootstrapping are unreliable in small sample classification.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {performance estimation,supervised classification},
month = {oct},
number = {14},
pages = {1960--1965},
title = {{Cross-validation and bootstrapping are unreliable in small sample classification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865508002158},
volume = {29},
year = {2008}
}
@article{Lopez-paz2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.02398v1},
author = {Lopez-paz, David and Sch, Bernhard},
eprint = {arXiv:1502.02398v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lopez-paz, Sch - 2015 - Towards a Learning Theory of Causation.pdf:pdf},
title = {{Towards a Learning Theory of Causation}},
year = {2015}
}
@article{Norton2003,
author = {Norton, John D},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Norton - 2003 - A Material Theory of Induction.pdf:pdf},
journal = {Philosophy of Science},
number = {October},
pages = {647--670},
title = {{A Material Theory of Induction}},
volume = {70},
year = {2003}
}
@book{Spirtes2000,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Spirtes, Peter and Glymour, Clark and Scheines, Richard},
booktitle = {Journal of Chemical Information and Modeling},
edition = {Second},
eprint = {arXiv:1011.1669v3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Spirtes, Glymour, Scheines - 2000 - Causation, Prediction, and Search.pdf:pdf},
isbn = {0-262-19440-6},
issn = {1098-6596},
number = {9},
pmid = {25246403},
title = {{Causation, Prediction, and Search}},
volume = {53},
year = {2000}
}
@article{Chopin2015,
abstract = {Abstract. Whenever a new approach to perform Bayesian computation is introduced, a common practice is to showcase this approach on a binary regression model and datasets of moderate size. This paper discusses to which extent this practice is sound. It also reviews the current state of the art of Bayesian computation, using binary regression as a running example. Both sampling-based algorithms (importance sampling, MCMC and SMC) and fast approximations (Laplace and EP) are covered. Extensive numerical results are provided, some of which might go against conventional wisdom regarding the effectiveness of certain algorithms. Implications for other problems (variable selection) and other models are also discussed.},
archivePrefix = {arXiv},
arxivId = {1506.08640},
author = {Chopin, Nicolas and Ridgway, James},
doi = {10.1214/16-STS581},
eprint = {1506.08640},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/euclid.ss.1491465628.pdf:pdf},
issn = {0883-4237},
keywords = {Bayesian computation, expectation propagation, Mar,and phrases,bayesian computation,expectation propagation,markov chain monte carlo,sequential monte carlo,variational inference},
number = {1},
pages = {1--28},
title = {{Leave Pima Indians alone: binary regression as a benchmark for Bayesian computation}},
url = {http://arxiv.org/abs/1506.08640},
volume = {32},
year = {2015}
}
@inproceedings{Muandet2012,
author = {Muandet, Krikamol and Fukumizu, K},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Muandet, Fukumizu - 2012 - Learning from distributions via support measure machines.pdf:pdf},
pages = {1--9},
title = {{Learning from distributions via support measure machines}},
url = {http://arxiv.org/abs/1202.6504},
year = {2012}
}
@article{Hand2009,
author = {Hand, David J.},
doi = {10.1007/s10994-009-5119-5},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hand - 2009 - Measuring classifier performance a coherent alternative to the area under the ROC curve.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {auc,classification,cost,error rate,loss,misclassification,rate,roc curves,sensitivity,specificity},
month = {jun},
number = {1},
pages = {103--123},
title = {{Measuring classifier performance: a coherent alternative to the area under the ROC curve}},
url = {http://link.springer.com/10.1007/s10994-009-5119-5},
volume = {77},
year = {2009}
}
@article{Grunwald2016,
abstract = {We formalize the idea of probability distributions that lead to reliable predictions about some, but not all aspects of a domain. The resulting notion of `safety' provides a fresh perspective on foundational issues in statistics, providing a middle ground between imprecise probability and multiple-prior models on the one hand and strictly Bayesian approaches on the other. It also allows us to formalize fiducial distributions in terms of the set of random variables that they can safely predict, thus taking some of the sting out of the fiducial idea. By restricting probabilistic inference to safe uses, one also automatically avoids paradoxes such as the Monty Hall problem. Safety comes in a variety of degrees, such as "validity" (the strongest notion), "calibration", "confidence safety" and "unbiasedness" (almost the weakest notion).},
archivePrefix = {arXiv},
arxivId = {1604.01785},
author = {Gr{\"{u}}nwald, Peter},
eprint = {1604.01785},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gr{\"{u}}nwald - 2016 - Safe Probability.pdf:pdf},
number = {1979},
pages = {1--39},
title = {{Safe Probability}},
url = {http://arxiv.org/abs/1604.01785},
year = {2016}
}
@phdthesis{Ommen2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1412.3730},
author = {Ommen, Thijs Van},
eprint = {arXiv:1412.3730},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ommen - 2015 - Better Predictions when Models are Wrong or Underspecified.pdf:pdf},
isbn = {9789462596894},
school = {Leiden University},
title = {{Better Predictions when Models are Wrong or Underspecified}},
year = {2015}
}
@inproceedings{Reid2009,
abstract = {We present tight surrogate regret bounds for the class of proper (i.e., Fisher consistent) losses. The bounds generalise the margin-based bounds due to Bartlett et al. (2006). The proof uses Taylor's theorem and leads to new representations for loss and regret and a simple proof of the integral representation of proper losses. We also present a different formulation of a duality result of Bregman divergences which leads to a simple demonstration of the convexity of composite losses using canonical link functions},
author = {Reid, Mark and Williamson, Bob},
booktitle = {Proceedings of the 26th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Reid, Williamson - 2009 - Surrogate Regret Bounds for Proper Losses.pdf:pdf},
pages = {897--904},
title = {{Surrogate Regret Bounds for Proper Losses}},
url = {http://eprints.pascal-network.org/archive/00008977/},
year = {2009}
}
@article{Rtsne,
author = {Krijthe, Jesse Hendrik},
title = {{Rtsne: T-Distributed Stochastic Neighbor Embedding using a Barnes-Hut Implementation}},
year = {2016}
}
@article{Keil2014,
author = {Keil, Alexander P. and Edwards, Jessie K. and Richardson, David B. and Naimi, Ashley I. and Cole, Stephen R.},
doi = {10.1097/EDE.0000000000000160},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/00001648-201411000-00016.pdf:pdf},
isbn = {0000000000000},
issn = {1044-3983},
journal = {Epidemiology},
number = {6},
pages = {889--897},
title = {{The Parametric g-Formula for Time-to-event Data}},
url = {http://content.wkhealth.com/linkback/openurl?sid=WKPTLP:landingpage{\&}an=00001648-201411000-00016},
volume = {25},
year = {2014}
}
@article{Macia2013,
author = {Maci{\`{a}}, N{\'{u}}ria and Bernad{\'{o}}-Mansilla, Ester},
doi = {10.1016/j.ins.2013.08.059},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Maci{\`{a}}, Bernad{\'{o}}-Mansilla - 2013 - Towards UCI A mindful repository design.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
month = {sep},
title = {{Towards UCI+: A mindful repository design}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0020025513006336},
year = {2013}
}
@unpublished{Amasyali2009,
author = {Amasyali, M Fatih and Ersoy, Okan K.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Amasyali, Ersoy - 2009 - A Study of Meta Learning for Regression.pdf:pdf},
institution = {Purdue University},
title = {{A Study of Meta Learning for Regression}},
year = {2009}
}
@inproceedings{Szummer2002,
author = {Szummer, Martin and Jaakkola, Tommi S.},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Szummer, Jaakkola - 2002 - Information regularization with partially labeled data.pdf:pdf},
pages = {1025--1032},
title = {{Information regularization with partially labeled data}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AA69.pdf},
year = {2002}
}
@inproceedings{Dean2012,
author = {Dean, Jeffrey and Corrado, Greg S and Monga, Rajat and Chen, Kai and Devin, Matthieu and Le, Quoc V and Mao, Mark Z and Ranzato, Marc Aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Ng, Andrew Y},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dean et al. - 2012 - Large Scale Distributed Deep Networks.pdf:pdf},
pages = {1223--1231},
title = {{Large Scale Distributed Deep Networks}},
year = {2012}
}
@article{Sun2010,
author = {Sun, Shiliang and Shawe-taylor, John},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sun, Shawe-taylor - 2010 - Sparse Semi-supervised Learning Using Conjugate Functions.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {fenchel-legendre conjugate,multi-,representer theorem,semi-supervised learning,statistical learning theory,support vector machine,view regularization},
pages = {2423--2455},
title = {{Sparse Semi-supervised Learning Using Conjugate Functions}},
volume = {11},
year = {2010}
}
@inproceedings{Grunwald2000,
author = {Gr{\"{u}}nwald, Peter},
booktitle = {Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gr{\"{u}}nwald - 2000 - Maximum entropy and the glasses you are looking through.pdf:pdf},
pages = {238--246},
title = {{Maximum entropy and the glasses you are looking through}},
url = {http://dl.acm.org/citation.cfm?id=2073975},
year = {2000}
}
@article{Bertsimas2017,
author = {Bertsimas, Dimitris and Dunn, Jack},
doi = {10.1007/s10994-017-5633-9},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/art{\%}3A10.1007{\%}2Fs10994-017-5633-9.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {classification,decision trees,mixed integer optimization},
number = {September 2015},
publisher = {Springer US},
title = {{Optimal Classification Trees}},
year = {2017}
}
@article{Littwin2016,
author = {Littwin, Etai and Wolf, Lior},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Littwin, Wolf - 2016 - Complexity of Multiverse Networks and their Multilayer Generalization.pdf:pdf},
title = {{Complexity of Multiverse Networks and their Multilayer Generalization}},
year = {2016}
}
@article{Li2015b,
abstract = {The scarcity of data annotated at the desired level of granularity is a recurring issue in many applications. Significant amounts of effort have been devoted to developing weakly supervised methods tailored to each individual setting, which are often carefully designed to take advantage of the particular properties of weak supervision regimes, form of available data and prior knowledge of the task at hand. Unfortunately, it is difficult to adapt these methods to new tasks and/or forms of data, which often require different weak supervision regimes or models. We present a general-purpose method that can solve any weakly supervised learning problem irrespective of the weak supervision regime or the model. The proposed method turns any off-the-shelf strongly supervised classifier into a weakly supervised classifier and allows the user to specify any arbitrary weakly supervision regime via a loss function. We apply the method to several different weak supervision regimes and demonstrate competitive results compared to methods specifically engineered for those settings.},
archivePrefix = {arXiv},
arxivId = {1509.06807},
author = {Li, Ke and Malik, Jitendra},
eprint = {1509.06807},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li, Malik - 2015 - Bandit Label Inference for Weakly Supervised Learning.pdf:pdf},
pages = {1--10},
title = {{Bandit Label Inference for Weakly Supervised Learning}},
url = {http://arxiv.org/abs/1509.06807},
year = {2015}
}
@article{Kawakita2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1204.3965v1},
author = {Kawakita, Masanori and Kanamori, Takafumi},
eprint = {arXiv:1204.3965v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kawakita, Kanamori - 2013 - Semi-Supervised learning with Density-Ratio Estimation.pdf:pdf},
journal = {Machine Learning},
number = {2},
pages = {189--209},
title = {{Semi-Supervised learning with Density-Ratio Estimation}},
volume = {91},
year = {2013}
}
@inproceedings{Lopez-Paz2015,
abstract = {We describe generalized distillation, a framework to learn from multiple representations in a semisupervised fashion. We show that distillation (Hinton et al., 2015) and privileged information (Vapnik {\&} Izmailov, 2015) are particular instances of generalized distillation, give insight about why and when generalized distillation works, and provide numerical simulations to assess its effectiveness.},
archivePrefix = {arXiv},
arxivId = {1511.03643},
author = {Lopez-Paz, David and Bottou, L{\'{e}}on and Sch{\"{o}}lkopf, Bernhard and Vapnik, Vladimir},
booktitle = {Proceedings of the International Conference on Learning Representations},
eprint = {1511.03643},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lopez-Paz et al. - 2015 - Unifying distillation and privileged information.pdf:pdf},
pages = {1--10},
title = {{Unifying distillation and privileged information}},
url = {http://arxiv.org/abs/1511.03643},
year = {2016}
}
@article{Mayo2005,
author = {Mayo, Deborah G},
doi = {10.1353/csp.2011.0039},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mayo{\_}2005{\_}PeircePaper.pdf:pdf},
journal = {Transactions of the Charles S. Peirce Society},
number = {2},
pages = {299--319},
title = {{Peircean Induction and the Error-Correcting Thesis Error-Correcting Thesis}},
volume = {41},
year = {2005}
}
@inproceedings{Carroll2007,
author = {Carroll, James L. and Seppi, Kevin D.},
booktitle = {IJCNN Workshop on Meta-Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Carroll, Seppi - 2007 - No-free-lunch and Bayesian optimality.pdf:pdf},
title = {{No-free-lunch and Bayesian optimality}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.142.7564{\&}rep=rep1{\&}type=pdf},
year = {2007}
}
@book{Lehmann1998,
author = {Lehmann, E. L. and Casella, G.},
edition = {2},
publisher = {Springer-Verlag},
title = {{Theory of Point Estimation}},
year = {1998}
}
@inproceedings{Loog2012a,
author = {Loog, Marco},
booktitle = {Partially Supervised Learning (Lecture Notes in Artificial Intelligence Volume 7081)},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - 2012 - Semi-supervised linear discriminant analysis using moment constraints.pdf:pdf},
pages = {32--41},
publisher = {Springer},
title = {{Semi-supervised linear discriminant analysis using moment constraints}},
year = {2012}
}
@inproceedings{Abney2002,
author = {Abney, Steven},
booktitle = {Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Abney - 2002 - Bootstrapping.pdf:pdf},
number = {July},
pages = {360--367},
title = {{Bootstrapping}},
year = {2002}
}
@inproceedings{Sindhwani2006,
author = {Sindhwani, Vikas and Keerthi, S. S.},
booktitle = {Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sindhwani, Keerthi - 2006 - Large scale semi-supervised linear SVMs.pdf:pdf},
isbn = {1595933697},
keywords = {global optimiza-,support vector machines,text categorization,tion,unlabeled data},
pages = {477--484},
publisher = {ACM},
title = {{Large scale semi-supervised linear SVMs}},
year = {2006}
}
@article{Schaffer1993,
author = {Schaffer, Cullen},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schaffer - 1993 - Selecting a classification method by cross-validation.pdf:pdf},
journal = {Machine Learning},
keywords = {classification,cross-validation,decision trees,neural networks},
pages = {135--143},
title = {{Selecting a classification method by cross-validation}},
url = {http://link.springer.com/article/10.1007/BF00993106},
volume = {13},
year = {1993}
}
@inproceedings{Kemp2004,
author = {Kemp, Charles and Griffiths, Thomas L. and Stromsten, Sean and Tenenbaum, Joshua B.},
booktitle = {Advances in neural information processing systems 16},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kemp et al. - 2004 - Semi-supervised Learning with Trees.pdf:pdf},
pages = {1--9},
title = {{Semi-supervised Learning with Trees}},
year = {2004}
}
@article{Gelman2013b,
abstract = {The missionary zeal of many Bayesians of old has been matched, in the other direction, by an attitude among some theoreticians that Bayesian methods were absurd—notmerely misguided but obviously wrong in prin- ciple. We consider several examples, beginning with Feller's classic text on probability theory and continuing with more recent cases such as the perceived Bayesian nature of the so-called doomsday argument. We an- alyze in this note the intellectual background behind various misconcep- tions about Bayesian statistics, without aiming at a complete historical coverage of the reasons for this dismissal.},
archivePrefix = {arXiv},
arxivId = {arXiv:1006.5366v5},
author = {Gelman, Andrew and Robert, Christian P.},
doi = {10.1080/00031305.2013.760987},
eprint = {arXiv:1006.5366v5},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Robert - 2013 - “Not Only Defended But Also Applied” The Perceived Absurdity of Bayesian Inference.pdf:pdf},
isbn = {0003-1305},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {bayesian,bogosity,doomsdsay argument,foundations,frequentist,laplace law of succession},
number = {1},
pages = {1--5},
title = {{“Not Only Defended But Also Applied”: The Perceived Absurdity of Bayesian Inference}},
url = {http://basepub.dauphine.fr/handle/123456789/11069{\%}5Cnhttp://www.tandfonline.com/doi/abs/10.1080/00031305.2013.760987},
volume = {67},
year = {2013}
}
@article{Mooij2014,
abstract = {The discovery of causal relationships from purely observational data is a fundamental problem in science. The most elementary form of such a causal discovery problem is to decide whether X causes Y or, alternatively, Y causes X, given joint observations of two variables X,Y. An example is to decide whether altitude causes temperature, or vice versa, given only joint measurements of both variables. Even under the simplifying assumptions of causal sufficiency, no feedback loops, and no selection bias, such bivariate causal discovery problems are very challenging. Nevertheless, several approaches for addressing those problems have been proposed in recent years. We review two families of such methods: Additive Noise Methods (ANM) and Information Geometric Causal Inference (IGCI). We present the benchmark CauseEffectPairs that consists of data for 96 different cause-effect pairs selected from 34 datasets from various domains (e.g., meteorology, biology, medicine, engineering, economy, etc.). We motivate our decisions regarding the "ground truth" causal directions of all pairs. We evaluate the performance of several bivariate causal discovery methods on these real-world benchmark data and in addition on artificially simulated data. Our empirical results indicate that certain methods are able to distinguish cause from effect using only purely observational data with an accuracy of 63-69{\%}. Because of multiple-testing corrections, however, considerably more benchmark data would be needed to obtain statistically significant conclusions. A theoretical contribution of this paper is a proof of the consistency of the additive-noise method as originally proposed by Hoyer et al. (2009).},
archivePrefix = {arXiv},
arxivId = {1412.3773},
author = {Mooij, Joris M. and Peters, Jonas and Janzing, Dominik and Zscheischler, Jakob and Sch{\"{o}}lkopf, Bernhard},
eprint = {1412.3773},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mooij et al. - 2016 - Distinguishing cause from effect using observational data methods and benchmarks.pdf:pdf},
month = {dec},
pmid = {1000106307},
title = {{Distinguishing cause from effect using observational data: methods and benchmarks}},
url = {http://arxiv.org/abs/1412.3773},
volume = {17},
year = {2016}
}
@misc{RSSL,
author = {Krijthe, Jesse Hendrik},
title = {{RSSL: Semi-supervised Learning in R}},
url = {https://github.com/jkrijthe/RSSL},
year = {2016}
}
@article{Maaten2014,
author = {Maaten, Laurens Van Der},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Maaten - 2014 - Accelerating t-SNE using Tree-Based Algorithms.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {barnes-hut algorithm,dual-tree algorithm,embedding,multidimensional scaling,space-partitioning trees,t-sne},
pages = {3221−3245},
title = {{Accelerating t-SNE using Tree-Based Algorithms}},
volume = {15},
year = {2014}
}
@article{Tu2015,
author = {Tu, Enmei and Yang, Jie and Kasabov, Nicola and Zhang, Yaqian},
doi = {10.1016/j.neucom.2015.01.020},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tu et al. - 2015 - Posterior Distribution Learning (PDL) A novel supervised learning framework using unlabeled samples to improve classi.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Posterior distribution learning,Supervised learning,Supervised manifold classification},
pages = {173--186},
publisher = {Elsevier},
title = {{Posterior Distribution Learning (PDL): A novel supervised learning framework using unlabeled samples to improve classification performance}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231215000417},
volume = {157},
year = {2015}
}
@inproceedings{Skurichina1996,
abstract = {In this paper the possibilities for constructing linear classifiers are considered for very small sample sizes. We propose a stability measure and present a study on the performance and stability of the following techniques: regularization by the Ridge-estimate of the covariance matrix [12], bootstrapping followed by aggregation (‘bagging', [9]) and editing combined with pseudo- inversion [8]. It is shown that by these techniques a smooth transition can be made between the nearest mean classifier and the Fisher discriminant based on large samples sizes. Especially for highly correlated data very good results are obtained compared with the nearest mean method.},
author = {Skurichina, Marina and Duin, Robert P W},
booktitle = {Proceedings of the 13th International Conference on Pattern Recognition},
doi = {10.1109/ICPR.1996.547204},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Skurichina, Duin - 1996 - Stabilizing classifiers for very small sample sizes.pdf:pdf},
isbn = {081867282X},
issn = {10514651},
pages = {891--896},
title = {{Stabilizing classifiers for very small sample sizes}},
year = {1996}
}
@article{Schmidt1995,
author = {Schmidt, Karsten and Stahlecker, Peter},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schmidt, Stahlecker - 1995 - Reducing the Maximum Risk of Regression Estimators by Polyhedral Projection.pdf:pdf},
journal = {Journal of Statistical Computation and Simulation},
number = {1},
pages = {1--15},
title = {{Reducing the Maximum Risk of Regression Estimators by Polyhedral Projection}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00949659508811648},
volume = {52},
year = {1995}
}
@article{Rosenbaum1984,
author = {Rosenbaum, Paul R.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/2981697.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series A},
keywords = {ancillary statistics,binomial probabilities,conditioning,conservative,contingency tables,contingency test,continuity correction,fisher,goodness of fit,neyman-pearson theory,one-tail,s exact test,test},
number = {3},
pages = {426--463},
title = {{The Consquences of Adjustment for a Concomitant Variable That Has Been Affected by the Treatment}},
volume = {147},
year = {1984}
}
@article{Alaoui2016,
abstract = {Given a weighted graph with {\$}N{\$} vertices, consider a real-valued regression problem in a semi-supervised setting, where one observes {\$}n{\$} labeled vertices, and the task is to label the remaining ones. We present a theoretical study of {\$}\backslashell{\_}p{\$}-based Laplacian regularization under a {\$}d{\$}-dimensional geometric random graph model. We provide a variational characterization of the performance of this regularized learner as {\$}N{\$} grows to infinity while {\$}n{\$} stays constant, the associated optimality conditions lead to a partial differential equation that must be satisfied by the associated function estimate {\$}\backslashhat{\{}f{\}}{\$}. From this formulation we derive several predictions on the limiting behavior the {\$}d{\$}-dimensional function {\$}\backslashhat{\{}f{\}}{\$}, including (a) a phase transition in its smoothness at the threshold {\$}p = d + 1{\$}, and (b) a tradeoff between smoothness and sensitivity to the underlying unlabeled data distribution {\$}P{\$}. Thus, over the range {\$}p \backslashleq d{\$}, the function estimate {\$}\backslashhat{\{}f{\}}{\$} is degenerate and "spiky," whereas for {\$}p\backslashgeq d+1{\$}, the function estimate {\$}\backslashhat{\{}f{\}}{\$} is smooth. We show that the effect of the underlying density vanishes monotonically with {\$}p{\$}, such that in the limit {\$}p = \backslashinfty{\$}, corresponding to the so-called Absolutely Minimal Lipschitz Extension, the estimate {\$}\backslashhat{\{}f{\}}{\$} is independent of the distribution {\$}P{\$}. Under the assumption of semi-supervised smoothness, ignoring {\$}P{\$} can lead to poor statistical performance, in particular, we construct a specific example for {\$}d=1{\$} to demonstrate that {\$}p=2{\$} has lower risk than {\$}p=\backslashinfty{\$} due to the former penalty adapting to {\$}P{\$} and the latter ignoring it. We also provide simulations that verify the accuracy of our predictions for finite sample sizes. Together, these properties show that {\$}p = d+1{\$} is an optimal choice, yielding a function estimate {\$}\backslashhat{\{}f{\}}{\$} that is both smooth and non-degenerate, while remaining maximally sensitive to {\$}P{\$}.},
archivePrefix = {arXiv},
arxivId = {1603.00564},
author = {Alaoui, Ahmed El and Cheng, Xiang and Ramdas, Aaditya and Wainwright, Martin J. and Jordan, Michael I.},
eprint = {1603.00564},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Alaoui et al. - 2016 - Asymptotic behavior of {\$}ell{\_}p{\$}-based Laplacian regularization in semi-supervised learning.pdf:pdf},
journal = {arXiv preprint},
keywords = {absolutely minimal lipschitz extension,asymptotic behav-,geometric random graph model,ior,phase transition,regularization,semi-supervised learning,ℓ p -based laplacian},
title = {{Asymptotic behavior of {\$}\backslashell{\_}p{\$}-based Laplacian regularization in semi-supervised learning}},
url = {http://arxiv.org/abs/1603.00564},
year = {2016}
}
@article{Corani2016,
abstract = {Usually one compares the accuracy of two competing classifiers via null hypothesis significance tests (nhst). Yet the nhst tests suffer from important shortcomings, which can be overcome by switching to Bayesian hypothesis testing. We propose a Bayesian hierarchical model which jointly analyzes the cross-validation results obtained by two classifiers on multiple data sets. It returns the posterior probability of the accuracies of the two classifiers being practically equivalent or significantly different. A further strength of the hierarchical model is that, by jointly analyzing the results obtained on all data sets, it reduces the estimation error compared to the usual approach of averaging the cross-validation results obtained on a given data set.},
archivePrefix = {arXiv},
arxivId = {1609.08905},
author = {Corani, Giorgio and Benavoli, Alessio and Dem{\v{s}}ar, Janez and Mangili, Francesca and Zaffalon, Marco},
doi = {10.1007/s10994-017-5641-9},
eprint = {1609.08905},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/art{\%}3A10.1007{\%}2Fs10994-017-5641-9.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
publisher = {Springer US},
title = {{Statistical comparison of classifiers through Bayesian hierarchical modelling}},
url = {http://arxiv.org/abs/1609.08905},
year = {2016}
}
@article{Krishnasamy2016,
author = {Krishnasamy, Ganesh and Paramesran, Raveendran},
doi = {10.1016/j.neucom.2016.05.039},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Krishnasamy, Paramesran - 2016 - Hessian semi-supervised extreme learning machine.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Extreme learning machine,Hessian regularization,Manifold learning,Semi-supervised learning,extreme learning machine,semi-supervised learning},
pages = {560--567},
publisher = {Elsevier},
title = {{Hessian semi-supervised extreme learning machine}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231216303915},
volume = {207},
year = {2016}
}
@inproceedings{Kleiner2012,
abstract = {The bootstrap provides a simple and pow- erful means of assessing the quality of esti- mators. However, in settings involving large datasets, the computation of bootstrap-based quantities can be prohibitively demanding. As an alternative, we present the Bag of Lit- tle Bootstraps (BLB), a new procedure which incorporates features of both the bootstrap and subsampling to obtain a robust, compu- tationally efficient means of assessing estima- tor quality. BLB is well suited to modern par- allel and distributed computing architectures and retains the generic applicability, statisti- cal efficiency, and favorable theoretical prop- erties of the bootstrap. We provide the re- sults of an extensive empirical and theoretical investigation of BLB's behavior, including a study of its statistical correctness, its large- scale implementation and performance, selec- tion of hyperparameters, and performance on real data. 1.},
author = {Kleiner, Ariel and Talwalkar, Ameet and Sarkar, Purnamrita and Jordan, Michael I.},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kleiner et al. - 2012 - The Big Data Bootstrap.pdf:pdf},
pages = {1759--1766},
title = {{The Big Data Bootstrap}},
year = {2012}
}
@article{Sohn1999,
author = {Sohn, So Young},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sohn - 1999 - Meta Analysis of Classification Algorithms for Pattern Recognition.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {11},
pages = {1137--1144},
title = {{Meta Analysis of Classification Algorithms for Pattern Recognition}},
volume = {21},
year = {1999}
}
@inproceedings{Zhu2003,
author = {Zhu, Xiaojin and Ghahramani, Zoubin and Lafferty, John},
booktitle = {Proceedings of the 20th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhu, Ghahramani, Lafferty - 2003 - Semi-supervised learning using gaussian fields and harmonic functions.pdf:pdf},
pages = {912--919},
title = {{Semi-supervised learning using gaussian fields and harmonic functions}},
year = {2003}
}
@book{Wickham2009,
author = {Wickham, Hadley},
isbn = {978-0-387-98140-6},
publisher = {Springer-Verlag New York},
title = {{ggplot2: Elegant Graphics for Data Analysis}},
url = {http://ggplot2.org},
year = {2009}
}
@article{Castelli1995,
author = {Castelli, Vittorio and Cover, Thomas M.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Castelli, Cover - 1995 - On the exponential value of labeled samples.pdf:pdf},
journal = {Pattern Recognition Letters},
pages = {105--111},
title = {{On the exponential value of labeled samples}},
volume = {16},
year = {1995}
}
@inproceedings{Fujino2005,
author = {Fujino, Akinori and Ueda, Naonori and Saito, Kazumi},
booktitle = {Proceedings of the National Conference on Artificial Intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fujino, Ueda, Saito - 2005 - A hybrid generativediscriminative approach to semi-supervised classifier design.pdf:pdf},
number = {2},
pages = {764--769},
title = {{A hybrid generative/discriminative approach to semi-supervised classifier design}},
url = {http://www.aaai.org/Papers/AAAI/2005/AAAI05-120.pdf},
volume = {20},
year = {2005}
}
@article{Roweis1999,
abstract = {Factor analysis, principal component analysis, mixtures of gaussian clusters, vector quantization, Kalman filter models, and hidden Markov models can all be unified as variations of unsupervised learning under a single basic generative model. This is achieved by collecting together disparate observations and derivations made by many previous authors and introducing a new way of linking discrete and continuous state models using a simple nonlinearity. Through the use of other nonlinearities, we show how independent component analysis is also a variation of the same basic generative model. We show that factor analysis and mixtures of gaussians can be implemented in autoencoder neural networks and learned using squared error plus the same regularization term. We introduce a new model for static data, known as sensible principal component analysis, as well as a novel concept of spatially adaptive observation noise. We also review some of the literature involving global and local mixtures of the basic models and provide pseudocode for inference and learning for all the basic models.},
author = {Roweis, Sam and Ghahramani, Zoubin},
doi = {10.1162/089976699300016674},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Roweis, Ghahramani - 1999 - A unifying review of linear gaussian models.pdf:pdf},
isbn = {0899766993000},
issn = {0899-7667},
journal = {Neural computation},
number = {1995},
pages = {305--345},
pmid = {9950734},
title = {{A unifying review of linear gaussian models.}},
volume = {11},
year = {1999}
}
@article{Dagenais1971,
abstract = {The purpose of this article is to suggest a method of estimating parameters of linear regressions containing two independent variables, when data is missing among these variables. The problem envisaged concerns the case where: (1) the independent variables are considered as fixed numbers; (2) each observation contains the values of the dependent variable and at least one of the independent variables; (3) some observations are complete. In contrast with other approaches dealing with similar problems, the technique developed in this article has the following advantages: (1) it is based on rather unrestrictive hypotheses; (2) the resulting estimators are consistent; (3) the asymptotic variances of these estimators are smaller than those of comparable estimators described in the literature. Although the question is not examined in the present article, it seems also that the proposed method offers good possibilities of generalization.},
author = {Dagenais, Marcel G},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dagenais - 1971 - Further suggestions concerning the utilization of incomplete observations in regression analysis.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {Missing data,Model specification},
number = {333},
pages = {93--98},
title = {{Further suggestions concerning the utilization of incomplete observations in regression analysis }},
volume = {66},
year = {1971}
}
@article{Collobert2006,
author = {Collobert, Ronan and Sinz, Fabian and Weston, Jason and Bottou, Leon},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Collobert et al. - 2006 - Large scale transductive SVMs.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {cccp,semi-supervised learning,transduction,transductive svms},
pages = {1687--1712},
title = {{Large scale transductive SVMs}},
volume = {7},
year = {2006}
}
@inproceedings{Mooij,
author = {Mooij, Joris M and Heskes, Tom and Janzing, Dominik and Sch{\"{o}}lkopf, Bernhard},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mooij et al. - 2011 - On Causal Discovery with Cyclic Additive Noise Models.pdf:pdf},
pages = {639--647},
title = {{On Causal Discovery with Cyclic Additive Noise Models}},
year = {2011}
}
@inproceedings{Niu2012,
author = {Niu, Gang and Dai, Bo and Yamada, Makoto and Sugiyama, Masashi},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Niu et al. - 2012 - Information-theoretic Semi-supervised Metric Learning via Entropy Regularization.pdf:pdf},
number = {c},
title = {{Information-theoretic Semi-supervised Metric Learning via Entropy Regularization}},
url = {http://arxiv.org/abs/1206.4614},
year = {2012}
}
@article{Freedman1999,
abstract = {There have been many efforts to infer causation from association by using statistical models. Algorithms for automating this process are a more recent innovation. In Humphreys and Freedman [(1996) British Journal for the Philosophy of Science 47, 113-123] we showed that one such approach, by Spirtes et al., was fatally flawed. Here we put our arguments in a broader context and reply to Korb and Wallace [(1997) British Journal for the Philosophy of Science 48, 543-553] and to Spirtes et al. [(1997) British Journal for the Philosophy of Science 48, 555-568]. Their arguments leave our position unchanged: claims to have developed a rigorous engine for inferring causation from association are premature at best, the theorems have no implications for samples of any realistic size, and the examples used to illustrate the algorithms are indicative of failure rather than success. The gap between association and causation has yet to be bridged},
author = {Freedman, David and Humphreys, Paul},
doi = {10.1023/A:1005277613752},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/A-1005277613752.pdf:pdf},
isbn = {00397857},
issn = {0039-7857},
journal = {Synthese},
keywords = {ALGORITHM,ASSOCIATION,Algorithms,Article,CAU,CAUSATION,FAILURE,FREEDMANS CRITIQUE,GAP,HUMPHREYS,Humanities,MARKOV-FIELDS,MODEL,Models,Netherlands,PHILOSOPHERS STONE,Philosophy,SAMPLE,SEARCH,SIZE,Science,Social Sciences and Law,Statistical,TIMES,Time,innovation},
number = {1-2},
pages = {29--54},
title = {{Are there algorithms that discover causal structure?}},
url = {isi:000086030500003{\%}5Cnhttp://www.springerlink.com/content/u4h7765l87v817x4/},
volume = {121},
year = {1999}
}
@article{Castelli1996,
author = {Castelli, Vittorio and Cover, Thomas M.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Castelli, Cover - 1996 - The Relative Value of Labeled and Unlabeled Samples in Pattern Recognition with an Unknown Mixing Parameter.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
number = {6},
pages = {2102--2117},
title = {{The Relative Value of Labeled and Unlabeled Samples in Pattern Recognition with an Unknown Mixing Parameter}},
volume = {42},
year = {1996}
}
@article{Bussas2017,
author = {Bussas, Matthias and Sawade, Christoph and K{\"{u}}hn, Nicolas and Scheffer, Tobias and Landwehr, Niels},
doi = {10.1007/s10994-017-5639-3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/art{\%}3A10.1007{\%}2Fs10994-017-5639-3.pdf:pdf},
issn = {1573-0565},
journal = {Machine Learning},
keywords = {Transfer learning,Varying-coefficient models,Housi},
publisher = {Springer US},
title = {{Varying-coefficient models for geospatial transfer learning}},
year = {2017}
}
@article{Pearl2009,
author = {Pearl, Judea},
doi = {10.1214/09-SS057},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pearl - 2009 - Causal inference in statistics An overview.pdf:pdf},
issn = {1935-7516},
journal = {Statistics Surveys},
keywords = {Structural equation models, confounding, graphical,and phrases,causal effects,causes of effects,confounding,counterfactuals,graph-,ical methods,mediation,policy evaluation,potential-outcome,received september 2009,structural equation models},
number = {September},
pages = {96--146},
title = {{Causal inference in statistics: An overview}},
url = {http://projecteuclid.org/euclid.ssu/1255440554},
volume = {3},
year = {2009}
}
@article{Munneke2014,
author = {Munneke, M and Bloem, B R},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Munneke, Bloem - 2014 - Tien jaar ParkinsonNet.pdf:pdf},
number = {3},
title = {{Tien jaar ParkinsonNet}},
volume = {115},
year = {2014}
}
@inproceedings{Huang2014a,
author = {Huang, Gao and Song, Shiji and Xu, Zhixiang and Weinberger, Kilian},
booktitle = {ECML PKDD},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huang et al. - 2014 - Transductive minimax probability machine.pdf:pdf},
isbn = {9783662448472},
issn = {16113349},
keywords = {minimax probability machine,semi-supervised learning,transductive learning},
number = {PART 1},
pages = {579--594},
title = {{Transductive minimax probability machine}},
volume = {8724 LNAI},
year = {2014}
}
@misc{Lichman2013,
author = {Lichman, M.},
publisher = {University of California, Irvine, School of Information and Computer Sciences},
title = {{UCI Machine Learning Repository}},
url = {http://archive.ics.uci.edu/ml},
year = {2013}
}
@inproceedings{Cortes2004,
author = {Cortes, Corinna and Mohri, Mehryar},
booktitle = {Advances in Neural Information Processing Systems 16},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cortes, Mohri - 2004 - AUC optimization vs. error rate minimization.pdf:pdf},
pages = {313--320},
title = {{AUC optimization vs. error rate minimization}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=0F-9C7K8fQ8C{\&}oi=fnd{\&}pg=PA313{\&}dq=AUC+Optimization+vs+.+Error+Rate+Minimization{\&}ots=TGKup{\_}Ra93{\&}sig=VTdv-C5TW9itNMlz43YJjmxRKAc},
year = {2004}
}
@article{Wagenmakers2011,
abstract = {Does psi exist? D. J. Bem (2011) conducted 9 studies with over 1,000 participants in an attempt to demonstrate that future events retroactively affect people's responses. Here we discuss several limitations of Bem's experiments on psi; in particular, we show that the data analysis was partly exploratory and that one-sided p values may overstate the statistical evidence against the null hypothesis. We reanalyze Bem's data with a default Bayesian t test and show that the evidence for psi is weak to nonexistent. We argue that in order to convince a skeptical audience of a controversial claim, one needs to conduct strictly confirmatory studies and analyze the results with statistical tests that are conservative rather than liberal. We conclude that Bem's p values do not indicate evidence in favor of precognition; instead, they indicate that experimental psychologists need to change the way they conduct their experiments and analyze their data.},
author = {Wagenmakers, Eric-Jan and Wetzels, Ruud and Borsboom, Denny and van der Maas, Han L J},
doi = {10.1037/a0022790},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wagenmakers et al. - 2011 - Why psychologists must change the way they analyze their data the case of psi comment on Bem (2011).pdf:pdf},
isbn = {0022-3514},
issn = {1939-1315},
journal = {Journal of personality and social psychology},
number = {3},
pages = {426--32},
pmid = {21280965},
title = {{Why psychologists must change the way they analyze their data: the case of psi: comment on Bem (2011).}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21280965},
volume = {100},
year = {2011}
}
@inproceedings{Bottou2011,
author = {Bottou, Leon and Bousquet, Olivier},
booktitle = {Advances in Neural Information Processing Systems 24},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bottou, Bousquet - 2011 - The Tradeoffs of Large-Scale Learning.pdf:pdf},
pages = {In Advances in Neural Information Processing Syste},
title = {{The Tradeoffs of Large-Scale Learning}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=JPQx7s2L1A8C{\&}oi=fnd{\&}pg=PA351{\&}dq=The+Tradeoffs+of+Large+Scale+Learning{\&}ots=vbhayjhcGc{\&}sig=kWCMo7N51TgoLQSVSv2f{\_}ILArjo http://books.google.com/books?hl=en{\&}lr={\&}id=JPQx7s2L1A8C{\&}oi=fnd{\&}pg=PA351{\&}dq=The+Tradeoffs+of+Large-Scale+Learning{\&}ots=vbjaAkg8Fe{\&}sig=chdz7lCKXTFdUaLPYAgH{\_}FfgLmA},
year = {2011}
}
@article{Schwarzschild2008,
abstract = {OBJECTIVE: To determine whether concentration of serum urate, a purine metabolite and potent antioxidant that has been linked to a reduced risk of Parkinson disease (PD), predicts prognosis in PD.$\backslash$n$\backslash$nDESIGN: Prospective study.$\backslash$n$\backslash$nSETTING: The Parkinson Research Examination of CEP-1347 Trial (PRECEPT) study, which investigated the effects of a potential neuroprotectant on rates of PD progression, was conducted between April 2002 and August 2005 (average follow-up time 21.4 months).$\backslash$n$\backslash$nPARTICIPANTS: Eight hundred four subjects with early PD enrolled in the PRECEPT study.$\backslash$n$\backslash$nMAIN OUTCOME MEASURES: The primary study end point was progression to clinical disability sufficient to warrant dopaminergic therapy. Cox proportional hazards models were used to estimate the hazard ratio (HR) of reaching end point according to quintiles of baseline serum urate concentration, adjusting for sex, age, and other potential covariates. Change in striatal uptake of iodine I 123-labeled 2-beta-carbomethoxy-3-beta-(4-iodophenyl)tropane ([(123)I]beta-CIT), a marker for the presynaptic dopamine transporter, was assessed with linear regression for a subset of 399 subjects.$\backslash$n$\backslash$nRESULTS: The adjusted HR of reaching end point declined with increasing baseline concentrations of urate; subjects in the top quintile reached the end point at only half the rate of subjects in the bottom quintile (HR, 0.51; 95{\%} confidence interval [CI], 0.37-0.72; P for trend {\textless} .001). This association was markedly stronger in men (HR, 0.39; 95{\%} CI, 0.26-0.60; P for trend {\textless} .001) than in women (HR, 0.77; 95{\%} CI, 0.39-1.50; P for trend = .33). The percentage of loss in striatal [(123)I]beta-CIT uptake also improved with increasing serum urate concentrations (overall P for trend = .002; men, P = .001; women, P = .43).$\backslash$n$\backslash$nCONCLUSIONS: These findings identify serum urate as the first molecular factor directly linked to the progression of typical PD and suggest that targeting urate or its determinants could be an effective disease-modifying therapy in PD. Trial Registration clinicaltrials.gov Identifier: NCT00040404.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Schwarzschild, Michael A and Schwid, Steven R and Marek, Kenneth and Watts, Arthur and Lang, Anthony E and Oakes, David and Shoulson, Ira and Ascherio, Alberto and Hyson, Christopher and Gorbold, Emily and Rudolph, Alice and Kieburtz, Karl and Fahn, Stanley and Gauger, Lisa and Goetz, Christopher and Seibyl, John and Forrest, Misser and Ondrasik, John},
doi = {10.1001/archneur.2008.65.6.nct70003},
eprint = {NIHMS150003},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schwarzschild{\_}JAMA neurol (2008) Serum urate as predictor of clinical and radiographic progression in PD.pdf:pdf},
isbn = {1538-3687 (Electronic)},
issn = {1538-3687},
journal = {Archives of neurology},
keywords = {Biological Markers,Biological Markers: blood,Cohort Studies,Disease Progression,Double-Blind Method,Female,Follow-Up Studies,Humans,Longitudinal Studies,Male,Middle Aged,Parkinson Disease,Parkinson Disease: blood,Parkinson Disease: diagnosis,Parkinson Disease: radiography,Predictive Value of Tests,Prospective Studies,Uric Acid,Uric Acid: blood},
number = {6},
pages = {716--23},
pmid = {18413464},
title = {{Serum urate as a predictor of clinical and radiographic progression in Parkinson disease.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2574855{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {65},
year = {2008}
}
@article{Grunwald2004,
abstract = {We describe and develop a close relationship between two problems that have customarily been regarded as distinct: that of maximizing entropy, and that of minimizing worst-case expected loss.  Using a formulation grounded in the equilibrium theory of zero-sum games between Decision Maker and Nature, these two problems are shown to be dual to each other, the solution to each providing that to the other.  Although Topsoe described this connection for the Shannon entropy over 20 years ago, it does not appear to be widely known even in that important special case. 

We here generalize this theory to apply to arbitrary decision problems and loss functions.  We indicate how an appropriate generalized definition of entropy can be associated with such a problem, and we show that, subject to certain regularity conditions, the above-mentioned duality continues to apply in this extended context.  This simultaneously provides a possible rationale for maximizing entropy and a tool for finding robust Bayes acts.  We also describe the essential identity between the problem of maximizing entropy and that of minimizing a related discrepancy or divergence between distributions.  This leads to an extension, to arbitrary discrepancies, of a well-known minimax theorem for the case of Kullback-Leibler divergence (the "redundancy-capacity theorem'' of information theory).  For the important case of families of distributions having certain mean values specified, we develop simple sufficient conditions and methods for identifying the desired solutions.},
archivePrefix = {arXiv},
arxivId = {math/0410076},
author = {Gr{\"{u}}nwald, Peter D. and {Philip Dawid}, a.},
doi = {10.1214/009053604000000553},
eprint = {0410076},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gr{\"{u}}nwald, Philip Dawid - 2004 - Game theory, maximum entropy, minimum discrepancy and robust Bayesian decision theory.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Additive model,Bayes act,Bregman divergence,Brier score,Convexity,Duality,Equalizer rule,Exponential family,Gamma-minimax,Generalized exponential family,Kullback-Leibler divergence,Logarithmic score,Maximin,Mean-value constraints,Minimax},
number = {4},
pages = {1367--1433},
primaryClass = {math},
title = {{Game theory, maximum entropy, minimum discrepancy and robust Bayesian decision theory}},
volume = {32},
year = {2004}
}
@article{Wang2015a,
author = {Wang, Fang and Li, Renfu and Lei, Zhikun and Ni, Xuelei Sherry and Huo, Xiaoming and Chen, Ming},
doi = {10.1016/j.patrec.2015.06.005},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang et al. - 2015 - Kernel Fusion-Refinement for Semi-supervised Nonlinear Dimension Reduction.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Dimension reduction,Semi-supervised learning,kernel fusion-refinement},
publisher = {Elsevier Ltd.},
title = {{Kernel Fusion-Refinement for Semi-supervised Nonlinear Dimension Reduction}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865515001671},
year = {2015}
}
@article{Lapin2014,
abstract = {Prior knowledge can be used to improve predictive performance of learning algorithms or reduce the amount of data required for training. The same goal is pursued within the learning using privileged information paradigm which was recently introduced by Vapnik et al.and is aimed at utilizing additional information available only at training time-a framework implemented by SV M+. We relate the privileged information to importance weighting and show that the prior knowledge expressible with privileged features can also be encoded by weights associated with every training example. We show that a weighted SVM can always replicate an SV M+ solution, while the converse is not true and we construct a counterexample highlighting the limitations of SV M+. Finally, we touch on the problem of choosing weights for weighted SVMs when privileged features are not available. ?? 2014 Elsevier Ltd.},
archivePrefix = {arXiv},
arxivId = {arXiv:1306.3161v1},
author = {Lapin, Maksim and Hein, Matthias and Schiele, Bernt},
doi = {10.1016/j.neunet.2014.02.002},
eprint = {arXiv:1306.3161v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lapin, Hein, Schiele - 2014 - Learning using privileged information SV M and weighted SVM.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
keywords = {Importance weighting,Prior knowledge,Privileged information,SV M+,SVM,Weighted SVM},
pages = {95--108},
pmid = {24576747},
title = {{Learning using privileged information: SVM+ and weighted SVM}},
volume = {53},
year = {2014}
}
@article{Titterington1976,
abstract = {A Bayesian approach is made to the problem of using individuals of$\backslash$nunconfirmed categories to provide information supplementary to a$\backslash$nbasic data bank of categorized observations. The exact analysis is$\backslash$nbriefly presented, followed by suggestions for more practicable approximate$\backslash$nprocedures, which are applied to examples involving medical and simulated$\backslash$ndata. The general conclusion is that the discriminatory performance$\backslash$nof the data bank can be usefully improved by making use of uncategorized$\backslash$nobservations.},
author = {Titterington, D M},
doi = {10.2307/2347231},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Titterington - 1976 - Updating a diagnostic system using unconfirmed cases.pdf:pdf},
issn = {00359254},
journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
keywords = {bayesian updating,discrimination,distributions,medical differential diagnosis,predictive,undiagnosed individuals},
number = {3},
pages = {238--247},
title = {{Updating a diagnostic system using unconfirmed cases}},
volume = {25},
year = {1976}
}
@inproceedings{Klinkenberg2001,
author = {Klinkenberg, Ralf},
booktitle = {Workshop notes of the IJCAI-01 Workshop on Learning from Temporal and Spatial Data},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Klinkenberg - 2001 - Using labeled and unlabeled data to learn drifting concepts.pdf:pdf},
pages = {16--24},
title = {{Using labeled and unlabeled data to learn drifting concepts}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.23.1798{\&}rep=rep1{\&}type=pdf},
year = {2001}
}
@article{Dougherty2001,
abstract = {In order to study the molecular biological differences between normal and diseased tissues, it is desirable to perform classification among diseases and stages of disease using microarray-based gene-expression values. Owing to the limited number of microarrays typically used in these studies, serious issues arise with respect to the design, performance and analysis of classifiers based on microarray data. This paper reviews some fundamental issues facing small-sample classification: classification rules, constrained classifiers, error estimation and feature selection. It discusses both unconstrained and constrained classifier design from sample data, and the contributions to classifier error from constrained optimization and lack of optimality owing to design from sample data. The difficulty with estimating classifier error when confined to small samples is addressed, particularly estimating the error from training data. The impact of small samples on the ability to include more than a few variables as classifier features is explained.},
author = {Dougherty, Edward R.},
doi = {10.1002/cfg.62},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dougherty - 2001 - Small sample issues for microarray-based classification.pdf:pdf},
issn = {1531-6912},
journal = {Comparative and functional genomics},
month = {jan},
number = {1},
pages = {28--34},
pmid = {18628896},
title = {{Small sample issues for microarray-based classification.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2447190{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {2},
year = {2001}
}
@book{Gelman,
author = {Gelman, Andrew and Hill, Jennifer},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Hill - 2007 - Data Analysis Using Regression and MiltilevelHierarchical Models.pdf:pdf},
title = {{Data Analysis Using Regression and Miltilevel/Hierarchical Models}},
year = {2007}
}
@article{Welling2007,
author = {Welling, M},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/KF-welling-notes.pdf:pdf},
number = {3},
pages = {11},
title = {{The Kalman Filter}},
url = {papers://a6591aa3-238d-4eda-a3d9-0181a4361186/Paper/p217},
year = {2007}
}
@article{Poggio2003,
author = {Poggio, Tomaso and Smale, Steve},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Poggio, Smale - 2003 - The Mathematics of Learning Dealing with Data.pdf:pdf},
journal = {Notices of the AMS},
number = {5},
pages = {537--544},
title = {{The Mathematics of Learning: Dealing with Data}},
volume = {50},
year = {2003}
}
@article{Li2015,
author = {Li, Yu-Feng and Zhou, Zhi-Hua},
doi = {10.1109/TPAMI.2014.2299812},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li, Zhou - 2015 - Towards Making Unlabeled Data Never Hurt.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {jan},
number = {1},
pages = {175--188},
title = {{Towards Making Unlabeled Data Never Hurt}},
volume = {37},
year = {2015}
}
@article{Erren2007,
author = {Erren, Thomas C and Bourne, Philip E},
doi = {10.1371/journal.pcbi.0030102},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Erren, Bourne - 2007 - Ten simple rules for a good poster presentation.pdf:pdf},
issn = {1553-7358},
journal = {PLoS computational biology},
keywords = {Algorithms,Audiovisual Aids,Biomedical Research,Communication,Congresses as Topic,Exhibits as Topic,Information Dissemination,Information Dissemination: methods,Professional Competence},
month = {may},
number = {5},
pages = {e102},
pmid = {17530921},
title = {{Ten simple rules for a good poster presentation.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1876493{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {3},
year = {2007}
}
@article{Cesa-Bianchi2007,
author = {Cesa-Bianchi, Nicol{\`{o}}},
doi = {10.1016/j.tcs.2007.03.053},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cesa-Bianchi - 2007 - Applications of regularized least squares to pattern classification.pdf:pdf},
issn = {03043975},
journal = {Theoretical Computer Science},
keywords = {on-line learning,perceptron,ridge regression,selective sampling},
month = {sep},
number = {3},
pages = {221--231},
title = {{Applications of regularized least squares to pattern classification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S030439750700237X},
volume = {382},
year = {2007}
}
@article{Esposito2015,
author = {Esposito, G. and Martin, M.},
doi = {10.1080/08839514.2015.1035951},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Esposito, Martin - 2015 - A Randomized Algorithm for the Exact Solution of Transductive Support Vector Machines.pdf:pdf},
issn = {0883-9514},
journal = {Applied Artificial Intelligence},
number = {5},
pages = {459--479},
title = {{A Randomized Algorithm for the Exact Solution of Transductive Support Vector Machines}},
url = {http://www.tandfonline.com/doi/full/10.1080/08839514.2015.1035951},
volume = {29},
year = {2015}
}
@article{Lazer2014,
abstract = {In February 2013, Google Flu Trends (GFT) made headlines but not for a reason that Google executives or the creators of the flu tracking system would have hoped. Nature reported that GFT was predicting more than double the proportion of doctor visits for influenza-like illness (ILI) than the Centers for Disease Control and Prevention (CDC), which bases its estimates on surveillance reports from laboratories across the United States (1, 2). This happened despite the fact that GFT was built to predict CDC reports. Given that GFT is often held up as an exemplary use of big data (3, 4), what lessons can we draw from this error?},
author = {Lazer, David and Kennedy, Ryan and King, Gary and Vespignani, Alessandro},
doi = {10.1126/science.1248506},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/0314policyforumff.pdf:pdf},
isbn = {0036-8075},
issn = {1095-9203},
journal = {Science},
number = {6167},
pages = {1203--1205},
pmid = {24626916},
title = {{The Parable of Google Flu: Traps in Big Data Analysis}},
url = {http://www.sciencemag.org/content/343/6176/1203},
volume = {343},
year = {2014}
}
@article{Krahenbuhl2011,
abstract = {Most state-of-the-art techniques for multi-class image segmentation and labeling use conditional random fields defined over pixels or image regions. While regionlevel models often feature dense pairwise connectivity, pixel-level models are considerably larger and have only permitted sparse graph structures. In this paper, we consider fully connected CRF models defined on the complete set of pixels in an image. The resulting graphs have billions of edges, making traditional inference algorithms impractical. Our main contribution is a highly efficient approximate inference algorithm for fully connected CRF models in which the pairwise edge potentials are defined by a linear combination of Gaussian kernels. Our experiments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy.},
archivePrefix = {arXiv},
arxivId = {1210.5644},
author = {Krahenbuhl, Philipp and Koltun, Vladlen and Kr¨ahenb¨uhl, Philipp and Koltun, Vladlen and Krahenbuhl, Philipp},
eprint = {1210.5644},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Krahenbuhl et al. - 2011 - Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials.pdf:pdf},
isbn = {9781618395993},
journal = {Advances in Neural Information Processing Systems},
keywords = {conditional random field,filtering,message passing,sampling,segmentation},
number = {4},
pages = {1--9},
title = {{Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials}},
year = {2011}
}
@article{Shore1980,
author = {Shore, John E. and Johnson, Rodney W.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shore, Johnson - 1980 - Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
number = {1},
pages = {26--37},
title = {{Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1056144},
volume = {26},
year = {1980}
}
@inproceedings{Collobert2006a,
author = {Collobert, Ronan and Sinz, Fabian and Weston, Jason and Bottou, L},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Collobert et al. - 2006 - Trading convexity for scalability.pdf:pdf},
pages = {201--208},
title = {{Trading convexity for scalability}},
url = {http://dl.acm.org/citation.cfm?id=1143870},
year = {2006}
}
@article{Hand2013,
author = {Hand, D.J. and Anagnostopoulos, C.},
doi = {10.1016/j.patrec.2012.12.004},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hand, Anagnostopoulos - 2013 - When is the area under the receiver operating characteristic curve an appropriate measure of classifier p.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {area under the curve},
month = {apr},
number = {5},
pages = {492--495},
title = {{When is the area under the receiver operating characteristic curve an appropriate measure of classifier performance?}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865512003923},
volume = {34},
year = {2013}
}
@inproceedings{Cozman2002,
author = {Cozman, FG and Cohen, Ira},
booktitle = {FLAIRS Conference},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cozman, Cohen - 2002 - Unlabeled Data Can Degrade Classification Performance of Generative Classifiers.pdf:pdf},
pages = {327--331},
title = {{Unlabeled Data Can Degrade Classification Performance of Generative Classifiers.}},
url = {http://www.aaai.org/Papers/FLAIRS/2002/FLAIRS02-065.pdf},
year = {2002}
}
@article{Bartlett2006,
author = {Bartlett, Peter L and Jordan, Michael I. and McAuliffe, Jon D},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bartlett, Jordan, McAuliffe - 2006 - Convexity, Classification, and Risk Bounds.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {boosting,convex optimization},
number = {473},
pages = {138--156},
title = {{Convexity, Classification, and Risk Bounds}},
volume = {101},
year = {2006}
}
@book{Glymour2001,
author = {Glymour, Clark},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Glymour - 2001 - The Mind's Arrow.pdf:pdf},
isbn = {0262072203},
title = {{The Mind's Arrow}},
year = {2001}
}
@inproceedings{Zhou2005a,
author = {Zhou, Zhi-hua and Li, Ming},
booktitle = {International Joint Conferences on Artificial Intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Li - 2005 - Semi-Supervised Regression with Co-Training.pdf:pdf},
title = {{Semi-Supervised Regression with Co-Training.}},
url = {http://ijcai.org/Past Proceedings/IJCAI-05/PDF/0689.pdf},
year = {2005}
}
@article{Kalousis1999,
author = {Kalousis, Alexis and Theoharis, T},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kalousis, Theoharis - 1999 - NOEMON An intelligent Assistant for Classifier Selection.pdf:pdf},
journal = {Intelligent Data Analysis},
keywords = {classifier comparison,classifier selection,dataset morphology,multidimensional metrics},
number = {5},
pages = {319--337},
title = {{NOEMON: An intelligent Assistant for Classifier Selection}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.38.7762},
volume = {3},
year = {1999}
}
@article{Braga-Neto2004,
abstract = {MOTIVATION: Microarray classification typically possesses two striking attributes: (1) classifier design and error estimation are based on remarkably small samples and (2) cross-validation error estimation is employed in the majority of the papers. Thus, it is necessary to have a quantifiable understanding of the behavior of cross-validation in the context of very small samples. RESULTS: An extensive simulation study has been performed comparing cross-validation, resubstitution and bootstrap estimation for three popular classification rules-linear discriminant analysis, 3-nearest-neighbor and decision trees (CART)-using both synthetic and real breast-cancer patient data. Comparison is via the distribution of differences between the estimated and true errors. Various statistics for the deviation distribution have been computed: mean (for estimator bias), variance (for estimator precision), root-mean square error (for composition of bias and variance) and quartile ranges, including outlier behavior. In general, while cross-validation error estimation is much less biased than resubstitution, it displays excessive variance, which makes individual estimates unreliable for small samples. Bootstrap methods provide improved performance relative to variance, but at a high computational cost and often with increased bias (albeit, much less than with resubstitution).},
author = {Braga-Neto, Ulisses M and Dougherty, Edward R.},
doi = {10.1093/bioinformatics/btg419},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Braga-Neto, Dougherty - 2004 - Is cross-validation valid for small-sample microarray classification.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Benchmarking,Benchmarking: methods,Breast Neoplasms,Breast Neoplasms: diagnosis,Breast Neoplasms: genetics,Computer Simulation,Gene Expression Profiling,Gene Expression Profiling: methods,Genetic Predisposition to Disease,Genetic Predisposition to Disease: genetics,Genetic Testing,Genetic Testing: methods,Humans,Models, Genetic,Models, Statistical,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Pattern Recognition, Automated,Reproducibility of Results,Sample Size,Sensitivity and Specificity},
month = {feb},
number = {3},
pages = {374--80},
pmid = {14960464},
title = {{Is cross-validation valid for small-sample microarray classification?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/14960464},
volume = {20},
year = {2004}
}
@article{Guo2010,
author = {Guo, Yuanyuan and Niu, Xiaoda and Zhang, Harry},
doi = {10.1109/ICDM.2010.66},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Guo, Niu, Zhang - 2010 - An Extensive Empirical Study on Semi-supervised Learning.pdf:pdf},
isbn = {978-1-4244-9131-5},
journal = {IEEE International Conference on Data Mining},
keywords = {-semi-supervised learning,bayesian classifiers},
month = {dec},
pages = {186--195},
publisher = {Ieee},
title = {{An Extensive Empirical Study on Semi-supervised Learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5693972},
year = {2010}
}
@article{Blei2016a,
abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability distributions. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation about the posterior. In this paper, we review variational inference (VI), a method from machine learning that approximates probability distributions through optimization. VI has been used in myriad applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of distributions and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this widely-used class of algorithms.},
archivePrefix = {arXiv},
arxivId = {1601.00670},
author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
doi = {10.1080/01621459.2017.1285773},
eprint = {1601.00670},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Blei, Kucukelbir, McAuliffe - 2016 - Variational Inference A Review for Statisticians.pdf:pdf;:Users/jkrijthe/Documents/Mendeley Desktop/Variational Inference A Review for Statisticians.pdf:pdf},
isbn = {1601.00670},
issn = {0162-1459},
journal = {arXiv},
keywords = {Graphical Model,Variational Inference},
pages = {1--33},
title = {{Variational Inference: A Review for Statisticians}},
url = {http://arxiv.org/abs/1601.00670},
year = {2016}
}
@inproceedings{Drummond2009,
abstract = {At various machine learning conferences, at various times, there have been discussions arising from the inability to replicate the experimental results published in a paper. There seems to be a wide spread view that we need to do something to address this problem, as it is essential to the advancement of our field. The most compelling argument would seem to be that reproducibility of experimental results is the hallmark of science. Therefore, given that most of us regard machine learning as a scientific discipline, being able to replicate experiments is paramount. I want to challenge this view by separating the notion of reproducibility, a generally desirable property, from replicability, its poor cousin. I claim there are important differences between the two. Reproducibility requires changes; replicability avoids them. Although reproducibility is desirable, I contend that the impoverished version, replicability, is one not worth having.},
author = {Drummond, Chris},
booktitle = {Proceedings of the Evaluation Methods for Machine Learning Workshop at the 26th ICML},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Drummond - 2009 - Replicability is not Reproducibility Nor is it Good Science.pdf:pdf},
keywords = {Artificial Intelligence},
title = {{Replicability is not Reproducibility: Nor is it Good Science}},
year = {2009}
}
@article{Hand2014,
author = {Hand, David J.},
doi = {10.1214/13-STS446},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hand - 2014 - Wonderful Examples, but Let's not Close Our Eyes.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Frequentist, likelihood inference, Neyman-Pearson,and phrases,frequentist,hypothesis testing,informative and thought-provoking,likelihood inference,making specific comments,neyman,on each of these,pearson,schools of inference,space prohibits me from},
month = {feb},
number = {1},
pages = {98--100},
title = {{Wonderful Examples, but Let's not Close Our Eyes}},
url = {http://projecteuclid.org/euclid.ss/1399645735},
volume = {29},
year = {2014}
}
@article{Fernandez-Delgado2014,
abstract = {We evaluate 179 classifiers arising from 17 families (discriminant analysis, Bayesian, neural networks, support vector machines, decision trees, rule-based classifiers, boosting, bagging, stacking, random forests and other ensembles, generalized linear models, nearest-neighbors, partial least squares and principal component regression, logistic and multino-mial regression, multiple adaptive regression splines and other methods), implemented in Weka, R (with and without the caret package), C and Matlab, including all the relevant classifiers available today. We use 121 data sets, which represent the whole UCI data base (excluding the large-scale problems) and other own real problems, in order to achieve significant conclusions about the classifier behavior, not dependent on the data set col-lection. The classifiers most likely to be the bests are the random forest (RF) versions, the best of which (implemented in R and accessed via caret) achieves 94.1{\%} of the maximum accuracy overcoming 90{\%} in the 84.3{\%} of the data sets. However, the dif-ference is not statistically significant with the second best, the SVM with Gaussian kernel implemented in C using LibSVM, which achieves 92.3{\%} of the maximum accuracy. A few models are clearly better than the remaining ones: random forest, SVM with Gaussian and polynomial kernels, extreme learning machine with Gaussian kernel, C5.0 and avNNet (a committee of multi-layer perceptrons implemented in R with the caret package). The random forest is clearly the best family of classifiers (3 out of 5 bests classifiers are RF), followed by SVM (4 classifiers in the top-10), neural networks and boosting ensembles (5 and 3 members in the top-20, respectively).},
author = {Fern{\'{a}}ndez-Delgado, Manuel and Cernadas, Eva and Barro, Sen{\'{e}}n and Amorim, Dinani and {Amorim Fern{\'{a}}ndez-Delgado}, Dinani},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fern{\'{a}}ndez-Delgado et al. - 2014 - Do we Need Hundreds of Classifiers to Solve Real World Classification Problems.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {Bayesian classifiers,UCI data base,classification,decision trees,discriminant analysis,ensembles,generalized linear models,logistic and multinomial regression,multiple adaptive regression splines,nearest-neighbors,neural networks,random forest,rule-based classifiers,support vector machine},
pages = {3133--3181},
title = {{Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?}},
volume = {15},
year = {2014}
}
@incollection{Krijthe2016rssl,
archivePrefix = {arXiv},
arxivId = {1612.07993},
author = {Krijthe, Jesse Hendrik},
booktitle = {Reproducible Research in Pattern Recognition. RRPR 2016. Lecture Notes in Computer Science, vol 10214.},
doi = {10.1007/978-3-319-56414-2_8},
editor = {Kerautret, B. and Colom, M. and Monasse, P.},
eprint = {1612.07993},
isbn = {978-3-319-56414-2},
pages = {104--115},
publisher = {Springer International Publishing},
title = {{RSSL: R package for Semi-supervised Learning}},
year = {2016}
}
@article{Hughes1968,
author = {Hughes, Gordon F.},
doi = {10.1109/TIT.1968.1054102},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hughes - 1968 - On the Mean Accuracy of Statistical Pattern Recognizers.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
number = {1},
pages = {55--63},
title = {{On the Mean Accuracy of Statistical Pattern Recognizers}},
volume = {14},
year = {1968}
}
@article{Fallis2013,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fallis, A.G},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Etz et al, 8 steps v2.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{How to become a Bayesian in eight easy steps: An annotated reading list}},
volume = {53},
year = {2013}
}
@inproceedings{Bernad2004,
author = {Bernad{\'{o}}-Mansilla, Ester and Ho, Tin Kam},
booktitle = {Proceedings of the 17th International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bernad{\'{o}}-Mansilla, Ho - 2004 - On classifier domains of competence.pdf:pdf},
title = {{On classifier domains of competence}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1334026},
year = {2004}
}
@inproceedings{Zhang2004,
author = {Zhang, P. and Peng, J.},
booktitle = {Proceedings of the 17th International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2004.1334050},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang, Peng - 2004 - SVM vs regularized least squares classification.pdf:pdf},
isbn = {0-7695-2128-2},
pages = {176--179},
title = {{SVM vs regularized least squares classification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1334050},
year = {2004}
}
@article{Suykens1999,
author = {Suykens, Johan A. K. and Vandewalle, J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Suykens, Vandewalle - 1999 - Least Squares Support Vector Machine Classifiers.pdf:pdf},
journal = {Neural Processing Letters},
keywords = {abbreviations,classification,linear least squares,radial basis,radial basis function kernel,rbf,support vector machines,svm,vapnik-chervonenkis,vc},
pages = {293--300},
title = {{Least Squares Support Vector Machine Classifiers}},
volume = {9},
year = {1999}
}
@inproceedings{Yu2013,
author = {Yu, Felix X. and Liu, Dong and Kumar, Sanjiv and Jebara, Tony and Chang, Shih-Fu},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yu et al. - 2013 - {\$}{\$}backslash{\$} propto {\$} SVM for Learning with Label Proportions.pdf:pdf},
pages = {504--512},
title = {{{\$}{\$}$\backslash$backslash{\$} propto {\$} SVM for Learning with Label Proportions}},
year = {2013}
}
@inproceedings{Collins1999,
author = {Collins, Michael and Singer, Yoram},
booktitle = {Proceedings of the joint SIGDAT conference on empirical methods in natural language processing and very large corpora},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Collins, Singer - 1999 - Unsupervised models for named entity classification.pdf:pdf},
pages = {189--196},
title = {{Unsupervised models for named entity classification}},
url = {http://acl.ldc.upenn.edu/W/W99/W99-0613.pdf?ref=Sawos.OrgR{\%}7B.����ǝ�‡{\%}5E��{\`{e}}7},
year = {1999}
}
@inproceedings{Goodman2016,
abstract = {We summarize the potential impact that the European Union's new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which "significantly affect" users. The law will also create a "right to explanation," whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for machine learning researchers to take the lead in designing algorithms and evaluation frameworks which avoid discrimination.},
archivePrefix = {arXiv},
arxivId = {1606.08813},
author = {Goodman, Bryce and Flaxman, Seth},
booktitle = {ICML Workshop on Human Interpretability in Machine Learning (WHI 2016)},
eprint = {1606.08813},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Goodman, Flaxman - 2016 - EU regulations on algorithmic decision-making and a right to explanation.pdf:pdf},
keywords = {machine learning},
title = {{EU regulations on algorithmic decision-making and a "right to explanation"}},
url = {http://arxiv.org/abs/1606.08813},
year = {2016}
}
@inproceedings{Shaffer1994,
author = {Schaffer, Cullen},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schaffer - 1994 - A conservation law for generalization performance.pdf:pdf},
title = {{A conservation law for generalization performance}},
url = {http://dml.cs.byu.edu/{~}cgc/docs/mldm{\_}tools/Reading/LCG.pdf},
year = {1994}
}
@inproceedings{Giraud-Carrier2008,
author = {Giraud-carrier, Christophe},
booktitle = {Tutorial at the 2008 International Conference on Machine Learning and Applications},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Giraud-carrier - 2008 - Metalearning - A Tutorial.pdf:pdf},
number = {December},
title = {{Metalearning - A Tutorial}},
url = {http://dml.cs.byu.edu/{~}cgc/docs/ICMLA2008Tut/ICMLA 2008.pdf},
year = {2008}
}
@phdthesis{Marlin2008,
author = {Marlin, BM},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Marlin - 2008 - Missing data problems in machine learning.pdf:pdf},
school = {University of Toronto},
title = {{Missing data problems in machine learning}},
url = {http://www-devel.cs.ubc.ca/{~}bmarlin/research/phd{\_}thesis/marlin-phd-thesis.pdf},
year = {2008}
}
@book{Basu2006,
author = {Basu, Mitra and Ho, Tin Kam},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Basu, Ho - 2006 - Data complexity in pattern recognition.pdf:pdf},
isbn = {9781846281716},
title = {{Data complexity in pattern recognition}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=GflBKbzym9oC{\&}oi=fnd{\&}pg=PR11{\&}dq=Data+Complexity+in+Pattern+Recognition{\&}ots=igbI3IXn6d{\&}sig=-7L3L4iU5lzLaNaCVoEux{\_}GbVn4},
year = {2006}
}
@article{VanderMaaten2008,
author = {van der Maaten, L.J.P. and Hinton, G.E.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/van der Maaten, Hinton - 2008 - Visualizing High-Dimensional Data Using t-SNE.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {dimensionality reduction,embedding algorithms,manifold learning,multidimensional scaling,visualization},
pages = {2579--2605},
title = {{Visualizing High-Dimensional Data Using t-SNE}},
volume = {9},
year = {2008}
}
@unpublished{Sakai2016,
abstract = {Semi-supervised learning based on the low-density separation principle such as the cluster and manifold assumptions has been extensively studied in the last decades. However, such semi-supervised learning methods do not always perform well due to violation of the cluster and manifold assumptions. In this paper, we propose a novel approach to semi-supervised learning that does not require such restrictive assumptions. Our key idea is to combine learning from positive and negative data (standard supervised learning) and learning from positive and unlabeled data (PU learning), the latter is guaranteed to be able to utilize unlabeled data without the cluster and manifold assumptions. We theoretically and experimentally show the usefulness of our approach.},
archivePrefix = {arXiv},
arxivId = {1605.06955},
author = {Sakai, Tomoya and du Plessis, Marthinus Christoffel and Niu, Gang and Sugiyama, Masashi},
eprint = {1605.06955},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sakai et al. - 2016 - Beyond the Low-density Separation Principle A Novel Approach to Semi-supervised Learning.pdf:pdf},
title = {{Beyond the Low-density Separation Principle: A Novel Approach to Semi-supervised Learning}},
year = {2016}
}
@article{Poggio2004,
abstract = {Developing theoretical foundations for learning is a key step towards understanding intelligence. 'Learning from examples' is a paradigm in which systems (natural or artificial) learn a functional relationship from a training set of examples. Within this paradigm, a learning algorithm is a map from the space of training sets to the hypothesis space of possible functional solutions. A central question for the theory is to determine conditions under which a learning algorithm will generalize from its finite training set to novel examples. A milestone in learning theory was a characterization of conditions on the hypothesis space that ensure generalization for the natural class of empirical risk minimization (ERM) learning algorithms that are based on minimizing the error on the training set. Here we provide conditions for generalization in terms of a precise stability property of the learning process: when the training set is perturbed by deleting one example, the learned hypothesis does not change much. This stability property stipulates conditions on the learning map rather than on the hypothesis space, subsumes the classical theory for ERM algorithms, and is applicable to more general algorithms. The surprising connection between stability and predictivity has implications for the foundations of learning theory and for the design of novel algorithms, and provides insights into problems as diverse as language learning and inverse problems in physics and engineering.},
author = {Poggio, Tomaso and Rifkin, Ryan and Mukherjee, Sayan and Niyogi, Partha},
doi = {10.1038/nature02341},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Poggio et al. - 2004 - General conditions for predictivity in learning theory.pdf:pdf},
issn = {1476-4687},
journal = {Nature},
keywords = {Algorithms,Intelligence,Language,Learning,Learning: physiology,Models, Theoretical,Probability,Research Design},
month = {mar},
number = {6981},
pages = {419--22},
pmid = {15042089},
title = {{General conditions for predictivity in learning theory.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15042089},
volume = {428},
year = {2004}
}
@article{Gelman2006,
abstract = {It is common to summarize statistical comparisons by declarations of statistical significance or nonsignificance. Here we discuss one problem with such declarations, namely that changes in statistical significance are often not themselves statistically significant. By this, we are not merely making the commonplace observation that any particular threshold is arbitrary?for example, only a small change is required to move an estimate from a 5.1{\%} significance level to 4.9{\%}, thus moving it into statistical significance. Rather, we are pointing out that even large changes in significance levels can correspond to small, nonsignificant changes in the underlying quantities.The error we describe is conceptually different from other oft-cited problems?that statistical significance is not the same as practical importance, that dichotomization into significant and nonsignificant results encourages the dismissal of observed differences in favor of the usually less interesting null hypothesis of no difference, and that any particular threshold for declaring significance is arbitrary. We are troubled by all of these concerns and do not intend to minimize their importance. Rather, our goal is to bring attention to this additional error of interpretation. We illustrate with a theoretical example and two applied examples. The ubiquity of this statistical error leads us to suggest that students and practitioners be made more aware that the difference between ?significant? and ?not significant? is not itself statistically significant. It is common to summarize statistical comparisons by declarations of statistical significance or nonsignificance. Here we discuss one problem with such declarations, namely that changes in statistical significance are often not themselves statistically significant. By this, we are not merely making the commonplace observation that any particular threshold is arbitrary?for example, only a small change is required to move an estimate from a 5.1{\%} significance level to 4.9{\%}, thus moving it into statistical significance. Rather, we are pointing out that even large changes in significance levels can correspond to small, nonsignificant changes in the underlying quantities.The error we describe is conceptually different from other oft-cited problems?that statistical significance is not the same as practical importance, that dichotomization into significant and nonsignificant results encourages the dismissal of observed differences in favor of the usually less interesting null hypothesis of no difference, and that any particular threshold for declaring significance is arbitrary. We are troubled by all of these concerns and do not intend to minimize their importance. Rather, our goal is to bring attention to this additional error of interpretation. We illustrate with a theoretical example and two applied examples. The ubiquity of this statistical error leads us to suggest that students and practitioners be made more aware that the difference between ?significant? and ?not significant? is not itself statistically significant.},
author = {Gelman, Andrew and Stern, Hal},
doi = {10.1198/000313006X152649},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Stern - 2006 - The Difference Between “Significant” and “Not Significant” is not Itself Statistically Significant.pdf:pdf},
isbn = {0602440371100},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {comparison,hypothesis testing,meta-analysis,pairwise,replication},
number = {4},
pages = {328--331},
title = {{The Difference Between “Significant” and “Not Significant” is not Itself Statistically Significant}},
volume = {60},
year = {2006}
}
@article{Box1976,
abstract = {Accessed: 30-03-2017 16:02 UTC JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. Aspects of scientific method are discussed: In particular, its repre-sentation as a motivated iteration in which, in succession, practice confronts theory, and theory, practice. Rapid progress requires suffi-cient flexibility to profit from such confrontations, and the ability to devise parsimonious but effective models, to worry selectively about model inadequacies and to employ mathematics skillfully but appropriately. The development of statistical methods at Rothamsted Experimental Station by Sir Ronald Fisher is used to illustrate these themes.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Box, George E P},
doi = {10.1641/B570910},
eprint = {arXiv:1011.1669v3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/2286841.pdf:pdf},
isbn = {0006-3568},
issn = {0006-3568},
journal = {Science and Statistics Author Journal of the American Statistical Association},
number = {356},
pages = {791--799},
pmid = {496},
title = {{Science and Statistics}},
url = {http://www.jstor.org/stable/2286841 http://about.jstor.org/terms},
volume = {71},
year = {1976}
}
@incollection{Krijthe2016reproducing,
archivePrefix = {arXiv},
arxivId = {1612.08650},
author = {Krijthe, Jesse Hendrik and Loog, Marco},
booktitle = {Reproducible Research in Pattern Recognition. RRPR 2016. Lecture Notes in Computer Science, vol 10214.},
doi = {10.1007/978-3-319-56414-2_4},
editor = {Kerautret, B. and Colom, M. and Monasse, P.},
eprint = {1612.08650},
isbn = {978-3-319-56414-2},
pages = {48--59},
publisher = {Springer International Publishing},
title = {{Reproducible Pattern Recognition Research: The Case of Optimistic SSL}},
year = {2016}
}
@article{Donoho2009,
author = {Donoho, David L. and Maleki, Arian and Rahman, Inam Ur and Shahram, Morteza and Stodden, Victoria},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Donoho et al. - 2009 - Reproducible Research in Computational Harmonic Analysis.pdf:pdf},
issn = {0036-8075, 1095-9203},
journal = {Computing in Science {\&} Engineering},
number = {1},
pages = {8--18},
title = {{Reproducible Research in Computational Harmonic Analysis}},
volume = {11},
year = {2009}
}
@article{Wang2007a,
author = {Wang, Junhui and Shen, Xiaotong and Pan, Wei},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Shen, Pan - 2007 - On Transductive Support Vector Machines.pdf:pdf},
journal = {Contemporary Mathematics},
pages = {7--19},
title = {{On Transductive Support Vector Machines}},
volume = {443},
year = {2007}
}
@article{Bodo2015,
abstract = {{\textless}p{\textgreater}Semi-supervised learning has become an important and thoroughly studied subdomain of machine learning in the past few years, because gathering large unlabeled data is almost costless, and the costly human labeling process can be minimized by semi-supervision. Label propagation is a transductive semi-supervised learning method that operates on the—most of the time undirected—data graph. It was introduced in [8] and since many variants were proposed. However, the base algorithm has two variants: the first variant presented in [8] and its slightly modified version used afterwards, e.g. in [7]. This paper presents and compares the two algorithms—both theoretically and experimentally—and also tries to make a recommendation which variant to use.{\textless}/p{\textgreater}},
author = {Bod{\'{o}}, Zal{\'{a}}n and Csat{\'{o}}, Lehel},
doi = {10.1515/ausi-2015-0010},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bod{\'{o}}, Csat{\'{o}} - 2015 - A note on label propagation for semi-supervised learning.pdf:pdf},
issn = {2066-7760},
journal = {Acta Universitatis Sapientiae, Informatica},
keywords = {and phrases,label propagation,semi-supervised learning},
number = {1},
pages = {18--30},
title = {{A note on label propagation for semi-supervised learning}},
url = {http://www.degruyter.com/view/j/ausi.2015.7.issue-1/ausi-2015-0010/ausi-2015-0010.xml},
volume = {7},
year = {2015}
}
@book{Berger1985,
author = {Berger, James O},
publisher = {Springer},
title = {{Statistical decision theory and Bayesian analysis}},
year = {1985}
}
@article{Meinshausen2010,
author = {Meinshausen, Nicolai and B{\"{u}}hlmann, Peter},
doi = {10.1111/j.1467-9868.2010.00740.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Meinshausen, B{\"{u}}hlmann - 2010 - Stability selection.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {high dimensional data,resampling,stability selection,structure estimation},
month = {jul},
number = {4},
pages = {417--473},
title = {{Stability selection}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2010.00740.x},
volume = {72},
year = {2010}
}
@inproceedings{Ravi2016,
abstract = {Traditional graph-based semi-supervised learning (SSL) approaches, even though widely applied, are not suited for massive data and large label scenarios since they scale linearly with the number of edges {\$}|E|{\$} and distinct labels {\$}m{\$}. To deal with the large label size problem, recent works propose sketch-based methods to approximate the distribution on labels per node thereby achieving a space reduction from {\$}O(m){\$} to {\$}O(\backslashlog m){\$}, under certain conditions. In this paper, we present a novel streaming graph-based SSL approximation that captures the sparsity of the label distribution and ensures the algorithm propagates labels accurately, and further reduces the space complexity per node to {\$}O(1){\$}. We also provide a distributed version of the algorithm that scales well to large data sizes. Experiments on real-world datasets demonstrate that the new method achieves better performance than existing state-of-the-art algorithms with significant reduction in memory footprint. We also study different graph construction mechanisms for natural language applications and propose a robust graph augmentation strategy trained using state-of-the-art unsupervised deep learning architectures that yields further significant quality gains.},
archivePrefix = {arXiv},
arxivId = {1512.01752},
author = {Ravi, Sujith and Diao, Qiming},
booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS)},
eprint = {1512.01752},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ravi, Diao - 2016 - Large Scale Distributed Semi-Supervised Learning Using Streaming Approximation.pdf:pdf},
title = {{Large Scale Distributed Semi-Supervised Learning Using Streaming Approximation}},
volume = {51},
year = {2016}
}
@article{Ireland1968,
author = {Ireland, C.T. and Kullback, S.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ireland, Kullback - 1968 - Minimum Discrimination Information Estimation.pdf:pdf},
journal = {Biometrics},
number = {3},
pages = {707--713},
title = {{Minimum Discrimination Information Estimation}},
url = {http://www.jstor.org/stable/10.2307/2528330},
volume = {24},
year = {1968}
}
@article{Minka1998,
author = {Minka, Thomas P},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Minka - 1998 - Expectation-Maximization as lower bound maximization.pdf:pdf},
number = {1977},
pages = {1--8},
title = {{Expectation-Maximization as lower bound maximization}},
year = {1998}
}
@inproceedings{Vandewalle2008,
author = {Vandewalle, Vincent and Biernacki, Christophe and Celeux, Gilles and Govaert, Gerard},
booktitle = {vincent.vandewalle.perso.sfr.fr},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vandewalle et al. - 2008 - Are unlabeled data useful in semi-supervised model-based classification Combining hypothesis testing and mode.pdf:pdf},
title = {{Are unlabeled data useful in semi-supervised model-based classification? Combining hypothesis testing and model choice}},
url = {http://vincent.vandewalle.perso.sfr.fr/documents/recherche/articles/vbcg.pdf},
year = {2008}
}
@inproceedings{Lawrence2004,
author = {Lawrence, Neil D. and Jordan, Michael I.},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lawrence, Jordan - 2004 - Semi-supervised learning via Gaussian processes.pdf:pdf},
pages = {753--760},
title = {{Semi-supervised learning via Gaussian processes}},
year = {2004}
}
@article{Pearl2009a,
author = {Pearl, Judea},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pearl - 2009 - The Art and Science of Cause and Effect.pdf:pdf},
journal = {Cambridge University Press},
number = {November 1996},
pages = {401--428},
title = {{The Art and Science of Cause and Effect}},
year = {2009}
}
@book{Taleb2007,
author = {Taleb, Nassim Nicholas},
publisher = {Random House},
title = {{The Black Swan: The Impact of the Highly Improbable}},
year = {2007}
}
@phdthesis{Mika2002,
author = {Mika, Sebastian},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mika - 2002 - Kernel fisher discriminants.pdf:pdf},
title = {{Kernel fisher discriminants}},
url = {http://opus.kobv.de/tuberlin/volltexte/2003/477/},
year = {2002}
}
@article{Wager2015,
abstract = {Many scientific and engineering challenges---ranging from personalized medicine to customized marketing recommendations---require an understanding of treatment effect heterogeneity. In this paper, we develop a non-parametric causal forest for estimating heterogeneous treatment effects that extends Breiman's widely used random forest algorithm. Given a potential outcomes framework with unconfoundedness, we show that causal forests are pointwise consistent for the true treatment effect, and have an asymptotically Gaussian and centered sampling distribution. We also discuss a practical method for constructing asymptotic confidence intervals for the true treatment effect that are centered at the causal forest estimates. Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms that, to our knowledge, is the first set of results that allows any type of random forest, including classification and regression forests, to be used for provably valid statistical inference. In experiments, we find causal forests to be substantially more powerful than classical methods based on nearest-neighbor matching, especially as the number of covariates increases.},
archivePrefix = {arXiv},
arxivId = {1510.04342},
author = {Wager, Stefan and Athey, Susan},
doi = {10.1073/pnas.1510489113},
eprint = {1510.04342},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/1510.04342.pdf:pdf},
issn = {1091-6490},
journal = {Working Paper},
keywords = {adaptive nearest neighbors matching,asymptotic normality,outcomes,potential,unconfoundedness},
pages = {1--43},
pmid = {27382149},
title = {{Estimation and Inference of Heterogeneous Treatment Effects using Random Forests}},
url = {http://arxiv.org/abs/1510.04342},
year = {2015}
}
@article{Reid2011,
abstract = {We unify f-divergences, Bregman divergences, surrogate loss bounds (regret bounds), proper scoring rules, matching losses, cost curves, ROC-curves and information. We do this by systematically studying integral and variational representations of these objects and in so doing identify their primitives which all are related to cost-sensitive binary classification. As well as clarifying relationships between generative and discriminative views of learning, the new machinery leads to tight and more general surrogate loss bounds and generalised Pinsker inequalities relating f-divergences to variational divergence. The new viewpoint illuminates existing algorithms: it provides a new derivation of Support Vector Machines in terms of divergences and relates Maximum Mean Discrepancy to Fisher Linear Discriminants. It also suggests new techniques for estimating f-divergences.},
archivePrefix = {arXiv},
arxivId = {0901.0356},
author = {Reid, Mark D. and Williamson, Robert C.},
eprint = {0901.0356},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Reid, Williamson - 2011 - Information, divergence and risk for binary experiments.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {classification,divergence,loss functions,regret bounds,statistical information},
pages = {731--817},
title = {{Information, divergence and risk for binary experiments}},
url = {http://arxiv.org/abs/0901.0356 http://dl.acm.org/citation.cfm?id=2021029},
volume = {12},
year = {2011}
}
@article{McDonald2012,
archivePrefix = {arXiv},
arxivId = {1212.0463},
author = {McDonald, Daniel J. and Shalizi, Cosma Rohilla and Schervish, Mark},
eprint = {1212.0463},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/13-336.pdf:pdf},
keywords = {generalization error,model selection,prediction risk,state-space,vc dimension},
pages = {1--34},
title = {{Nonparametric risk bounds for time-series forecasting}},
volume = {18},
year = {2012}
}
@incollection{Clarke1962,
address = {New York},
author = {Clarke, Arthur C.},
booktitle = {Profiles of the Future},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Unknown - Unknown - Futurists{\_}Hazards{\_}of{\_}Prophecy.Pdf.pdf:pdf},
publisher = {H.M.H. Publishing Co.},
title = {{Hazards of Prophecy}},
year = {1962}
}
@inproceedings{Brazdil1994,
author = {Brazdil, Pavel B. and Gama, Joao and Henery, Bob},
booktitle = {Proceedings of the European conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brazdil, Gama, Henery - 1994 - Characterizing the applicability of classification algorithms using meta-level learning.pdf:pdf},
pages = {83--102},
title = {{Characterizing the applicability of classification algorithms using meta-level learning}},
url = {http://link.springer.com/chapter/10.1007/3-540-57868-4{\_}52},
year = {1994}
}
@article{Kall2007,
abstract = {Shotgun proteomics uses liquid chromatography-tandem mass spectrometry to identify proteins in complex biological samples. We describe an algorithm, called Percolator, for improving the rate of confident peptide identifications from a collection of tandem mass spectra. Percolator uses semi-supervised machine learning to discriminate between correct and decoy spectrum identifications, correctly assigning peptides to 17{\%} more spectra from a tryptic Saccharomyces cerevisiae dataset, and up to 77{\%} more spectra from non-tryptic digests, relative to a fully supervised approach.},
author = {K{\"{a}}ll, Lukas and Canterbury, Jesse D and Weston, Jason and Noble, William Stafford and MacCoss, Michael J},
doi = {10.1038/nmeth1113},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/K{\"{a}}ll et al. - 2007 - Semi-supervised learning for peptide identification from shotgun proteomics datasets.pdf:pdf},
journal = {Nature methods},
number = {11},
pages = {923--925},
title = {{Semi-supervised learning for peptide identification from shotgun proteomics datasets.}},
volume = {4},
year = {2007}
}
@inproceedings{Kohavi1995,
author = {Kohavi, Ron},
booktitle = {International Joint Conferences on Artificial Intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kohavi - 1995 - A study of cross-validation and bootstrap for accuracy estimation and model selection.pdf:pdf},
number = {2},
pages = {1137--1145},
title = {{A study of cross-validation and bootstrap for accuracy estimation and model selection}},
url = {http://frostiebek.free.fr/docs/Machine Learning/validation-1.pdf},
volume = {14},
year = {1995}
}
@article{Abadi2015,
author = {Abadi, Martin and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Shlens, Jon and Steiner, Benoit and Sutskever, Ilya and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vinyals, Oriol and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Abadi et al. - 2015 - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:pdf},
title = {{TensorFlow : Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
year = {2015}
}
@phdthesis{Lu2009,
author = {Lu, Tyler},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lu - 2009 - Fundamental Limitations of Semi-Supervised Learning.pdf:pdf},
title = {{Fundamental Limitations of Semi-Supervised Learning}},
year = {2009}
}
@article{Telgarsky2015a,
abstract = {This paper proves, in very general settings, that convex risk minimization is a procedure to select a unique conditional probability model determined by the classification problem. Unlike most previous work, we give results that are general enough to include cases in which no minimum exists, as occurs typically, for instance, with standard boosting algorithms. Concretely, we first show that any sequence of predictors minimizing convex risk over the source distribution will converge to this unique model when the class of predictors is linear (but potentially of infinite dimension). Secondly, we show the same result holds for $\backslash$emph{\{}empirical{\}} risk minimization whenever this class of predictors is finite dimensional, where the essential technical contribution is a norm-free generalization bound.},
archivePrefix = {arXiv},
arxivId = {1506.04513},
author = {Telgarsky, Matus and Dud{\'{i}}k, Miroslav and Schapire, Robert},
eprint = {1506.04513},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Telgarsky, Dud{\'{i}}k, Schapire - 2015 - Convex Risk Minimization and Conditional Probability Estimation.pdf:pdf},
keywords = {classification,conditional probability estimation,consistency,convex duality,maximum entropy,orlicz spaces},
pages = {1--54},
title = {{Convex Risk Minimization and Conditional Probability Estimation}},
url = {http://arxiv.org/abs/1506.04513},
volume = {40},
year = {2015}
}
@article{Cutler1994,
author = {Cutler, Adele and Breiman, Leo},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cutler, Breiman - 1994 - Archetypal Analysis.pdf:pdf},
journal = {Technometrics},
keywords = {archetypes,convex hull,graphics,nonlinear optimization,principal},
number = {4},
pages = {338--347},
title = {{Archetypal Analysis}},
volume = {36},
year = {1994}
}
@article{Giraud-carrier2004,
author = {Giraud-carrier, Christophe and Vilalta, Ricardo and Brazdil, Pavel B.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Giraud-carrier, Vilalta, Brazdil - 2004 - Introduction to the special issue on meta-learning.pdf:pdf},
journal = {Machine learning},
keywords = {dynamic bias selection,inductive bias,meta-knowledge,meta-learning},
pages = {187--193},
title = {{Introduction to the special issue on meta-learning}},
url = {http://link.springer.com/article/10.1023/B:MACH.0000015878.60765.42},
volume = {54},
year = {2004}
}
@article{Huopaniemi2010,
abstract = {MOTIVATION: Analysis of variance (ANOVA)-type methods are the default tool for the analysis of data with multiple covariates. These tools have been generalized to the multivariate analysis of high-throughput biological datasets, where the main challenge is the problem of small sample size and high dimensionality. However, the existing multi-way analysis methods are not designed for the currently increasingly important experiments where data is obtained from multiple sources. Common examples of such settings include integrated analysis of metabolic and gene expression profiles, or metabolic profiles from several tissues in our case, in a controlled multi-way experimental setup where disease status, medical treatment, gender and time-series are usual covariates. RESULTS: We extend the applicability area of multivariate, multi-way ANOVA-type methods to multi-source cases by introducing a novel Bayesian model. The method is capable of finding covariate-related dependencies between the sources. It assumes the measurements consist of groups of similarly behaving variables, and estimates the multivariate covariate effects and their interaction effects for the discovered groups of variables. In particular, the method partitions the effects to those shared between the sources and to source-specific ones. The method is specifically designed for datasets with small sample sizes and high dimensionality. We apply the method to a lipidomics dataset from a lung cancer study with two-way experimental setup, where measurements from several tissues with mostly distinct lipids have been taken. The method is also directly applicable to gene expression and proteomics. AVAILABILITY: An R-implementation is available at http://www.cis.hut.fi/projects/mi/software/multiWayCCA/.},
author = {Huopaniemi, Ilkka and Suvitaival, Tommi and Nikkil{\"{a}}, Janne and Ore{\v{s}}i{\v{c}}, Matej and Kaski, Samuel},
doi = {10.1093/bioinformatics/btq174},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huopaniemi et al. - 2010 - Multivariate multi-way analysis of multi-source data.pdf:pdf},
isbn = {1367-4811 (Electronic)$\backslash$r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {12},
pages = {391--398},
pmid = {20529933},
title = {{Multivariate multi-way analysis of multi-source data}},
volume = {26},
year = {2010}
}
@inproceedings{Rifai2011,
abstract = {We combine three important ideas present in previous work for building classi- fiers: the semi-supervised hypothesis (the input distribution contains information about the classifier), the unsupervised manifold hypothesis (data density concen- trates near low-dimensional manifolds), and the manifold hypothesis for classifi- cation (different classes correspond to disjoint manifolds separated by low den- sity). We exploit a novel algorithm for capturing manifold structure (high-order contractive auto-encoders) and we showhowit builds a topological atlas of charts, each chart being characterized by the principal singular vectors of the Jacobian of a representation mapping. This representation learning algorithm can be stacked to yield a deep architecture, and we combine it with a domain knowledge-free version of the TangentProp algorithm to encourage the classifier to be insensitive to local directions changes along the manifold. Record-breaking classification results are obtained},
author = {Rifai, Salah and Dauphin, Yann N. and Vincent, Pascal and Bengio, Yoshua and {Xavier Muller}},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rifai et al. - 2011 - The Manifold Tangent Classifier.pdf:pdf},
pages = {2294--2302},
title = {{The Manifold Tangent Classifier}},
year = {2011}
}
@article{Zhang2014,
author = {Zhang, Kai and Lan, Liang and Kwok, James T. and Vucetic, Slobodan and Parvin, Bahram},
doi = {10.1109/TNNLS.2014.2315526},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang et al. - 2014 - Scaling Up Graph-Based Semisupervised Learning via Prototype Vector Machines.pdf:pdf},
issn = {2162-237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
pages = {1--1},
title = {{Scaling Up Graph-Based Semisupervised Learning via Prototype Vector Machines}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6803073},
year = {2014}
}
@article{Ojala2010,
abstract = {We explore the framework of permutation-based p-values for assessing the performance of classi- fiers. In this paperwe study two simple permutation tests. The first test assesswhether the classifier has found a real class structure in the data; the corresponding null distribution is estimated by per- muting the labels in the data. This test has been used extensively in classification problems in computational biology. The second test studies whether the classifier is exploiting the dependency between the features in classification; the corresponding null distribution is estimated by permut- ing the features within classes, inspired by restricted randomization techniques traditionally used in statistics. This new test can serve to identify descriptive features which can be valuable infor- mation in improving the classifier performance. We study the properties of these tests and present an extensive empirical evaluation on real and synthetic data. Our analysis shows that studying the classifier performance via permutation tests is effective. In particular, the restricted permutation test clearly reveals whether the classifier exploits the interdependency between the features in the data.},
author = {Ojala, Markus and Garriga, Gemma C},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ojala, Garriga - 2010 - Permutation Tests for Studying Classi er Performance.pdf:pdf},
isbn = {1532-4435},
issn = {1550-4786},
journal = {Journal of Machine Learning Research},
keywords = {classification,labeled data,permutation tests,restricted randomization,significance testing},
pages = {1833--1863},
title = {{Permutation Tests for Studying Classifier Performance}},
volume = {11},
year = {2010}
}
@article{Goodman2016a,
author = {Goodman, Steven N and Fanelli, Daniele and Ioannidis, John P A},
doi = {10.1126/scitranslmed.aaf5027},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Goodman, Fanelli, Ioannidis - 2016 - What does research reproducibility mean.pdf:pdf},
issn = {1946-6234},
journal = {Science Translational Medicine},
number = {341},
pages = {341ps12},
pmid = {27252173},
title = {{What does research reproducibility mean?}},
volume = {8},
year = {2016}
}
@inproceedings{Goldberg2009,
author = {Goldberg, Andrew B. and Zhu, Xiaojin},
booktitle = {Proceedings of the NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural Language Processing},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Goldberg, Zhu - 2009 - Keepin'it real semi-supervised learning with realistic tuning.pdf:pdf},
pages = {19--27},
title = {{Keepin'it real: semi-supervised learning with realistic tuning}},
year = {2009}
}
@article{VanderKooi2016,
author = {van der Kooi, A.L.F. and van den Heuvel-Eibrink, M.M. and van Noortwijk, A. and Neggers, S.J.C.M.M. and Pluijm, S.M.F. and {van Dulmen-den Broeder}, E. and van Dorp, W. and Laven, J.S.E.},
doi = {10.1093/humrep/dew278},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/van der Kooi et al. - 2016 - Longitudinal follow-up in female Childhood Cancer Survivors no signs of accelerated ovarian function loss.pdf:pdf},
issn = {0268-1161},
journal = {Human Reproduction},
keywords = {anti-m{\"{u}}llerian hormone,childhood cancer,females,follow-up studies,ive health,longitudinal studies,neoplasms,ovarian function,reproduct-,survivors},
pages = {1--8},
title = {{Longitudinal follow-up in female Childhood Cancer Survivors: no signs of accelerated ovarian function loss}},
url = {http://humrep.oxfordjournals.org/lookup/doi/10.1093/humrep/dew278},
year = {2016}
}
@article{Cabitza2017,
author = {Cabitza, Federico and Rasoini, Raffaele and Gensini, Gian Franco},
doi = {10.1001/jama.2017.7797},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cabitza{\_}JAMA (2017) Unintended consequences of machine learning and decision support systems in medicine.pdf:pdf},
issn = {0098-7484},
journal = {Jama},
pages = {9--10},
pmid = {28727867},
title = {{Unintended Consequences of Machine Learning in Medicine}},
url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.2017.7797},
year = {2017}
}
@article{Baraniuk2007,
author = {Baraniuk, Richard G.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Baraniuk - 2007 - Compressive sensing.pdf:pdf},
journal = {IEEE Signal Processing Magazine},
number = {July},
pages = {118--121},
title = {{Compressive sensing}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4286571 http://omni.isr.ist.utl.pt/{~}aguiar/CS{\_}notes.pdf},
year = {2007}
}
@inproceedings{Damianou2015,
author = {Damianou, Andreas and Lawrence, Neil D.},
booktitle = {31st Conference on Uncertainty in Artificial Intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Damianou, Lawrence - 2015 - Semi-described and semi-supervised learning with Gaussian processes.pdf:pdf},
title = {{Semi-described and semi-supervised learning with Gaussian processes}},
year = {2015}
}
@article{Kasabov2003,
author = {Kasabov, Nikola and Pang, Shaoning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kasabov, Pang - 2003 - Transductive Support Vector Machines and Applications in Bioinformatics for Promoter Recognition.pdf:pdf},
journal = {Proceedings of the International Conference on Neural networks and signal processing},
keywords = {inductive svm,motif,promoter,promoter recognition,transductive svm},
number = {2},
pages = {31--38},
title = {{Transductive Support Vector Machines and Applications in Bioinformatics for Promoter Recognition}},
volume = {3},
year = {2003}
}
@article{Platanios2014,
author = {Platanios, Emmanouil Antonios and Blum, Avrim and Mitchell, Tom},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Platanios, Blum, Mitchell - 2014 - Estimating Accuracy from Unlabeled Data.pdf:pdf},
isbn = {9780974903910},
journal = {30th Conference on Uncertainty in Artificial Intelligence},
title = {{Estimating Accuracy from Unlabeled Data}},
year = {2014}
}
@article{Chapelle2007,
abstract = {Most literature on support vector machines (SVMs) concentrates on the dual optimization problem. In this letter, we point out that the primal problem can also be solved efficiently for both linear and nonlinear SVMs and that there is no reason for ignoring this possibility. On the contrary, from the primal point of view, new families of algorithms for large-scale SVM training can be investigated.},
author = {Chapelle, Olivier},
doi = {10.1162/neco.2007.19.5.1155},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chapelle - 2007 - Training a support vector machine in the primal.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Models, Theoretical,Neural Networks (Computer),Nonlinear Dynamics,Pattern Recognition, Automated},
month = {may},
number = {5},
pages = {1155--78},
pmid = {17381263},
title = {{Training a support vector machine in the primal.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17381263},
volume = {19},
year = {2007}
}
@article{Hullermeier2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1305.0698v1},
author = {H{\"{u}}llermeier, Eyke},
doi = {10.1016/j.ijar.2013.09.003},
eprint = {arXiv:1305.0698v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/H{\"{u}}llermeier - 2013 - Learning from imprecise and fuzzy observations Data disambiguation through generalized loss minimization.pdf:pdf},
issn = {0888613X},
journal = {International Journal of Approximate Reasoning},
keywords = {data disambiguation,extension principle,fuzzy sets,imprecise data,inductive bias,logistic,loss function,machine learning,risk minimization},
pages = {1--16},
title = {{Learning from imprecise and fuzzy observations: Data disambiguation through generalized loss minimization}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0888613X13001722},
volume = {1},
year = {2013}
}
@inproceedings{Gretton2006,
author = {Gretton, Arthur and Sch{\"{o}}lkopf, Bernhard},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gretton, Sch{\"{o}}lkopf - 2006 - A Kernel Method for the Two-Sample-Problem.pdf:pdf},
pages = {513--520},
title = {{A Kernel Method for the Two-Sample-Problem}},
year = {2006}
}
@inproceedings{Rasmus2015,
archivePrefix = {arXiv},
arxivId = {1507.02672},
author = {Rasmus, Antti and Valpola, Harri and Honkala, Mikko and Berglund, Mathias and Raiko, Tapani},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1507.02672},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rasmus et al. - 2015 - Semi-supervised learning with Ladder Networks.pdf:pdf},
pages = {3546--3554},
title = {{Semi-supervised learning with Ladder Networks}},
year = {2015}
}
@article{Mann2010,
author = {Mann, Gideon S. and McCallum, Andrew Kachites},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mann, McCallum - 2010 - Generalized expectation criteria for semi-supervised learning with weakly labeled data.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {condi-,generalized expectation criteria,logistic regression,semi-supervised learning,tional random fields},
pages = {955--984},
title = {{Generalized expectation criteria for semi-supervised learning with weakly labeled data}},
volume = {11},
year = {2010}
}
@article{Marshall1960,
author = {Marshall, Albert W. and Olkin, Ingram},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Marshall, Olkin - 1960 - Multivariate Chebyshev Inequalities.pdf:pdf},
journal = {Annals of Mathematical Statistics},
pages = {1001--1014},
title = {{Multivariate Chebyshev Inequalities}},
volume = {31},
year = {1960}
}
@inproceedings{Plessis2012,
author = {du Plessis, Marthinus Christoffel and Sugiyama, Masashi},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Plessis, Sugiyama - 2012 - Semi-supervised learning of class balance under class-prior change by distribution matching.pdf:pdf},
title = {{Semi-supervised learning of class balance under class-prior change by distribution matching}},
url = {http://arxiv.org/abs/1206.4677},
year = {2012}
}
@article{Senn2016,
abstract = {Various sources of variation in observed response in clinical trials and clinical practice are considered, and ways in which the corresponding components of variation might be estimated are discussed. Although the issues have been generally well-covered in the statistical literature, they seem to be poorly understood in the medical literature and even the statistical literature occasionally shows some confusion. To increase understanding and communication, some simple graphical approaches to illustrating issues are proposed. It is also suggested that reducing variation in medical practice might make as big a contribution to improving health outcome as personalising its delivery according to the patient. It is concluded that the common belief that there is a strong personal element in response to treatment is not based on sound statistical evidence.},
author = {Senn, Stephen},
doi = {10.1002/sim.6739},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Senn-2016-Statistics{\_}in{\_}Medicine.pdf:pdf},
isbn = {1097-0258 (Electronic)$\backslash$r0277-6715 (Linking)},
issn = {10970258},
journal = {Statistics in Medicine},
keywords = {Components of variation,Cross-over trials,N-of-1 trials,Personalised medicine,Random effects},
number = {7},
pages = {966--977},
pmid = {26415869},
title = {{Mastering variation: Variance components and personalised medicine}},
volume = {35},
year = {2016}
}
@inproceedings{Cozman2003,
author = {Cozman, Fabio Gagliardi and Cohen, Ira and Cirelo, Marcelo Cesar},
booktitle = {Proceedings of the 20th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cozman, Cohen, Cirelo - 2003 - Semi-Supervised Learning of Mixture Models.pdf:pdf},
pages = {99--106},
title = {{Semi-Supervised Learning of Mixture Models}},
year = {2003}
}
@article{Greenland2017,
author = {Greenland, Sander},
doi = {10.1007/s10654-017-0230-6},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/art{\%}3A10.1007{\%}2Fs10654-017-0230-6.pdf:pdf},
issn = {0393-2990},
journal = {European Journal of Epidemiology},
keywords = {Bias,Causal inference,Causation,Counterfactuals,Po,analysis {\'{a}} modeling {\'{a}},bias {\'{a}} causal inference,counterfactuals {\'{a}} potential outcomes,hypothesis testing {\'{a}} intervention,significance testing {\'{a}} research,synthesis {\'{a}} statistical,{\'{a}} causation {\'{a}},{\'{a}} effect estimation {\'{a}}},
number = {1},
pages = {1--18},
publisher = {Springer Netherlands},
title = {{For and Against Methodologies: Some Perspectives on Recent Causal and Statistical Inference Debates}},
url = {http://link.springer.com/10.1007/s10654-017-0230-6},
volume = {32},
year = {2017}
}
@article{Todorovski2003,
author = {Todorovski, Ljupco and D{\v{z}}eroski, Saso},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Todorovski, D{\v{z}}eroski - 2003 - Combining classifiers with meta decision trees.pdf:pdf},
journal = {Machine learning},
keywords = {combining classifiers,decision trees,ensembles of classifiers,meta-level learning,stacking},
pages = {223--249},
title = {{Combining classifiers with meta decision trees}},
url = {http://link.springer.com/article/10.1023/A:1021709817809},
year = {2003}
}
@article{Yang2015,
author = {Yang, Yun and Liu, Xingchen},
doi = {10.1016/j.patrec.2015.08.009},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yang, Liu - 2015 - A robust semi-supervised learning approach via mixture of label information.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Clustering,semi-supervised learning},
publisher = {Elsevier Ltd.},
title = {{A robust semi-supervised learning approach via mixture of label information}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865515002688},
year = {2015}
}
@book{Barber2012,
author = {Barber, David},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Barber - 2012 - Bayesian reasoning and machine learning.pdf:pdf},
title = {{Bayesian reasoning and machine learning}},
year = {2012}
}
@article{Schuurmans2002,
author = {Schuurmans, Dale and Southey, Finnegan},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schuurmans, Southey - 2002 - Metric-Based Methods for Adaptive Model Selection and Regularization.pdf:pdf},
journal = {Machine Learning},
keywords = {model selection,regularization,unlabeled examples},
pages = {51--84},
title = {{Metric-Based Methods for Adaptive Model Selection and Regularization}},
volume = {48},
year = {2002}
}
@article{Wang2009a,
author = {Wang, Fei and Wang, Xin and Li, Tao},
doi = {10.1109/CVPR.2009.5206675},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Wang, Li - 2009 - Beyond the graphs Semi-parametric semi-supervised discriminant analysis.pdf:pdf},
isbn = {978-1-4244-3992-8},
journal = {IEEE Conference on Computer Vision and Pattern Recognition},
month = {jun},
pages = {2113--2120},
publisher = {Ieee},
title = {{Beyond the graphs: Semi-parametric semi-supervised discriminant analysis}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5206675},
year = {2009}
}
@article{Wang2009b,
author = {Wang, Junhui and Shen, Xiaotong and Pan, Wei},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Shen, Pan - 2009 - On efficient large margin semisupervised learning Method and theory.pdf:pdf},
journal = {The Journal of Machine Learning Research},
keywords = {classification,difference convex programming,nonconvex minimization,regulariza-,support vectors,tion},
pages = {719--742},
title = {{On efficient large margin semisupervised learning: Method and theory}},
url = {http://dl.acm.org/citation.cfm?id=1577094},
volume = {10},
year = {2009}
}
@inproceedings{Li2011,
author = {Li, Yu-Feng and Zhou, Zhi-hua},
booktitle = {Proceedings of the 28th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li, Zhou - 2011 - Towards Making Unlabeled Data Never Hurt.pdf:pdf},
pages = {1081--1088},
title = {{Towards Making Unlabeled Data Never Hurt}},
year = {2011}
}
@article{Alaa2017a,
abstract = {We propose a novel approach for inferring the individualized causal effects of a treatment (intervention) from observational data. Our approach conceptualizes causal inference as a multitask learning problem; we model a subject's potential outcomes using a deep multitask network with a set of shared layers among the factual and counterfactual outcomes, and a set of outcome-specific layers. The impact of selection bias in the observational data is alleviated via a propensity-dropout regularization scheme, in which the network is thinned for every training example via a dropout probability that depends on the associated propensity score. The network is trained in alternating phases, where in each phase we use the training examples of one of the two potential outcomes (treated and control populations) to update the weights of the shared layers and the respective outcome-specific layers. Experiments conducted on data based on a real-world observational study show that our algorithm outperforms the state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1706.05966},
author = {Alaa, Ahmed M. and Weisz, Michael and van der Schaar, Mihaela},
eprint = {1706.05966},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/ICML{\_}Deep{\_}CI.pdf:pdf},
keywords = {()},
title = {{Deep Counterfactual Networks with Propensity-Dropout}},
url = {http://arxiv.org/abs/1706.05966},
year = {2017}
}
@inproceedings{Narasimhan,
author = {Narasimhan, Mukund and Jojic, Nebojsa and Bilmes, Jeff},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Narasimhan, Jojic, Bilmes - 2005 - Q-Clustering.pdf:pdf},
pages = {979--986},
title = {{Q-Clustering}},
year = {2005}
}
@article{Hartley1968,
author = {Hartley, H.O. and Rao, J.N.K.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hartley, Rao - 1968 - A new estimation for sample theory surveys.pdf:pdf},
journal = {Biometrika},
number = {3},
pages = {547--557},
title = {{A new estimation for sample theory surveys}},
volume = {55},
year = {1968}
}
@article{Caticha2011,
author = {Caticha, Ariel and Mohammad-Djafari, Ali and Bercher, Jean-François and Bessiére, Pierre},
doi = {10.1063/1.3573619},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Caticha et al. - 2011 - Entropic Inference.pdf:pdf},
isbn = {9780735408609},
keywords = {bayes rule,entropy,information,maximum entropy},
number = {1},
pages = {20--29},
title = {{Entropic Inference}},
url = {http://link.aip.org/link/APCPCS/v1305/i1/p20/s1{\&}Agg=doi},
volume = {20},
year = {2011}
}
@article{Vapnik2009,
author = {Vapnik, Vladimir and Vashist, Akshay},
doi = {10.1016/j.neunet.2009.06.042},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vapnik, Vashist - 2009 - A new learning paradigm Learning using privileged information.pdf:pdf},
journal = {Neural Networks},
number = {5-6},
pages = {544--557},
publisher = {Elsevier Ltd},
title = {{A new learning paradigm : Learning using privileged information}},
volume = {22},
year = {2009}
}
@article{Preece1971,
author = {Preece, D.A.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Preece - 1971 - Iterative Procedures for Missing Values in Experiments.pdf:pdf},
journal = {Technometrics},
number = {4},
pages = {743--753},
title = {{Iterative Procedures for Missing Values in Experiments}},
volume = {13},
year = {1971}
}
@article{Duchi2016,
abstract = {We provide a unifying view of statistical information measures, multi-class classification problems, multi-way Bayesian hypothesis testing, and loss functions, elaborating equivalence results between all of these objects. In particular, we consider a particular generalization of {\$}f{\$}-divergences to multiple distributions, and we show that there is a constructive equivalence between {\$}f{\$}-divergences, statistical information (in the sense of uncertainty as elaborated by DeGroot), and loss functions for multi-category classification. We also study an extension of our results to multi-class classification problems in which we must both infer a discriminant function {\$}\backslashgamma{\$} and a data representation (or, in the setting of a hypothesis testing problem, an experimental design), represented by a quantizer {\$}\backslashmathsf{\{}q{\}}{\$} from a family of possible quantizers {\$}\backslashmathsf{\{}Q{\}}{\$}. There, we give a complete characterization of the equivalence between loss functions, meaning that optimizing either of two losses yields the same optimal discriminant and quantizer {\$}\backslashmathsf{\{}q{\}}{\$}. A main consequence of our results is to describe those convex loss functions that are Fisher consistent for jointly choosing a data representation and minimizing the (weighted) probability of error in multi-category classification and hypothesis testing problems.},
archivePrefix = {arXiv},
arxivId = {1603.00126},
author = {Duchi, John C. and Khosravi, Khashayar and Ruan, Feng},
eprint = {1603.00126},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Duchi, Khosravi, Ruan - 2016 - Information Measures, Experiments, Multi-category Hypothesis Tests, and Surrogate Losses.pdf:pdf},
title = {{Information Measures, Experiments, Multi-category Hypothesis Tests, and Surrogate Losses}},
url = {http://arxiv.org/abs/1603.00126},
year = {2016}
}
@inproceedings{Chapelle2005,
abstract = {We believe that the cluster assumption is key to successful semi-supervised learning. Based on this, we propose three semi-supervised algorithms: 1. deriving graph-based distances that emphazise low density regions between clusters, followed by training a standard SVM; 2. optimizing the Transductive SVM objective function, which places the decision boundary in low density regions, by gradient descent; 3. combining the first two to make maximum use of the cluster assumption. We compare with state of the art algorithms and demonstrate superior accuracy for the latter two methods.},
author = {Chapelle, Olivier and Zien, Alexander},
booktitle = {AISTATS},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chapelle, Zien - 2005 - Semi-Supervised Classification by Low Density Separation.pdf:pdf},
keywords = {learning,statistics {\&} optimisation,theory {\&} algorithms},
pages = {57--64},
title = {{Semi-Supervised Classification by Low Density Separation}},
year = {2005}
}
@article{Nock2009,
abstract = {Bartlett et al. (2006) recently proved that a ground condition for surrogates, classification calibration, ties up their consistent minimization to that of the classification risk, and left as an important problem the algorithmic questions about their minimization. In this paper, we address this problem for a wide set which lies at the intersection of classification calibrated surrogates and those of Murata et al. (2004). This set coincides with those satisfying three common assumptions about surrogates. Equivalent expressions for the members-sometimes well known-follow for convex and concave surrogates, frequently used in the induction of linear separators and decision trees. Most notably, they share remarkable algorithmic features: for each of these two types of classifiers, we give a minimization algorithm provably converging to the minimum of any such surrogate. While seemingly different, we show that these algorithms are offshoots of the same "master" algorithm. This provides a new and broad unified account of different popular algorithms, including additive regression with the squared loss, the logistic loss, and the top-down induction performed in CART, C4.5. Moreover, we show that the induction enjoys the most popular boosting features, regardless of the surrogate. Experiments are provided on 40 readily available domains.},
author = {Nock, Richard and Nielsen, Frank},
doi = {10.1109/TPAMI.2008.225},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nock, Nielsen - 2009 - Bregman divergences and surrogates for learning.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Computer Simulation,Decision Support Techniques,Models, Theoretical,Pattern Recognition, Automated,Pattern Recognition, Automated: methods},
month = {nov},
number = {11},
pages = {2048--59},
pmid = {19762930},
title = {{Bregman divergences and surrogates for learning.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19762930},
volume = {31},
year = {2009}
}
@article{Webb1996,
author = {Webb, Geoffrey I.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Webb - 1996 - Further Experimental Evidence against the Utility of Occam's Razor.pdf:pdf},
journal = {Journal of Artificial Intelligence Research},
pages = {397--417},
title = {{Further Experimental Evidence against the Utility of Occam's Razor}},
url = {http://arxiv.org/abs/cs/9605101},
volume = {4},
year = {1996}
}
@article{Pribram1978a,
author = {Afifi, A.A. and Elashoff, R.M.},
doi = {10.1017/S0140525X00060003},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Afifi, Elashoff - 1967 - Missing Observations in Multivariate Statistics II Point Estimation in Simple Linear Regression.pdf:pdf},
isbn = {0300104251},
issn = {0140-525X},
journal = {Journal of the American Statistical Association},
number = {317},
pages = {10--29},
title = {{Missing Observations in Multivariate Statistics II: Point Estimation in Simple Linear Regression}},
volume = {62},
year = {1967}
}
@article{Balsubramani,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.01811v1},
author = {Balsubramani, Akshay and Freund, Yoav},
eprint = {arXiv:1503.01811v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balsubramani, Freund - Unknown - Optimally Combining Classifiers Using Unlabeled Data.pdf:pdf},
title = {{Optimally Combining Classifiers Using Unlabeled Data}}
}
@incollection{Steinwart2013,
author = {Steinwart, Ingo},
booktitle = {Empirical Inference},
chapter = {4},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Steinwart - 2013 - Some Remarks on the Statistical Analysis of SVMs and Related Methods.pdf:pdf},
pages = {25--36},
title = {{Some Remarks on the Statistical Analysis of SVMs and Related Methods}},
year = {2013}
}
@article{Varma2006,
abstract = {Cross-validation (CV) is an effective method for estimating the prediction error of a classifier. Some recent articles have proposed methods for optimizing classifiers by choosing classifier parameter values that minimize the CV error estimate. We have evaluated the validity of using the CV error estimate of the optimized classifier as an estimate of the true error expected on independent data.},
author = {Varma, Sudhir and Simon, Richard},
doi = {10.1186/1471-2105-7-91},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Varma, Simon - 2006 - Bias in error estimation when using cross-validation for model selection.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Artificial Intelligence,Bias (Epidemiology),Computer Simulation,Data Interpretation, Statistical,Gene Expression Profiling,Gene Expression Profiling: methods,Models, Genetic,Models, Statistical,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Reproducibility of Results,Sensitivity and Specificity},
month = {jan},
pages = {91},
pmid = {16504092},
title = {{Bias in error estimation when using cross-validation for model selection.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1397873{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {7},
year = {2006}
}
@article{Meding2012,
abstract = {In clinical diagnostics, it is of outmost importance to correctly identify the source of a metastatic tumor, especially if no apparent primary tumor is present. Tissue-based proteomics might allow correct tumor classification. As a result, we performed MALDI imaging to generate proteomic signatures for different tumors. These signatures were used to classify common cancer types. At first, a cohort comprised of tissue samples from six adenocarcinoma entities located at different organ sites (esophagus, breast, colon, liver, stomach, thyroid gland, n = 171) was classified using two algorithms for a training and test set. For the test set, Support Vector Machine and Random Forest yielded overall accuracies of 82.74 and 81.18{\%}, respectively. Then, colon cancer liver metastasis samples (n = 19) were introduced into the classification. The liver metastasis samples could be discriminated with high accuracy from primary tumors of colon cancer and hepatocellular carcinoma. Additionally, colon cancer liver metastasis samples could be successfully classified by using colon cancer primary tumor samples for the training of the classifier. These findings demonstrate that MALDI imaging-derived proteomic classifiers can discriminate between different tumor types at different organ sites and in the same site.},
author = {Meding, Stephan and Nitsche, Ulrich and Balluff, Benjamin and Elsner, Mareike and Rauser, Sandra and Sch{\"{o}}ne, C{\'{e}}drik and Nipp, Martin and Maak, Matthias and Feith, Marcus and Ebert, Matthias P and Friess, Helmut and Langer, Rupert and H{\"{o}}fler, Heinz and Zitzelsberger, Horst and Rosenberg, Robert and Walch, Axel},
doi = {10.1021/pr200784p},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Meding et al. - 2012 - Tumor classification of six common cancer types based on proteomic profiling by MALDI imaging.pdf:pdf},
issn = {1535-3907},
journal = {Journal of proteome research},
keywords = {Adenocarcinoma,Adenocarcinoma: metabolism,Adenocarcinoma: secondary,Algorithms,Humans,Neoplasms,Neoplasms: diagnosis,Neoplasms: metabolism,Neoplasms: pathology,Proteome,Proteome: metabolism,Proteomics,Sensitivity and Specificity,Spectrometry, Mass, Matrix-Assisted Laser Desorpti,Support Vector Machines},
month = {mar},
number = {3},
pages = {1996--2003},
pmid = {22224404},
title = {{Tumor classification of six common cancer types based on proteomic profiling by MALDI imaging.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22224404},
volume = {11},
year = {2012}
}
@inproceedings{Ho2001a,
author = {Ho, Tin Kam},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho - 2001 - Data Complexity Analysis for Classifier Combination.pdf:pdf},
pages = {53--67},
title = {{Data Complexity Analysis for Classifier Combination}},
year = {2001}
}
@inproceedings{Krijthe2016b,
author = {Krijthe, Jesse Hendrik and Loog, Marco},
booktitle = {Structural, Syntactic, and Statistical Pattern Recognition. S+SSPR 2016. Lecture Notes in Computer Science, vol 10029.},
doi = {10.1007/978-3-319-49055-7_27},
editor = {Robles-Kelly, A. and Loog, M. and Biggio, B. and Escolano, F. and Wilson, R.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Krijthe, Loog - 2016 - The Peaking Phenomenon in Semi-supervised Learning.pdf:pdf},
keywords = {least squares classifier,peaking,pseudo-,semi-supervised learning},
publisher = {Springer},
title = {{The Peaking Phenomenon in Semi-supervised Learning}},
year = {2016}
}
@article{Nigam2000,
author = {Nigam, Kamal and McCallum, Andrew K. and Thrun, Sebastian and Mitchell, Tom},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nigam et al. - 2000 - Text classification from labeled and unlabeled documents using EM.pdf:pdf},
journal = {Machine learning},
keywords = {bayesian learning,combining labeled and unlabeled,data,expectation-maximization,integrating supervised and unsuper-,text classification,vised learning},
pages = {1--34},
title = {{Text classification from labeled and unlabeled documents using EM}},
volume = {34},
year = {2000}
}
@article{Lehmann1993,
author = {Lehmann, EL},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lehmann - 1993 - The Fisher, Neyman-Pearson theories of testing hypotheses One theory or two.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {424},
pages = {1242--1249},
title = {{The Fisher, Neyman-Pearson theories of testing hypotheses: One theory or two?}},
url = {http://www.jstor.org/stable/10.2307/2291263},
volume = {88},
year = {1993}
}
@article{Lv2014,
author = {Lv, Jinchi and Zheng, Zemin},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lv, Zheng - 2014 - Discussion “a significance test for the lasso”.pdf:pdf},
journal = {The Annals of Statistics},
number = {2},
pages = {493--500},
title = {{Discussion: “a significance test for the lasso”}},
volume = {42},
year = {2014}
}
@article{Rosasco2004,
abstract = {In this letter, we investigate the impact of choosing different loss functions from the viewpoint of statistical learning theory. We introduce a convexity assumption, which is met by all loss functions commonly used in the literature, and study how the bound on the estimation error changes with the loss. We also derive a general result on the minimizer of the expected risk for a convex loss function in the case of classification. The main outcome of our analysis is that for classification, the hinge loss appears to be the loss of choice. Other things being equal, the hinge loss leads to a convergence rate practically indistinguishable from the logistic loss rate and much better than the square loss rate. Furthermore, if the hypothesis space is sufficiently rich, the bounds obtained for the hinge loss are not loosened by the thresholding stage.},
author = {Rosasco, Lorenzo and {De Vito}, Ernesto and Caponnetto, Andrea and Piana, Michele and Verri, Alessandro},
doi = {10.1162/089976604773135104},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rosasco et al. - 2004 - Are loss functions all the same.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Learning,Learning: physiology,Linear Models,Models, Neurological,Statistics as Topic},
month = {may},
number = {5},
pages = {1063--76},
pmid = {15070510},
title = {{Are loss functions all the same?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15070510},
volume = {16},
year = {2004}
}
@book{Aubin2000,
author = {Aubin, Jean-Pierre},
edition = {2},
isbn = {9780471179764},
publisher = {John Wiley {\&} Sons},
title = {{Applied functional analysis}},
year = {2000}
}
@article{Jain1999a,
author = {Jain, A.K. and Murty, M.N. and Flynn, P.J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jain, Murty, Flynn - 1999 - Data clustering a review.pdf:pdf},
journal = {ACM computing surveys (CSUR)},
number = {3},
title = {{Data clustering: a review}},
url = {http://dl.acm.org/citation.cfm?id=331504},
volume = {31},
year = {1999}
}
@article{Mealli2015,
author = {Mealli, Fabrizia and Rubin, Donald B.},
doi = {10.1093/biomet/asv035},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mealli, Rubin - 2015 - Clarifying missing at random and related definitions, and implications when coupled with exchangeability.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
number = {4},
pages = {995--1000},
title = {{Clarifying missing at random and related definitions, and implications when coupled with exchangeability}},
url = {http://biomet.oxfordjournals.org/lookup/doi/10.1093/biomet/asv035},
volume = {102},
year = {2015}
}
@article{Lei2015,
author = {Lei, Zhikun and Li, Renfu and {Sherry Ni}, Xuelei and Huo, Xiaoming},
doi = {10.1016/j.sigpro.2015.03.003},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lei et al. - 2015 - High-dimensional semi-supervised learning via a fusion-refinement procedure.pdf:pdf},
issn = {01651684},
journal = {Signal Processing},
keywords = {Fusion-refinement (FR) procedure,Semi-supervised learning (SSL),Sufficient dimension reduction (SDR),fr,fusion-refinement,procedure,sdr,semi-supervised learning,ssl,sufficient dimension reduction},
pages = {171--182},
publisher = {Elsevier},
title = {{High-dimensional semi-supervised learning via a fusion-refinement procedure}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0165168415000961},
volume = {114},
year = {2015}
}
@misc{Shannon1956,
author = {Shannon, E},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shannon - 1956 - The Bandwagon.pdf:pdf},
title = {{The Bandwagon}},
year = {1956}
}
@article{Kleinberg1996,
author = {Kleinberg, E.M.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kleinberg - 1996 - An Overtraining-Resistant Stochastic Modeling Method for Pattern Recognition.pdf:pdf},
number = {6},
pages = {2319--2349},
title = {{An Overtraining-Resistant Stochastic Modeling Method for Pattern Recognition}},
volume = {24},
year = {1996}
}
@article{Chang2011,
author = {Chang, Chih-Chung and Lin, Chih-Jen},
journal = {ACM Transactions on Intelligent Systems and Technology},
number = {3},
pages = {27},
publisher = {ACM},
title = {{LIBSVM: a library for support vector machines}},
volume = {2},
year = {2011}
}
@book{BDA2013,
author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
edition = {3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman et al. - 2013 - Bayesian Data Analysis.pdf:pdf},
publisher = {CRC Press},
title = {{Bayesian Data Analysis}},
year = {2013}
}
@article{Darnstadt2013,
author = {Darnst{\"{a}}dt, Malte and Simon, HU and Sz{\"{o}}r{\'{e}}nyi, Bal{\'{a}}zs},
doi = {10.4230/LIPIcs.STACS.2013.185},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/unlabeled{\_}data{\_}2013.pdf:pdf},
isbn = {9783939897507},
issn = {18688969},
journal = {Stacs},
keywords = {185,2013,4230,and phrases algorithmic learning,digital object identifier 10,lipics,sample complexity,semi-supervised learning,stacs},
pages = {185--196},
title = {{Unlabeled Data Does Provably Help}},
volume = {i},
year = {2013}
}
@article{Markatou2005,
author = {Markatou, Marianthi and Tian, H},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Markatou, Tian - 2005 - Analysis of variance of cross-validation estimators of the generalization error.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine {\ldots}},
keywords = {cross-validation,generalization error,moment approximation,prediction,variance},
pages = {1127--1168},
title = {{Analysis of variance of cross-validation estimators of the generalization error}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/MarkatouTBH05.pdf},
volume = {6},
year = {2005}
}
@book{Rothenberg1973,
author = {Rothenberg, Thomas J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rothenberg - 1973 - Efficient Estimation with A Priori Information.pdf:pdf},
publisher = {Yale University Press},
title = {{Efficient Estimation with A Priori Information}},
url = {http://www.getcited.org/pub/101421013},
year = {1973}
}
@inproceedings{Steck2003,
author = {Steck, Harald and Jaakkola, Tommi S.},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Steck, Jaakkola - 2003 - Bias-corrected bootstrap and model uncertainty.pdf:pdf},
title = {{Bias-corrected bootstrap and model uncertainty}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2003{\_}AA66.pdf},
year = {2003}
}
@article{Sion1958,
author = {Sion, Maurice},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sion - 1958 - On general minimax theorems.pdf:pdf},
journal = {Pacific J. Math},
number = {1},
pages = {171--176},
title = {{On general minimax theorems}},
volume = {8},
year = {1958}
}
@article{Breiman2001,
author = {Breiman, Leo},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Breiman - 2001 - Statistical Modeling The Two Cultures.pdf:pdf},
journal = {Statistical Science},
number = {3},
pages = {199--231},
title = {{Statistical Modeling: The Two Cultures}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Statistical+Modeling+:+The+Two+Cultures{\#}2 http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Statistical+Modeling:+The+Two+Cultures{\#}2},
volume = {16},
year = {2001}
}
@article{Subramanya2011a,
author = {Subramanya, A and Bilmes, Jeff},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Subramanya, Bilmes - 2011 - Semi-supervised learning with measure propagation.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {graph-based semi-supervised learning,large-scale semi-supervised,learning,non-parametric models,transductive inference},
pages = {3311--3370},
title = {{Semi-supervised learning with measure propagation}},
url = {http://dl.acm.org/citation.cfm?id=2078212},
volume = {12},
year = {2011}
}
@article{Neal1998,
abstract = {The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Neal, R.M. and Hinton, G.E.},
doi = {10.1007/978-94-011-5014-9_12},
eprint = {arXiv:1011.1669v3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Neal, Hinton - 1998 - A view of the EM algorithm that justifies incremental, sparse, and other variants.pdf:pdf},
isbn = {0262600323},
issn = {978-1-932432-41-1},
journal = {Learning in Graphical Models},
pages = {355--368},
pmid = {15991970},
title = {{A view of the EM algorithm that justifies incremental, sparse, and other variants}},
volume = {89},
year = {1998}
}
@article{Bouckaert2004,
abstract = {Empirical research in learning algorithms for classification tasks gen- erally requires the use of significance tests.The quality of a test is typically judged on Type I error (how often the test indicates a difference when it should not) and Type II error (how often it indicates no difference when it should). In this paper we argue that the replicability of a test is also of importance.We say that a test has low replicability if its outcome strongly depends on the particular random parti- tioning of the data that is used to perform it. We present empirical measures of replicability and use them to compare the performance of several popular tests in a realistic setting involving standard learning algorithms and benchmark datasets. Based on our results we give recommendations on which test to use.},
author = {Bouckaert, Remco R and Frank, Eibe},
doi = {10.1007/978-3-540-24775-3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bouckaert, Frank - 2004 - Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms.pdf:pdf},
isbn = {3-540-22064-X},
issn = {0302-9743},
journal = {Advances in knowledge discovery and data mining},
pages = {3--12},
title = {{Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms}},
year = {2004}
}
@inbook{Krijthe2016,
archivePrefix = {arXiv},
arxivId = {1610.05160},
author = {Krijthe, Jesse H and Loog, Marco},
booktitle = {Structural, Syntactic, and Statistical Pattern Recognition. S+SSPR 2016. Lecture Notes in Computer Science, vol 10029.},
doi = {10.1007/978-3-319-49055-7_27},
editor = {Robles-Kelly, Antonio and Loog, Marco and Biggio, Battista and Escolano, Francisco and Wilson, Richard},
eprint = {1610.05160},
isbn = {978-3-319-49055-7},
pages = {299--309},
publisher = {Springer International Publishing},
title = {{The Peaking Phenomenon in Semi-supervised Learning}},
url = {http://dx.doi.org/10.1007/978-3-319-49055-7{\_}27},
year = {2016}
}
@article{Ding2015,
author = {Ding, Shifei and Zhu, Zhibin and Zhang, Xiekai},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ding, Zhu, Zhang - 2015 - An overview on semi-supervised support vector machine.pdf:pdf},
journal = {Neural Computing and Applications},
title = {{An overview on semi-supervised support vector machine}},
year = {2015}
}
@inproceedings{Krijthe2012b,
author = {Krijthe, Jesse Hendrik and Ho, Tin Kam and Loog, Marco},
booktitle = {Proceedings of the 21st International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Krijthe, Ho, Loog - 2012 - Improving cross-validation based classifier selection using meta-learning.pdf:pdf},
pages = {2873--2876},
title = {{Improving cross-validation based classifier selection using meta-learning}},
year = {2012}
}
@article{Liang2007,
archivePrefix = {arXiv},
arxivId = {arXiv:0710.4618v1},
author = {Liang, Feng and Mukherjee, Sayan and West, Mike},
doi = {10.1214/088342307000000032},
eprint = {arXiv:0710.4618v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Liang, Mukherjee, West - 2007 - The Use of Unlabeled Data in Predictive Modeling.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {and phrases,bayesian analysis,bayesian kernel regression,latent factor models,mixture models,pervised learning,predictive distribution,semisu-,unlabeled data},
month = {may},
number = {2},
pages = {189--205},
title = {{The Use of Unlabeled Data in Predictive Modeling}},
url = {http://projecteuclid.org/euclid.ss/1190905518},
volume = {22},
year = {2007}
}
@article{Betancourt2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.01510v1},
author = {Betancourt, Michael},
eprint = {arXiv:1502.01510v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Betancourt - 2015 - The Fundamental Incompatibility of Hamiltonian Monte Carlo and Data Subsampling.pdf:pdf},
journal = {Arxiv preprint},
title = {{The Fundamental Incompatibility of Hamiltonian Monte Carlo and Data Subsampling}},
year = {2015}
}
@article{VonHippel2007,
author = {von Hippel, Paul T.},
doi = {10.1111/j.1467-9531.2007.00180.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/von Hippel - 2007 - Regression With Missing Ys an Improved Strategy for Analyzing Multiply Imputed Data.pdf:pdf},
issn = {0081-1750},
journal = {Sociological Methodology},
month = {dec},
number = {1},
pages = {83--117},
title = {{Regression With Missing Ys: an Improved Strategy for Analyzing Multiply Imputed Data}},
url = {http://smx.sagepub.com/lookup/doi/10.1111/j.1467-9531.2007.00180.x},
volume = {37},
year = {2007}
}
@inproceedings{Miguel2014,
author = {Carreira-Perpinan, Miguel A. and Wang, Weiran},
booktitle = {AISTATS},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Carreira-Perpinan, Wang - 2014 - Distributed Optimization of Deeply Nested Systems.pdf:pdf},
pages = {10--19},
title = {{Distributed Optimization of Deeply Nested Systems}},
year = {2014}
}
@article{Amini2002,
author = {Amini, Massih-Reza and Gallinari, Patrick},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Amini, Gallinari - 2002 - Semi-supervised logistic regression.pdf:pdf},
journal = {15th European Conference on Artificial Intelligence},
pages = {390--394},
title = {{Semi-supervised logistic regression}},
year = {2002}
}
@article{Xu2009,
address = {New York, New York, USA},
author = {Xu, Linli and White, Martha and Schuurmans, Dale},
doi = {10.1145/1553374.1553519},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Xu, White, Schuurmans - 2009 - Optimal reverse prediction.pdf:pdf},
isbn = {9781605585161},
journal = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09},
pages = {1--8},
publisher = {ACM Press},
title = {{Optimal reverse prediction}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553519},
year = {2009}
}
@article{Wang2013,
author = {Wang, Jun and Jebara, Tony and Chang, Shih-Fu},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Jebara, Chang - 2013 - Semi-Supervised Learning Using Greedy Max-Cut.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {771--800},
title = {{Semi-Supervised Learning Using Greedy Max-Cut}},
url = {http://www.ee.columbia.edu/ln/dvmm/publications/13/ggmc{\_}13.pdf},
volume = {14},
year = {2013}
}
@inproceedings{Macia2009,
author = {Macia, Nuria and Orriols-puig, Albert and Bernad{\'{o}}-Mansilla, Ester},
booktitle = {Hybrid Artificial Intelligence Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Macia, Orriols-puig, Bernad{\'{o}}-Mansilla - 2009 - Beyond Homemade Artificial Data Sets.pdf:pdf},
keywords = {artificial data sets,data complexity,machine learning},
pages = {605--612},
title = {{Beyond Homemade Artificial Data Sets}},
url = {http://www.springerlink.com/index/N23720WL67U355MV.pdf http://link.springer.com/chapter/10.1007/978-3-642-02319-4{\_}73},
year = {2009}
}
@incollection{Vickers2016,
author = {Vickers, John},
booktitle = {The Stanford Encyclopedia of Philosophy},
edition = {Spring 201},
editor = {Zalta, Edward N},
howpublished = {$\backslash$url{\{}http://plato.stanford.edu/archives/spr2016/entries/induction-problem/{\}}},
title = {{The Problem of Induction}},
year = {2016}
}
@inproceedings{Duin2002,
author = {Pekalska, Ella and Duin, Robert P.W. and Skurichina, Marina},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pekalska, Duin, Skurichina - 2002 - A discussion on the classifier projection space for classifier combining.pdf:pdf},
pages = {137--148},
title = {{A discussion on the classifier projection space for classifier combining}},
url = {http://www.springerlink.com/index/A98FBKT93AK0YNNE.pdf},
year = {2002}
}
@book{Wickham2015,
author = {Wickham, Hadley},
publisher = {O'Reilly Media},
title = {{R packages}},
year = {2015}
}
@inproceedings{Cortes2010,
author = {Cortes, Corinna and Mansour, Yishay and Mohri, Mehryar},
booktitle = {Advances in Neural Information Processing Systems 23},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cortes, Mansour, Mohri - 2010 - Learning bounds for importance weighting.pdf:pdf},
pages = {442--450},
title = {{Learning bounds for importance weighting}},
url = {http://www.cs.nyu.edu/{~}mohri/pub/importance.pdf},
year = {2010}
}
@article{Efron,
abstract = {In the absence of relevant prior experience, popular Bayesian estimation techniques usually begin with some form of 'uninformative' prior distribution intended to have minimal infer-ential influence. The Bayes rule will still produce nice looking estimates and credible intervals, but these lack the logical force that is attached to experience-based priors and require further justification. The paper concerns the frequentist assessment of Bayes estimates. A simple for-mula is shown to give the frequentist standard deviation of a Bayesian point estimate. The same simulations as required for the point estimate also produce the standard deviation. Exponen-tial family models make the calculations particularly simple and bring in a connection to the parametric bootstrap.},
author = {Efron, Bradley},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Efron - 2015 - Frequentist Accuracy of Bayesian Estimates.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B},
keywords = {Approximate bootstrap confidence intervals,General accuracy formula,Hierarchical and empirical Bayes,Markov chain Monte Carlo methods,Parametric bootstrap},
number = {3},
pages = {617--646},
title = {{Frequentist Accuracy of Bayesian Estimates}},
volume = {77},
year = {2015}
}
@inproceedings{Auger2007,
author = {Auger, Anne and Teytaud, Olivier},
booktitle = {Proceedings of the 9th annual conference on Genetic and evolutionary computation},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Auger, Teytaud - 2007 - Continuous lunches are free!.pdf:pdf},
isbn = {9781595936974},
keywords = {free-lunch,kolmogorov,no-free-lunch,s extension theo-},
pages = {916--922},
title = {{Continuous lunches are free!}},
url = {http://dl.acm.org/citation.cfm?id=1277145},
year = {2007}
}
@article{Baumbach2016,
author = {Baumbach, Sebastian and Wittich, Frank and Sachs, Florian and Ahmed, Sheraz and Dengel, Andreas},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Baumbach et al. - 2016 - A Novel Approach for Data-Driven Automatic Site Recommendation and Selection.pdf:pdf},
title = {{A Novel Approach for Data-Driven Automatic Site Recommendation and Selection}},
year = {2016}
}
@inproceedings{Joachims2003,
author = {Joachims, Thorsten},
booktitle = {Proceedings of the 20th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Joachims - 2003 - Transductive learning via spectral graph partitioning.pdf:pdf},
pages = {290--297},
title = {{Transductive learning via spectral graph partitioning}},
url = {http://www.aaai.org/Papers/ICML/2003/ICML03-040.pdf},
year = {2003}
}
@article{Bruand2011,
abstract = {Mass Spectrometric Imaging (MSI) is a molecular imaging technique that allows the generation of 2D ion density maps for a large complement of the active molecules present in cells and sectioned tissues. Automatic segmentation of such maps according to patterns of co-expression of individual molecules can be used for discovery of novel molecular signatures (molecules that are specifically expressed in particular spatial regions). However, current segmentation techniques are biased toward the discovery of higher abundance molecules and large segments; they allow limited opportunity for user interaction, and validation is usually performed by similarity to known anatomical features. We describe here a novel method, AMASS (Algorithm for MSI Analysis by Semi-supervised Segmentation). AMASS relies on the discriminating power of a molecular signal instead of its intensity as a key feature, uses an internal consistency measure for validation, and allows significant user interaction and supervision as options. An automated segmentation of entire leech embryo data images resulted in segmentation domains congruent with many known organs, including heart, CNS ganglia, nephridia, nephridiopores, and lateral and ventral regions, each with a distinct molecular signature. Likewise, segmentation of a rat brain MSI slice data set yielded known brain features and provided interesting examples of co-expression between distinct brain regions. AMASS represents a new approach for the discovery of peptide masses with distinct spatial features of expression. Software source code and installation and usage guide are available at http://bix.ucsd.edu/AMASS/ .},
author = {Bruand, Jocelyne and Alexandrov, Theodore and Sistla, Srinivas and Wisztorski, Maxence and Meriaux, C{\'{e}}line and Becker, Michael and Salzet, Michel and Fournier, Isabelle and Macagno, Eduardo and Bafna, Vineet},
doi = {10.1021/pr2005378},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bruand et al. - 2011 - AMASS algorithm for MSI analysis by semi-supervised segmentation.pdf:pdf},
issn = {1535-3907},
journal = {Journal of proteome research},
keywords = {Algorithms,Animals,Automatic Data Processing,Brain,Brain: metabolism,Cluster Analysis,Computational Biology,Computational Biology: methods,Gene Expression Regulation,Gene Expression Regulation, Developmental,Image Processing, Computer-Assisted,Image Processing, Computer-Assisted: methods,Leeches,Mass Spectrometry,Mass Spectrometry: methods,Peptides,Peptides: chemistry,Rats,Spectrometry, Mass, Matrix-Assisted Laser Desorpti},
month = {oct},
number = {10},
pages = {4734--43},
pmid = {21800894},
title = {{AMASS: algorithm for MSI analysis by semi-supervised segmentation.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3190602{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {10},
year = {2011}
}
@book{Little2002,
address = {New York},
author = {Little, Roderick J. A. and Rubin, Donald B.},
booktitle = {Wiley, New York.},
edition = {Second},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Little, Rubin - 2002 - Statistical Analysis with Missing Data.pdf:pdf},
isbn = {3175723993},
publisher = {Wiley},
title = {{Statistical Analysis with Missing Data}},
year = {2002}
}
@inproceedings{Herbrich2006a,
author = {Herbrich, Ralf and Minka, Tom and Graepel, Thore},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Herbrich, Minka, Graepel - 2006 - TrueSkill A Bayesian Skill Rating System.pdf:pdf},
title = {{TrueSkill: A Bayesian Skill Rating System}},
year = {2006}
}
@article{Klambauer2017,
abstract = {Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.},
archivePrefix = {arXiv},
arxivId = {1706.02515},
author = {Klambauer, G{\"{u}}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
doi = {1706.02515},
eprint = {1706.02515},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/1706.02515.pdf:pdf},
title = {{Self-Normalizing Neural Networks}},
url = {http://arxiv.org/abs/1706.02515},
year = {2017}
}
@article{Jarrett2016,
author = {Jarrett, Richard G.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jarrett - 2016 - The Analysis of Designed Experiments with Missing Observations.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
number = {1},
pages = {38--46},
title = {{The Analysis of Designed Experiments with Missing Observations}},
volume = {27},
year = {2016}
}
@article{Cheplygina2010,
author = {Cheplygina, Veronika},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cheplygina - 2010 - Random Subspace Method for One-Class Classifiers.pdf:pdf},
title = {{Random Subspace Method for One-Class Classifiers}},
year = {2010}
}
@inproceedings{White2012,
author = {White, Martha and Schuurmans, Dale},
booktitle = {International Conference on Artificial Intelligence and Statistics},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/White, Schuurmans - 2012 - Generalized optimal reverse prediction.pdf:pdf},
pages = {1305--1313},
title = {{Generalized optimal reverse prediction}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS2012{\_}WhiteS12.pdf},
year = {2012}
}
@article{Nowak2008,
abstract = {When applying hierarchical clustering algorithms to cluster patient samples from microarray data, the clustering patterns generated by most algorithms tend to be dominated by groups of highly differentially expressed genes that have closely related expression patterns. Sometimes, these genes may not be relevant to the biological process under study or their functions may already be known. The problem is that these genes can potentially drown out the effects of other genes that are relevant or have novel functions. We propose a procedure called complementary hierarchical clustering that is designed to uncover the structures arising from these novel genes that are not as highly expressed. Simulation studies show that the procedure is effective when applied to a variety of examples. We also define a concept called relative gene importance that can be used to identify the influential genes in a given clustering. Finally, we analyze a microarray data set from 295 breast cancer patients, using clustering with the correlation-based distance measure. The complementary clustering reveals a grouping of the patients which is uncorrelated with a number of known prognostic signatures and significantly differing distant metastasis-free probabilities.},
author = {Nowak, Gen and Tibshirani, Robert},
doi = {10.1093/biostatistics/kxm046},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nowak, Tibshirani - 2008 - Complementary hierarchical clustering.pdf:pdf},
issn = {1468-4357},
journal = {Biostatistics (Oxford, England)},
keywords = {Algorithms,Breast Neoplasms,Breast Neoplasms: genetics,Cluster Analysis,Computer Simulation,Female,Fuzzy Logic,Gene Expression,Gene Expression Profiling,Gene Expression Profiling: methods,Gene Expression Profiling: statistics {\&} numerical,Genetic Markers,Humans,Information Storage and Retrieval,Information Storage and Retrieval: methods,Neoplasm Metastasis,Neoplasm Metastasis: genetics,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Pattern Recognition, Automated,Principal Component Analysis,Reference Values},
month = {jul},
number = {3},
pages = {467--83},
pmid = {18093965},
title = {{Complementary hierarchical clustering.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3294318{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {9},
year = {2008}
}
@book{Casella2002,
author = {Casella, George and Berger, Roger L.},
edition = {Second},
title = {{Statistical Inference}},
year = {2002}
}
@incollection{Xie2014,
author = {Xie, Yihui},
booktitle = {Implementing Reproducible Research},
chapter = {1},
editor = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger D.},
publisher = {CRC Press},
title = {{knitr: A Comprehensive Tool for Reproducible Research in R}},
year = {2014}
}
@book{Durbin2012,
abstract = {This new edition updates Durbin {\&} Koopman's important text on the state space approach to time series analysis. The distinguishing feature of state space time series models is that observations are regarded as made up of distinct components such as trend, seasonal, regression elements and disturbance terms, each of which is modelled separately. The techniques that emerge from this approach are very flexible and are capable of handling a much wider range of problems than the main analytical system currently in use for time series analysis, the Box-Jenkins ARIMA system. Additions to this second edition include the filtering of nonlinear and non-Gaussian series. Part I of the book obtains the mean and variance of the state, of a variable intended to measure the effect of an interaction and of regression coefficients, in terms of the observations. Part II extends the treatment to nonlinear and non-normal models. For these, analytical solutions are not available so methods are based on simulation.},
author = {Durbin, James and Koopman, Siem Jan},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Durbin{\_}J.,{\_}Koopman{\_}S.J.{\_}Time{\_}Series{\_}Analysis{\_}by{\_}State{\_}Space{\_}Methods.pdf:pdf},
pages = {346},
title = {{Time Series Analysis by State Space Methods: Second Edition}},
url = {https://books.google.com/books?id=fOq39Zh0olQC{\&}pgis=1},
year = {2012}
}
@article{Raudys1998,
author = {Raudys, Sarunas and Duin, Robert P W},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Raudys, Duin - 1998 - Expected classification error of the Fisher linear classifier with pseudo-inverse covariance matrix.pdf:pdf},
journal = {Pattern Recognition Letters},
keywords = {Dimensionality,fisher linear discriminant,generalization error,pseudo-inversion,sample size,scissors effect,statistical classification},
month = {apr},
number = {5-6},
pages = {385--392},
publisher = {Elsevier},
title = {{Expected classification error of the Fisher linear classifier with pseudo-inverse covariance matrix}},
volume = {19},
year = {1998}
}
@article{Lockhart2014c,
author = {Wasserman, Larry},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wasserman - 2014 - Discussion “a significance test for the lasso”.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {apr},
number = {2},
pages = {501--508},
title = {{Discussion: “a significance test for the lasso”}},
url = {http://projecteuclid.org/euclid.aos/1400592161},
volume = {42},
year = {2014}
}
@article{Janzing2015,
author = {Janzing, Dominik and Sch{\"{o}}lkopf, Bernhard},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Janzing, Sch{\"{o}}lkopf - 2015 - Semi-Supervised Interpolation in an Anticausal Learning Scenario.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {anticausal learning,causality,independence of cause and,information geometry,mechanism,semi-supervised learning},
number = {Sep},
pages = {1923--1948},
title = {{Semi-Supervised Interpolation in an Anticausal Learning Scenario}},
volume = {16},
year = {2015}
}
@inproceedings{Chapelle2002,
author = {Chapelle, Olivier and Weston, Jason and Sch{\"{o}}lkopf, Bernhard},
booktitle = {Advances in Neural Information Processing Systems 14},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chapelle, Weston, Sch{\"{o}}lkopf - 2002 - Cluster kernels for semi-supervised learning.pdf:pdf},
pages = {585--592},
title = {{Cluster kernels for semi-supervised learning}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AA13.pdf},
year = {2002}
}
@article{Hanczar2010,
author = {Hanczar, Blaise and Dougherty, Edward R.},
doi = {10.2174/157489310790596376},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hanczar, Dougherty - 2010 - On the Comparison of Classifiers for Microarray Data.pdf:pdf},
issn = {15748936},
journal = {Current Bioinformatics},
keywords = {classifier comparison,error estimation,microarray classification,variance study},
month = {mar},
number = {1},
pages = {29--39},
title = {{On the Comparison of Classifiers for Microarray Data}},
url = {http://openurl.ingenta.com/content/xref?genre=article{\&}issn=1574-8936{\&}volume=5{\&}issue=1{\&}spage=29},
volume = {5},
year = {2010}
}
@article{Krause2014,
abstract = {Submodularity1 is a property of set functions with deep theoretical consequences and far– reaching applications. At first glance it appears very similar to concavity, in other ways it resembles convexity. It appears in a wide variety of applications: in Computer Science it ... $\backslash$n},
author = {Krause, Andreas and Golovin, Daniel},
doi = {10.1017/CBO9781139177801.004},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Krause, Golovin - 2014 - Submodular function maximization.pdf:pdf},
isbn = {9781139177801},
issn = {{\textless}null{\textgreater}},
journal = {Tractability: Practical Approaches to Hard Problems},
pages = {71--104},
title = {{Submodular function maximization}},
volume = {3},
year = {2014}
}
@article{Xiang2016a,
author = {Xiang, Zhen James and Wang, Yun and Ramadge, Peter J},
doi = {10.1109/TPAMI.2016.2568185},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/07469344.pdf:pdf},
issn = {0162-8828},
number = {SEPTEMBER 2015},
pages = {1--20},
title = {{Screening Tests for Lasso Problems}},
volume = {8828},
year = {2016}
}
@inproceedings{Chaubey2003,
author = {Chaubey, Yogendra P. and Nebebe, Fassil and Sen, Debaraj},
booktitle = {Joint Statistical Meetings},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chaubey, Nebebe, Sen - 2003 - Estimation of Joint Distribution from Marginal Distributions.pdf:pdf},
keywords = {bayesian prediction,because they require multidimensional,ble,contingency ta- methods,dirichlet prior,however,is preferred as it,merical integration,nu-,of the,readily presents an estimate,the bayesian method},
pages = {883--889},
title = {{Estimation of Joint Distribution from Marginal Distributions}},
url = {http://www.amstat.org/sections/SRMS/Proceedings/y2003/Files/JSM2003-000794.pdf},
year = {2003}
}
@article{Devroye1982,
abstract = {Consider the basic discrimination problem based on a sample of size n drawn from the distribution of (X, Y) on the Borel sets of Rdx {\{}O, 1{\}}. If 0 {\textless} R*{\textless} is a given number, and 'n - 0 is an arbitrary positive sequence, then for any discrimination rule one can find a distribution for (X, Y), not depending upon n, with Bayes probability of error R* such that the probability of error (Rn) of the discrimination rule is larger than R* + 'On for infinitely many n. We give a formal proof of this result, which is a generalization of a result by Cover [1].},
author = {Devroye, L},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Devroye - 1982 - Any discrimination rule can have an arbitrarily bad probability of error for finite sample size.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {2},
pages = {154--157},
title = {{Any discrimination rule can have an arbitrarily bad probability of error for finite sample size.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21869021},
volume = {4},
year = {1982}
}
@inproceedings{Joulin2012,
author = {Joulin, Armand and Bach, Francis},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Joulin, Bach - 2012 - A convex relaxation for weakly supervised classifiers.pdf:pdf},
keywords = {MIL,convex relaxation,weak supervision},
pages = {1279--1286},
title = {{A convex relaxation for weakly supervised classifiers}},
url = {http://arxiv.org/abs/1206.6413},
year = {2012}
}
@inproceedings{Joachims1999,
author = {Joachims, Thorsten},
booktitle = {Proceedings of the 16th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Joachims - 1999 - Transductive inference for text classification using support vector machines.pdf:pdf},
pages = {200--209},
publisher = {Morgan Kaufmann Publishers},
title = {{Transductive inference for text classification using support vector machines}},
year = {1999}
}
@article{Rahimi2007,
abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. Our randomized features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shift-invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classification and regression tasks linear machine learning algorithms that use these features outperform state-of-the-art large-scale kernel machines.},
author = {Rahimi, Ali and Recht, Ben},
doi = {10.1.1.145.8736},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rahimi, Recht - 2007 - Random features for large-scale kernel machines.pdf:pdf},
isbn = {160560352X},
journal = {Advances in Neural Information Processing Systems},
number = {1},
pages = {1--8},
title = {{Random features for large-scale kernel machines}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2007{\_}833.pdf},
year = {2007}
}
@article{Chatterjee2007,
annote = {The authors show how to use genetic algorithms from Matlab to generate datasets with the same statistics but different visual characteristics, like the manually constructed Anscombe datasets. Results show some interesting datasets for some datasets, although a better measure of fitness is likely needed to get datasets that have the same stricking differences as the Anscombe datasets.},
author = {Chatterjee, Sangit and Firat, Aykut},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chatterjee, Firat - 2007 - Generating Data with Identical Statistics but Dissimilar Graphics.pdf:pdf},
journal = {The American Statistician},
month = {aug},
number = {3},
pages = {248--254},
title = {{Generating Data with Identical Statistics but Dissimilar Graphics}},
volume = {61},
year = {2007}
}
@article{Simmons2011,
author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
doi = {10.1177/0956797611417632},
isbn = {1467-9280 (Electronic)$\backslash$n0956-7976 (Linking)},
issn = {0956-7976},
journal = {Psychological Science},
month = {nov},
number = {11},
pages = {1359--1366},
pmid = {22006061},
title = {{False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant}},
url = {http://pss.sagepub.com/lookup/doi/10.1177/0956797611417632},
volume = {22},
year = {2011}
}
@article{Terwee2009,
abstract = {Objective: To provide a practical example showing how to relate measurement error to minimal important change (MIC) to estimate sample sizes required for detecting important changes in research, and to determine the usefulness of patient-reported outcomes (PROs) in daily clinical practice. Study Design and Setting: The standard error of measurement (SEM) can be calculated as a parameter of measurement error. MIC can be determined by taking the mean change in score on the questionnaire in the subgroup of patients that reported to be "a little better" or "better" on a global rating scale minus the mean change in score in the subgroup of patients that reported to be "not changed." Based on SEM and MIC, formulas are presented that can be used to estimate sample sizes and to determine whether the questionnaire can be used to monitor changes in individual patients in daily clinical practice. Results: A fully worked-out example is presented, using Western Ontario and McMaster University Osteoarthritis Index data from a cohort of patients undergoing hip or knee replacement. Conclusion: We strongly recommend investigators to present the SEM and MIC of PROs and use these to estimate sample sizes for research and to determine the usefulness of these outcomes in daily clinical practice. {\textcopyright} 2009 Elsevier Inc. All rights reserved.},
author = {Terwee, Caroline B. and Roorda, Leo D. and Knol, Dirk L. and {De Boer}, Michiel R. and {De Vet}, Henrica C.W.},
doi = {10.1016/j.jclinepi.2008.10.011},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/2009 Linking measurement error to minimal important change.pdf:pdf},
isbn = {1878-5921 (Electronic)$\backslash$r0895-4356 (Linking)},
issn = {08954356},
journal = {Journal of Clinical Epidemiology},
keywords = {Clinical significance,Osteoarthritis,Psychometrics,Quality of life,Reproducibility of results,Validation study},
number = {10},
pages = {1062--1067},
pmid = {19230609},
publisher = {Elsevier Inc},
title = {{Linking measurement error to minimal important change of patient-reported outcomes}},
url = {http://dx.doi.org/10.1016/j.jclinepi.2008.10.011},
volume = {62},
year = {2009}
}
@article{Salimans2016,
abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classifica-tion on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a hu-man error rate of 21.3{\%}. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable fea-tures of ImageNet classes.},
archivePrefix = {arXiv},
arxivId = {1606.03498},
author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
eprint = {1606.03498},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Salimans et al. - 2016 - Improved Techniques for Training GANs.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {1--10},
title = {{Improved Techniques for Training GANs}},
year = {2016}
}
@article{Ascherio2009,
abstract = {The risk of Parkinson disease (PD) and its rate of progression may decline with increasing concentration of blood urate, a major antioxidant.},
author = {Ascherio, Alberto},
doi = {10.1001/archneurol.2009.247},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ascherio{\_}JAMA neurol (2009) Urate as predictor of rate of clinical decline in PD.pdf:pdf},
isbn = {1538-3687 (Electronic)},
issn = {0003-9942},
journal = {Archives of Neurology},
number = {12},
pages = {1460},
pmid = {19822770},
title = {{Urate as a Predictor of the Rate of Clinical Decline in Parkinson Disease}},
url = {http://archneur.jamanetwork.com/article.aspx?doi=10.1001/archneurol.2009.247},
volume = {66},
year = {2009}
}
@article{White1982,
author = {White, Halbert},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/White - 1982 - Maximum Likelihood Estimation of Misspecified Models.pdf:pdf},
journal = {Econometrica},
number = {1},
pages = {1--25},
title = {{Maximum Likelihood Estimation of Misspecified Models}},
url = {http://www.jstor.org/stable/10.2307/1912526},
volume = {50},
year = {1982}
}
@article{Kawano2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1108.5244v3},
author = {Kawano, Shuichi},
eprint = {arXiv:1108.5244v3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kawano - 2012 - Semi-supervised logistic discrimination via labeled data and unlabeled data from different sampling distributions.pdf:pdf},
journal = {arXiv preprint},
keywords = {and phrases,covariate shift,em algorithm,model selection,reg-,semi-supervised learning,ularization},
pages = {1--19},
title = {{Semi-supervised logistic discrimination via labeled data and unlabeled data from different sampling distributions}},
year = {2012}
}
@unpublished{Loog2013,
author = {Loog, Marco},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - 2013 - Conservative Transductive and Semi-Supervised Empirical Risk Minimization.pdf:pdf},
pages = {1--9},
title = {{Conservative Transductive and Semi-Supervised Empirical Risk Minimization}},
year = {2013}
}
@book{Mohri2012,
author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mohri, Rostamizadeh, Talwalkar - 2012 - Foundations of Machine Learning.pdf:pdf},
isbn = {978-0-262-01825-8},
pages = {1--},
publisher = {The MIT Press},
title = {{Foundations of Machine Learning}},
year = {2012}
}
@article{Fan2014,
author = {Fan, Jianqing and Ke, Zheng Tracy},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fan, Ke - 2014 - Discussion “a significance test for the lasso”.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {apr},
number = {2},
pages = {483--492},
title = {{Discussion: “a significance test for the lasso”}},
volume = {42},
year = {2014}
}
@article{Sugiyama2009,
author = {Sugiyama, Masashi and Id{\'{e}}, Tsuyoshi and Nakajima, Shinichi and Sese, Jun},
doi = {10.1007/s10994-009-5125-7},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sugiyama et al. - 2009 - Semi-supervised local Fisher discriminant analysis for dimensionality reduction.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = {jul},
number = {1-2},
pages = {35--61},
title = {{Semi-supervised local Fisher discriminant analysis for dimensionality reduction}},
url = {http://www.springerlink.com/index/10.1007/s10994-009-5125-7},
volume = {78},
year = {2009}
}
@article{Dimitrakakis2016,
author = {Dimitrakakis, Christos and Nelson, Blaine},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/15-257.pdf:pdf},
pages = {1--33},
title = {{Bayesian Differential Privacy through Posterior Sampling ∗}},
volume = {16},
year = {2016}
}
@article{Harrell2015,
author = {Harrell, Frank E Jr. and Slaughter, James C},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/bbr.pdf:pdf},
journal = {Power},
keywords = {roduction to biostatistics for},
pages = {158},
title = {{Biostatistics for Biomedical Research}},
year = {2015}
}
@article{Ho2004,
abstract = {Studies on ensemble methods for classification suffer from the difficulty of modeling the complementary strengths of the components. Kleinberg's theory of stochastic discrimination (SD) addresses this rigorously via mathematical notions of enrichment, uniformity, and projectability of an ensemble. We explain these concepts via a very simple numerical example that captures the basic principles of the SD theory and method. We focus on a fundamental symmetry in point set covering that is the key observation leading to the foundation of the theory. We believe a better understanding of the SD method will lead to developments of better tools for analyzing other ensemble methods.},
archivePrefix = {arXiv},
arxivId = {cs/0402021},
author = {Ho, Tin Kam},
eprint = {0402021},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho - 2004 - A Numerical Example on the Principles of Stochastic Discrimination.pdf:pdf},
primaryClass = {cs},
title = {{A Numerical Example on the Principles of Stochastic Discrimination}},
url = {http://arxiv.org/abs/cs/0402021},
year = {2004}
}
@article{Schmidt1996,
author = {Schmidt, Karsten},
doi = {10.1007/BF00046993},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schmidt - 1996 - A comparison of minimax and least squares estimators in linear regression with polyhedral prior information.pdf:pdf},
issn = {0167-8019},
journal = {Acta Applicandae Mathematicae},
keywords = {1,3,average performance,estimator,inequality restricted least squares,minimax estima-,n vector of,parameter restrictions,polyhedral prior information in,projection estimators,regression model y,the linear regression model,tion,u,we consider the linear,where y is an,x},
month = {apr},
number = {1},
pages = {127--138},
title = {{A comparison of minimax and least squares estimators in linear regression with polyhedral prior information}},
url = {http://link.springer.com/10.1007/BF00046993},
volume = {43},
year = {1996}
}
@article{Le2015,
author = {Le, Thanh-Binh and Kim, Sang-Woon},
doi = {10.1016/j.patrec.2015.04.011},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Le, Kim - 2015 - Modified criterion to select useful unlabeled data for improving semi-supervised support vector machines.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Semi-supervised learning,Semi-supervised boosting,,semi-supervised boosting,semi-supervised learning,support vector machines},
pages = {48--56},
publisher = {Elsevier Ltd.},
title = {{Modified criterion to select useful unlabeled data for improving semi-supervised support vector machines}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865515001282},
volume = {60-61},
year = {2015}
}
@article{Williamson,
author = {Williamson, Robert C},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Williamson - Unknown - Loss Functions.pdf:pdf},
number = {3},
pages = {1--10},
title = {{Loss Functions}}
}
@article{Ben-David2011b,
author = {Ben-david, Shai and Srebro, Nati and Urner, Ruth},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-david, Srebro, Urner - 2011 - Is learning possible without Prior Knowledge Do Universal Learners exist High level view of ( Statis.pdf:pdf},
title = {{Is learning possible without Prior Knowledge ? Do Universal Learners exist ? High level view of ( Statistical ) Machine Learning}},
year = {2011}
}
@article{Abney2004,
author = {Abney, Steven},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Abney - 2004 - Understanding the Yarowsky algorithm.pdf:pdf},
journal = {Computational Linguistics},
number = {3},
pages = {365--395},
title = {{Understanding the Yarowsky algorithm}},
volume = {30},
year = {2004}
}
@article{Bousquet2004,
author = {Bousquet, Olivier and Boucheron, Stephane and Lugosi, Gabor},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bousquet, Boucheron, Lugosi - 2004 - Introduction to statistical learning theory.pdf:pdf},
journal = {Lecture Notes in Computer Science},
pages = {169--207},
title = {{Introduction to statistical learning theory}},
url = {http://www.springerlink.com/index/CGW0K6W5W1W1WR9B.pdf},
volume = {3176},
year = {2004}
}
@article{Arnold2000,
author = {Arnold, Bernard F. and Stahlecker, Peter},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Arnold, Stahlecker - 2000 - The minimax adjustment principle.pdf:pdf},
journal = {Mathematical methods of operations research},
keywords = {ellipsoidal information,minimax,minimax adjustment principle,principle,projection estimator,supply policy},
pages = {103--113},
title = {{The minimax adjustment principle}},
url = {http://link.springer.com/article/10.1007/s001860050005},
volume = {51},
year = {2000}
}
@article{Donoho2015,
author = {Donoho, David},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Donoho - 2015 - 50 years of data collection UNHCR expce.pdf:pdf},
pages = {1--41},
title = {{50 years of data collection UNHCR expce}},
year = {2015}
}
@article{Jung2008,
abstract = {In recent years, there has been a growing interest among researchers in the use of latent class and growth mixture modeling techniques for applications in the social and psychological sciences, in part due to advances in and availability of computer software designed for this purpose (e.g., Mplus and SAS Proc Traj). Latent growth modeling approaches, such as latent class growth analysis (LCGA) and growth mixture modeling (GMM), have been increasingly recognized for their usefulness for identifying homogeneous subpopulations within the larger heterogeneous population and for the identification of meaningful groups or classes of individuals. The purpose of this paper is to provide an overview of LCGA and GMM, compare the different techniques of latent growth modeling, discuss current debates and issues, and provide readers with a practical guide for conducting LCGA and GMM using the Mplus software.},
author = {Jung, Tony and Wickrama, K. A.},
doi = {10.1111/j.1751-9004.2007.00054.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jung, Wickrama - 2008 - An introduction to latent class growth analysis and growth mixture modeling.pdf:pdf},
isbn = {1751-9004},
issn = {1751-9004},
journal = {Social and Personality Psychology Compass},
number = {1},
pages = {302--317},
title = {{An introduction to latent class growth analysis and growth mixture modeling}},
url = {http://doi.wiley.com/10.1111/j.1751-9004.2007.00054.x{\%}5Cnhttp://onlinelibrary.wiley.com/doi/10.1111/j.1751-9004.2007.00054.x/full},
volume = {2},
year = {2008}
}
@inproceedings{Elworthy1994,
archivePrefix = {arXiv},
arxivId = {arXiv:cmp-lg/9410012v2},
author = {Elworthy, David},
booktitle = {Proceedings of the 4th Conference on Applied Natural Language Processing},
eprint = {9410012v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Elworthy - 1994 - Does Baum-Welch re-estimation help taggers.pdf:pdf},
pages = {53--58},
primaryClass = {arXiv:cmp-lg},
title = {{Does Baum-Welch re-estimation help taggers?}},
year = {1994}
}
@inproceedings{Ghahramani1994,
annote = {Shows how minimization the complete data likelihood of generative models using EM allows one to fit both mixtures and deal with missing values in a uniform way. They point out that for missing values, one does not just predict them in the E step, but rather one uses the expected sufficient statistics belonging to these unknown values.},
author = {Ghahramani, Zoubin and Jordan, Michael I.},
booktitle = {Advances in Neural Information Processing Systems 6},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ghahramani, Jordan - 1994 - Supervised Learning from incomplete data via an EM approach.pdf:pdf},
title = {{Supervised Learning from incomplete data via an EM approach}},
year = {1994}
}
@inproceedings{Cortes1993,
author = {Cortes, Corinna and Jackel, L.D.},
booktitle = {Advances in Neural Information Processing Systems 6},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cortes, Jackel - 1993 - Learning Cuves Asymptotic Values and Rate of Convergence.pdf:pdf},
pages = {327--334},
title = {{Learning Cuves: Asymptotic Values and Rate of Convergence}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Learning+Cuves:+Asymptotic+Values+and+Rate+of+Convergence{\#}0},
year = {1993}
}
@article{Cai2014,
author = {Cai, T. Tony and Yuan, Ming},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cai, Yuan - 2014 - Discussion “a significance test for the lasso”.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {apr},
number = {2},
pages = {478--482},
title = {{Discussion: “a significance test for the lasso”}},
volume = {42},
year = {2014}
}
@article{Balsubramania,
author = {Balsubramani, Akshay},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balsubramani - Unknown - Scalable Semi-Supervised Aggregation of Classifiers.pdf:pdf},
pages = {1--9},
title = {{Scalable Semi-Supervised Aggregation of Classifiers}}
}
@article{Denis2015,
abstract = {Confident prediction is highly relevant in machine learning; for example, in applications such as medical diagnoses, wrong prediction can be fatal. For classification, there already exist procedures that allow to not classify data when the confidence in their prediction is weak. This approach is known as classification with reject option. In the present paper, we provide new methodology for this approach. Predicting a new instance via a confidence set, we ensure an exact control of the probability of classification. Moreover, we show that this methodology is easily implementable and entails attractive theoretical and numerical properties.},
archivePrefix = {arXiv},
arxivId = {1507.07235},
author = {Denis, Christophe and Hebiri, Mohamed},
eprint = {1507.07235},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Denis, Hebiri - 2015 - Consistency of plug-in confidence sets for classification in semi-supervised learning.pdf:pdf},
keywords = {classification,classification with reject option,confidence sets,conformal predictors,plug-in confidence sets},
pages = {1--26},
title = {{Consistency of plug-in confidence sets for classification in semi-supervised learning}},
url = {http://arxiv.org/abs/1507.07235},
year = {2015}
}
@article{Michie1994,
author = {Michie, D and Spiegelhalter, D J and Taylor, C C},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Michie, Spiegelhalter, Taylor - 1994 - Statlog.pdf:pdf},
title = {{Statlog}},
year = {1994}
}
@inproceedings{Krizhevsky2012,
author = {Krizhevsky, Alex and Sutskever, I and Hinton, GE},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
pages = {1097--1105},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{Stan2016,
author = {{Stan Development Team}},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Team - 2016 - Stan Modeling Language User ' s Guide and Reference Manual.pdf:pdf},
title = {{Stan Modeling Language User's Guide and Reference Manual}},
year = {2016}
}
@article{Vehtari2015b,
abstract = {Importance weighting is a convenient general way to adjust for draws from the wrong distribution, but the resulting ratio estimate can be noisy when the importance weights have a heavy right tail, as routinely occurs when there are aspects of the target distribution not well captured by the approximating distribution. More stable estimates can be obtained by truncating the importance ratios. Here we present a new method for stabilizing importance weights using generalized Pareto distribution fit to the upper tail of the distribution of the simulated importance ratios.},
archivePrefix = {arXiv},
arxivId = {1507.02646},
author = {Vehtari, Aki and Gelman, Andrew},
eprint = {1507.02646},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/1507.02646v4.pdf:pdf},
journal = {arXiv},
keywords = {bayesian computation,importance sampling,leave-one-,loo,monte carlo,out cross-validation},
number = {July},
title = {{Pareto Smoothed Importance Sampling}},
url = {http://arxiv.org/abs/1507.02646},
year = {2015}
}
@inproceedings{Sokolovska2008,
address = {Helsinki, Finland},
author = {Sokolovska, Nataliya and Capp{\'{e}}, Olivier and Yvon, Francois},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
editor = {Cohen, William W. and McCallum, Andrew and Roweis, Sam T.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sokolovska, Capp{\'{e}}, Yvon - 2008 - The asymptotics of semi-supervised learning in discriminative probabilistic models.pdf:pdf},
pages = {984--991},
publisher = {ACM Press},
title = {{The asymptotics of semi-supervised learning in discriminative probabilistic models}},
year = {2008}
}
@article{Li2013,
author = {Li, YF and Tsang, IW and Kwok, JT and Zhou, ZH},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li et al. - 2013 - Convex and Scalable Weakly Labeled SVMs.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {2151--2188},
title = {{Convex and Scalable Weakly Labeled SVMs}},
url = {http://arxiv.org/abs/1303.1271},
volume = {14},
year = {2013}
}
@inproceedings{Bennett1998,
author = {Bennett, Kristin P. and Demiriz, Ayhan},
booktitle = {Advances in Neural Information Processing Systems 11},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bennett, Demiriz - 1998 - Semi-supervised support vector machines.pdf:pdf},
pages = {368--374},
title = {{Semi-supervised support vector machines}},
year = {1998}
}
@article{VanRooden2011,
abstract = {The clinical heterogeneity of Parkinson's disease (PD) may point at the existence of subtypes. Because subtypes likely reflect distinct underlying etiologies, their identification may facilitate future genetic and pharmacotherapeutic studies. Aim of this study was to identify subtypes by a data-driven approach applied to a broad spectrum of motor and nonmotor features of PD. Data of motor and nonmotor PD symptoms were collected in 802 patients in two different European prevalent cohorts. A model-based cluster analysis was conducted on baseline data of 344 patients of a Dutch cohort (PROPARK). Reproducibility of these results was tested in data of the second annual assessment of the same cohort and validated in an independent Spanish cohort (ELEP) of 357 patients. The subtypes were subsequently characterized on clinical and demographic variables. Four similar PD subtypes were identified in two different populations and are largely characterized by differences in the severity of nondopaminergic features and motor complications: Subtype 1 was mildly affected in all domains, Subtype 2 was predominantly characterized by severe motor complications, Subtype 3 was affected mainly on nondopaminergic domains without prominent motor complications, while Subtype 4 was severely affected on all domains. The subtypes had largely similar mean disease durations (nonsignificant differences between three clusters) but showed considerable differences with respect to their association with demographic and clinical variables. In prevalent disease, PD subtypes are largely characterized by the severity of nondopaminergic features and motor complications and likely reflect complex interactions between disease mechanisms, treatment, aging, and gender.},
author = {van Rooden, Stephanie M and Colas, Fabrice P. R. and Mart{\'{i}}nez-Mart{\'{i}}n, Pablo and Visser, Martine and Verbaan, Dagmar and Marinus, Johan and Chaudhuri, Ray K and Kok, Joost N and van Hilten, Jacobus J},
doi = {10.1002/mds.23346},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/van Rooden et al. - 2011 - Clinical subtypes of Parkinson's disease.pdf:pdf},
issn = {1531-8257},
journal = {Movement disorders : official journal of the Movement Disorder Society},
keywords = {Aged,Cluster Analysis,Cohort Studies,Disease Progression,Female,Germany,Humans,Male,Middle Aged,Neurologic Examination,Parkinson Disease,Parkinson Disease: classification,Parkinson Disease: physiopathology,Reproducibility of Results,Spain,Time Factors},
month = {jan},
number = {1},
pages = {51--8},
pmid = {21322019},
title = {{Clinical subtypes of Parkinson's disease.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21322019},
volume = {26},
year = {2011}
}
@inproceedings{McWilliams2013,
abstract = {This paper presents Correlated Nystr¨ om Views (XNV), a fast semi-supervised al- gorithm for regression and classification. The algorithm draws on two main ideas. First, it generates two views consisting of computationally inexpensive random features. Second, multiview regression, using Canonical Correlation Analysis (CCA) on unlabeled data, biases the regression towards useful features. It has been shown that CCA regression can substantially reduce variance with a mini- mal increase in bias if the views contains accurate estimators. Recent theoretical and empirical work shows that regression with random features closely approxi- mates kernel regression, implying that the accuracy requirement holds for random views. We show that XNV consistently outperforms a state-of-the-art algorithm for semi-supervised learning: substantially improving predictive performance and reducing the variability of performance on a wide variety of real-world datasets, whilst also reducing runtime by orders of magnitude. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1306.5554v1},
author = {McWilliams, Brian and Balduzzi, David and Buhmann, Joachim M.},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {arXiv:1306.5554v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/McWilliams, Balduzzi, Buhmann - 2013 - Correlated random features for fast semi-supervised learning.pdf:pdf},
pages = {440--44},
title = {{Correlated random features for fast semi-supervised learning}},
year = {2013}
}
@inproceedings{Furnkranz2001,
author = {F{\"{u}}rnkranz, Johannes and Petrak, Johann},
booktitle = {Proceedings of the ECML/PKDD Workshop on Integrating Aspects of Data Mining, Decision Support and Meta-Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/F{\"{u}}rnkranz, Petrak - 2001 - An Evaluation of Landmarking Variants.pdf:pdf},
pages = {57--68},
title = {{An Evaluation of Landmarking Variants}},
url = {http://ai.ijs.si/branax/iddm-2001-proceedings/paper9.pdf},
year = {2001}
}
@incollection{Krijthe2015,
address = {Saint {\'{E}}tienne. France},
archivePrefix = {arXiv},
arxivId = {1507.06802},
author = {Krijthe, Jesse Hendrik and Loog, Marco},
booktitle = {Advances in Intelligent Data Analysis XIV. Lecture Notes in Computer Science, vol 9385.},
doi = {10.1007/978-3-319-24465-5_14},
editor = {Fromont, Elisa and {De Bie}, Tijl and van Leeuwen, Matthijs},
eprint = {1507.06802},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Krijthe, Loog - 2015 - Implicitly Constrained Semi-Supervised Least Squares Classification.pdf:pdf},
keywords = {constrained,least squares classification,semi-supervised learning},
pages = {158--169},
publisher = {Springer},
title = {{Implicitly Constrained Semi-Supervised Least Squares Classification}},
year = {2015}
}
@article{Gelman2013e,
abstract = {Researcher degrees of freedom can lead to a multiple comparisons problem, even in settings where researchers perform only a single analysis on their data. The problem is there can be a large number of potential comparisons when the details of data analysis are highly contingent on data, without the researcher having to perform any conscious procedure of fishing or examining multiple p-values. We discuss in the context of several examples of published papers where data-analysis decisions were theoretically-motivated based on previous literature, but where the details of data selection and analysis were not pre-specified and, as a result, were contingent on data. 1.},
author = {Gelman, Andrew and Loken, Eric},
doi = {10.1037/a0037714},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Loken - 2013 - The garden of forking paths Why multiple comparisons can be a problem, even when there is no “fishing exp.pdf:pdf},
issn = {1939-1455},
pages = {1--17},
pmid = {25180805},
title = {{The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis}},
url = {http://www.stat.columbia.edu/{~}gelman/research/unpublished/p{\_}hacking.pdf},
year = {2013}
}
@article{Scott2009,
author = {Scott, Clayton and Blanchard, Gilles},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Scott, Blanchard - 2009 - Novelty detection Unlabeled data definitely help.pdf:pdf},
pages = {464--471},
title = {{Novelty detection: Unlabeled data definitely help}},
url = {http://eprints.pascal-network.org/archive/00004475/},
volume = {5},
year = {2009}
}
@article{Eckhardt1987a,
abstract = {The Monte Carlo method is a statistical sampling technique that over$\backslash$nthe years has been applied successfully to a vast number of scientific$\backslash$nproblems. Although the computer codes that implement Monte Carlo$\backslash$nhave grown ever more sophisticated, the essence of the method is$\backslash$ncaptured in some unpublished remarks Stan made in 1983 about solitaire.},
author = {Eckhardt, R and Eckhardt},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/00326867.pdf:pdf},
issn = {0028-0836},
journal = {Los Alamos Science},
pages = {131--143},
title = {{Stan Ulam, Jihn Von Neumann, and the Monte Carlo Method}},
url = {http://www.fas.org/sgp/othergov/doe/lanl/pubs/00326867.pdf},
volume = {15},
year = {1987}
}
@inproceedings{Lu2016,
author = {Lu, Kai and Xie, Jiang and Shu, Junhui},
booktitle = {High Performance Computing Applications},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lu, Xie, Shu - 2016 - A Fast Training Method for Transductive Support Vector Machine in Semi-supervised Learning.pdf:pdf},
keywords = {fast training,semi-supervised classification,transductive support},
pages = {211--217},
title = {{A Fast Training Method for Transductive Support Vector Machine in Semi-supervised Learning}},
year = {2016}
}
@inproceedings{Fan2008,
author = {Fan, Bin and Lei, Zhen and Li, Stan Z.},
booktitle = {The 8th International Conference on Automatic Face {\&} Gesture Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fan, Lei, Li - 2008 - Normalized LDA for Semi-supervised Learning.pdf:pdf},
pages = {1--6},
title = {{Normalized LDA for Semi-supervised Learning}},
year = {2008}
}
@article{Hand2006a,
author = {Hand, David J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hand - 2006 - Classifier Technology and the Illusion of Progress.pdf:pdf},
journal = {Statistical Science},
number = {1},
pages = {1--14},
title = {{Classifier Technology and the Illusion of Progress}},
volume = {21},
year = {2006}
}
@article{Duin2015,
author = {Duin, Robert P.W.},
doi = {10.1016/j.patrec.2015.04.015},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Duin - 2015 - The dissimilarity representation for finding universals from particulars by an anti-essentialist approach.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Anti-essentialism,Generalization,Nearest Neighbor Rule,Representation},
publisher = {Elsevier Ltd.},
title = {{The dissimilarity representation for finding universals from particulars by an anti-essentialist approach}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865515001324},
year = {2015}
}
@article{Mayo2013,
author = {Mayo, Deborah G.},
doi = {10.1111/j.2044-8317.2012.02064.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mayo-2013-The{\_}British{\_}journal{\_}of{\_}mathematical{\_}and{\_}statistical{\_}psychology.pdf:pdf},
isbn = {0007-1102},
issn = {00071102},
journal = {British Journal of Mathematical and Statistical Psychology},
number = {1},
pages = {57--64},
pmid = {23046378},
title = {{The error-statistical philosophy and the practice of Bayesian statistics: Comments on Gelman and Shalizi: 'Philosophy and the practice of Bayesian statistics'}},
volume = {66},
year = {2013}
}
@article{Bengio2010,
author = {Bengio, Yoshua and Delalleau, Olivier and Simard, Clarence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bengio, Delalleau, Simard - 2010 - Decision trees do not generalize to new variations.pdf:pdf},
journal = {Computational Intelligence},
number = {4},
pages = {449--467},
title = {{Decision trees do not generalize to new variations}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8640.2010.00366.x/full},
volume = {26},
year = {2010}
}
@article{Premachandran2016,
author = {Premachandran, Vittal and Tarlow, Daniel and Yuille, Alan and Batra, Dhruv},
doi = {10.1109/TPAMI.2016.2537807},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Premachandran et al. - 2016 - Empirical Minimum Bayes Risk Prediction.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {1},
pages = {1--1},
title = {{Empirical Minimum Bayes Risk Prediction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7425216},
volume = {6},
year = {2016}
}
@inproceedings{Tax2005,
author = {Tax, David M.J. and Duin, Robert P.W.},
booktitle = {Proceedings of the Sixteenth Annual Symposium of the Pattern Recognition Association of South Africa},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tax, Duin - 2005 - Characterizing one-class datasets.pdf:pdf},
number = {4},
pages = {21--26},
title = {{Characterizing one-class datasets}},
url = {http://mediamatica.ewi.tudelft.nl/sites/default/files/TaxDui2005.pdf},
volume = {1},
year = {2005}
}
@book{Lehmann1986,
author = {Lehmann, E.L.},
edition = {2},
publisher = {Wiley},
title = {{Testing Statistical Hyptoheses}},
year = {1986}
}
@article{Vandewalle2013,
author = {Vandewalle, Vincent and Biernacki, Christophe},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vandewalle, Biernacki - 2013 - A predictive deviance criterion for selecting a generative model in semi-supervised classification.pdf:pdf},
journal = {Computational Statistics {\&} Data Analysis},
pages = {220--236},
title = {{A predictive deviance criterion for selecting a generative model in semi-supervised classification}},
url = {http://www.sciencedirect.com/science/article/pii/S0167947313000546},
volume = {64},
year = {2013}
}
@phdthesis{Krijthe2012,
abstract = {In order to choose from the large number of classification methods available for use, cross-validation error estimates are often employed. We present this cross-validation selection strategy in the framework of meta-learning and show that conceptually, meta- learning techniques could provide better classifier selections than traditional cross-validation selection. Using various simulation studies we illustrate and discuss this possibility. Through a collection of datasets resembling real-world data, we investigate whether these improvements could possibly exist in the real-world as well. Although the approach presented here currently requires signifi- cant investment when applied to practical applications, the concept of being able to outperform cross-validation selection opens the door to new classifier selection strategies.},
author = {Krijthe, Jesse Hendrik},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Krijthe - 2012 - Improving Cross-Validation Classifier Selection Accuracy through Meta-Learning.pdf:pdf},
keywords = {Classifier Selection,Error estimation,Meta-Learning},
mendeley-tags = {Classifier Selection,Error estimation,Meta-Learning},
school = {Delft University of Technology},
title = {{Improving Cross-Validation Classifier Selection Accuracy through Meta-Learning}},
year = {2012}
}
@article{Lanckriet2002,
abstract = {When constructing a classifier, the probability of correct classification of future data points should be maximized. We consider a binary classification problem where the mean and covariance matrix of each class are assumed to be known. No further assumptions are made with respect to the class- conditional distributions. Misclassification probabilities are then controlled in a worst-case setting: that is, under all possible choices of class-conditional densities with given mean and covariance matrix, we minimize the worst-case (maximum) probability of misclassification of future data points. For a linear decision boundary, this desideratum is translated in a very direct way into a (convex) second order cone optimization problem, with complexity similar to a support vector machine problem. The minimax problem can be interpreted geometrically as minimizing the maximum of the Mahalanobis distances to the two classes. We address the issue of robustness with respect to estimation errors (in the means and covariances of the classes) via a simple modification of the input data. We also show how to exploit Mercer kernels in this setting to obtain nonlinear decision boundaries, yielding a classifier which proves to be competitive with current methods, including support vector machines. An important feature of this method is that a worst-case bound on the probability of misclassification of future data is always obtained explicitly},
author = {Lanckriet, Gert R. G. and Ghaoui, Laurent El and Bhattacharyya, Chiranjib and Jordan, Michael I.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lanckriet et al. - 2002 - A Robust Minimax Approach to Classification.pdf:pdf},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {classification,convex optimization,kernel methods,second order cone programming},
pages = {555--582},
title = {{A Robust Minimax Approach to Classification}},
volume = {3},
year = {2002}
}
@inproceedings{Druck2010,
author = {Druck, Gregory and McCallum, Andrew Kachites},
booktitle = {Proceedings of the 27th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Druck, McCallum - 2010 - High-performance semi-supervised learning using discriminatively constrained generative models.pdf:pdf},
pages = {319--326},
title = {{High-performance semi-supervised learning using discriminatively constrained generative models}},
url = {http://www.cs.umass.edu/{~}gdruck/pubs/druck10high.pdf http://machinelearning.wustl.edu/mlpapers/paper{\_}files/icml2010{\_}DruckM10.pdf},
year = {2010}
}
@book{Bertsekas1999,
author = {Bertsekas, Dimitri P},
publisher = {Athena scientific},
title = {{Nonlinear programming}},
year = {1999}
}
@article{Byrd1995,
author = {Byrd, Richard H. and Lu, Peihuang and Nocedal, Jorge and Zhu, Ciyou},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Byrd et al. - 1995 - A limited memory algorithm for bound constrained optimization.pdf:pdf},
journal = {SIAM Journal on Scientific Computing},
number = {5},
pages = {1190--1208},
title = {{A limited memory algorithm for bound constrained optimization}},
volume = {16},
year = {1995}
}
@article{Wilson2016a,
abstract = {We present a set of computing tools and techniques that every researcher can and should adopt. These recommendations synthesize inspiration from our own work, from the experiences of the thousands of people who have taken part in Software Carpentry and Data Carpentry workshops over the past six years, and from a variety of other guides. Unlike some other guides, our recommendations are aimed specifically at people who are new to research computing.},
archivePrefix = {arXiv},
arxivId = {1609.00037},
author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
eprint = {1609.00037},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wilson et al. - 2016 - Good Enough Practices in Scientific Computing.pdf:pdf},
pages = {1--30},
title = {{Good Enough Practices in Scientific Computing}},
url = {http://arxiv.org/abs/1609.00037},
year = {2016}
}
@inproceedings{Subramanya2009,
author = {Subramanya, Amarnag and Bilmes, Jeff a},
booktitle = {Advances in Neural Information Processing Systems 22},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Subramanya, Bilmes - 2009 - Entropic Graph Regularization in Non-Parametric Semi-Supervised Classification.pdf:pdf},
pages = {1803--1811},
title = {{Entropic Graph Regularization in Non-Parametric Semi-Supervised Classification}},
year = {2009}
}
@article{Buhlmann2002,
author = {Buhlmann, Peter and Yu, Bin},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Buhlmann, Yu - 2002 - Analyzing bagging.pdf:pdf},
journal = {The Annals of Statistics},
number = {4},
pages = {927--961},
title = {{Analyzing bagging}},
volume = {30},
year = {2002}
}
@article{Gelman2013a,
author = {Gelman, Andrew},
doi = {10.1097/EDE.0b013e31827886f7},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman - 2013 - P values and statistical practice.pdf:pdf},
issn = {1531-5487},
journal = {Epidemiology},
month = {jan},
number = {1},
pages = {69--72},
pmid = {23232612},
title = {{P values and statistical practice.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23232612},
volume = {24},
year = {2013}
}
@article{Tanha2015,
author = {Tanha, Jafar and van Someren, Maarten and Afsarmanesh, Hamideh},
doi = {10.1007/s13042-015-0328-7},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tanha, van Someren, Afsarmanesh - 2015 - Semi-supervised self-training for decision tree classifiers.pdf:pdf},
isbn = {1304201503},
issn = {1868-8071},
journal = {International Journal of Machine Learning and Cybernetics},
number = {JANUARY},
title = {{Semi-supervised self-training for decision tree classifiers}},
url = {http://link.springer.com/10.1007/s13042-015-0328-7},
year = {2015}
}
@book{Rasmussen2005,
author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rasmussen, Williams - 2005 - Gaussian Processes for Machine Learning.pdf:pdf},
isbn = {9780262182539},
month = {apr},
publisher = {MIT Press},
title = {{Gaussian Processes for Machine Learning}},
year = {2005}
}
@article{Lee2013,
abstract = {Abstract We propose the simple and efficient method of semi-supervised learning for deep neural networks. Basically, the proposed network is trained in a supervised fashion with labeled and unlabeled data simultaneously. For unlabeled data, Pseudo -Labels, just ... $\backslash$n},
author = {Lee, D H},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lee - 2013 - Pseudo-label The simple and efficient semi-supervised learning method for deep neural networks.pdf:pdf},
journal = {ICML Workshop on Challenges in Representation Learning},
title = {{Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks}},
year = {2013}
}
@inproceedings{Grandvalet2005,
address = {Cambridge, MA},
author = {Grandvalet, Yves and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems 17},
editor = {Saul, L. K. and Weiss, Y. and Bottou, L.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Grandvalet, Bengio - 2005 - Semi-supervised learning by entropy minimization.pdf:pdf},
pages = {529--536},
publisher = {MIT Press},
title = {{Semi-supervised learning by entropy minimization}},
year = {2005}
}
@article{Salimans2014,
abstract = {Recent advances in stochastic gradient variational inference have made it possible to perform variational Bayesian inference with posterior approximations containing auxiliary random variables. This enables us to explore a new synthesis of variational inference and Monte Carlo methods where we incorporate one or more steps of MCMC into our variational approximation. By doing so we obtain a rich class of inference algorithms bridging the gap between variational methods and MCMC, and offering the best of both worlds: fast posterior approximation through the maximization of an explicit objective, with the option of trading off additional computation for additional accuracy. We describe the theoretical foundations that make this possible and show some promising first results.},
archivePrefix = {arXiv},
arxivId = {1410.6460},
author = {Salimans, Tim and Kingma, Diederik P. and Welling, Max},
eprint = {1410.6460},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/salimans15.pdf:pdf},
isbn = {1410.6460},
title = {{Markov Chain Monte Carlo and Variational Inference: Bridging the Gap}},
url = {http://arxiv.org/abs/1410.6460},
year = {2014}
}
@article{Balcan2010,
author = {Balcan, Maria-Florina and Blum, Avrim},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balcan, Blum - 2010 - A Discriminative Model for Semi-Supervised Learning.pdf:pdf},
journal = {Journal of the ACM (JACM)},
number = {3},
title = {{A Discriminative Model for Semi-Supervised Learning}},
url = {http://dl.acm.org/citation.cfm?id=1706599},
volume = {57},
year = {2010}
}
@article{Krijthe2017,
archivePrefix = {arXiv},
arxivId = {1512.08240},
author = {Krijthe, Jesse Hendrik and Loog, Marco},
doi = {10.1016/j.patcog.2016.09.009},
eprint = {1512.08240},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Krijthe, Loog - 2017 - Robust Semi-supervised Least Squares Classification by Implicit Constraints.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {least squares classification,robust,semi-supervised learning},
pages = {115--126},
publisher = {Elsevier},
title = {{Robust Semi-supervised Least Squares Classification by Implicit Constraints}},
volume = {63},
year = {2017}
}
@book{Dresher1961,
author = {Dresher, Melvin},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dresher - 1961 - Games of Strategy Theory and Applications.pdf:pdf},
pages = {1--186},
publisher = {Prentice Hall},
title = {{Games of Strategy: Theory and Applications}},
year = {1961}
}
@article{Kass2011,
abstract = {Statistics has moved beyond the frequentist-Bayesian controversies of the past. Where does this leave our ability to interpret results? I suggest that a philosophy compatible with statistical practice, labeled here statistical pragmatism, serves as a foundation for inference. Statistical pragmatism is inclusive and emphasizes the assumptions that connect statistical models with observed data. I argue that introductory courses often mischaracterize the process of statistical inference and I propose an alternative "big picture" depiction.},
archivePrefix = {arXiv},
arxivId = {1106.2895},
author = {Kass, Robert E.},
doi = {10.1214/10-STS337},
eprint = {1106.2895},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/bigpic.pdf:pdf},
isbn = {0883-4237 (Print)$\backslash$r0883-4237 (Linking)},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Bayesian, confidence, frequentist, statistical edu,and phrases,bayesian,cation,confidence,frequentist,statistical edu-,statistical pragmatism,statistical significance},
number = {1},
pages = {1--9},
pmid = {21841892},
title = {{Statistical Inference: The Big Picture}},
url = {http://projecteuclid.org/euclid.ss/1307626554},
volume = {26},
year = {2011}
}
@inproceedings{Ben-David2008,
author = {Ben-David, Shai and Lu, Tyler and P{\'{a}}l, David},
booktitle = {Proceedings of the 21st Annual Conference on Learning Theory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-David, Lu, P{\'{a}}l - 2008 - Does Unlabeled Data Provably Help Worst-case Analysis of the Sample Complexity of Semi-Supervised Learning.pdf:pdf},
pages = {33--44},
title = {{Does Unlabeled Data Provably Help? Worst-case Analysis of the Sample Complexity of Semi-Supervised Learning.}},
year = {2008}
}
@article{Munafo2017,
author = {Munaf{\`{o}}, Marcus R and Nosek, Brian A and Bishop, Dorothy V M and Button, Katherine S and Chambers, Christopher D and Percie, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-jan},
doi = {10.1038/s41562-016-0021},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/2017{\_}NatureHumanBehaviour{\_}Munafo{\_}Ioannidis{\_}ManifestoReproducibleScience.pdf:pdf},
isbn = {4156201600},
issn = {2397-3374},
journal = {Nature Publishing Group},
number = {January},
pages = {1--9},
publisher = {Macmillan Publishers Limited},
title = {{A manifesto for reproducible science}},
url = {http://dx.doi.org/10.1038/s41562-016-0021},
volume = {1},
year = {2017}
}
@article{Tibshirani1996,
author = {Tibshirani, Robert},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tibshirani - 1996 - Regression shrinkage and selection via the lasso.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B},
keywords = {quadratic programming,regression,shrinkage,subset selection},
number = {1},
pages = {267--288},
title = {{Regression shrinkage and selection via the lasso}},
volume = {58},
year = {1996}
}
@inproceedings{Loog2010,
author = {Loog, Marco},
booktitle = {Machine Learning and Knowledge Discovery in Databases (Lecture Notes in Computer Science Volume 6322)},
doi = {10.1007/978-3-642-15883-4_19},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - 2010 - Constrained Parameter Estimation for Semi-Supervised Learning The Case of the Nearest Mean Classifier.pdf:pdf},
pages = {291--304},
publisher = {Springer},
title = {{Constrained Parameter Estimation for Semi-Supervised Learning: The Case of the Nearest Mean Classifier}},
year = {2010}
}
@article{Herndon2014,
author = {Herndon, Thomas and Ash, Michael and Pollin, Robert},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Herndon, Ash, Pollin - 2014 - Does High Public Debt Consistently Stifle Economic Growth A Critique of Reinhart and Rogoff.pdf:pdf},
journal = {Cambridge Journal of Economics},
number = {2},
pages = {257--279},
title = {{Does High Public Debt Consistently Stifle Economic Growth? A Critique of Reinhart and Rogoff}},
volume = {38},
year = {2014}
}
@article{Welinder2013,
author = {Welinder, Peter and Welling, Max and Perona, Pietro},
doi = {10.1109/CVPR.2013.419},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Welinder, Welling, Perona - 2013 - A Lazy Man's Approach to Benchmarking Semisupervised Classifier Evaluation and Recalibration.pdf:pdf},
isbn = {978-0-7695-4989-7},
journal = {2013 IEEE Conference on Computer Vision and Pattern Recognition},
month = {jun},
pages = {3262--3269},
publisher = {Ieee},
title = {{A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6619263},
year = {2013}
}
@article{Shaffer1991,
author = {Shaffer, Juliet Popper},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shaffer - 1991 - The Gauss-Markov Theorem and Random Regressors.pdf:pdf},
journal = {The American Statistician},
keywords = {best linear unbiased estimators,finite-,linear regression,population sampling,unbiased esti-},
number = {4},
pages = {269--273},
title = {{The Gauss-Markov Theorem and Random Regressors}},
volume = {45},
year = {1991}
}
@inproceedings{Chang2007,
author = {Chang, Ming-wei and Ratinov, Lev and Roth, Dan},
booktitle = {Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chang, Ratinov, Roth - 2007 - Guiding Semi-Supervision with Constraint-Driven Learning.pdf:pdf},
pages = {280--287},
title = {{Guiding Semi-Supervision with Constraint-Driven Learning}},
year = {2007}
}
@article{Doiron2017a,
author = {Doiron, Maxime and Langlois, M{\'{e}}lanie and Dupr{\'{e}}, Nicolas and Simard, Martine},
doi = {10.1002/gps.4735},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Doiron{\_}2017{\_}The influence of vascular risk factors on cognitive function in early Parkinson's disease(2).pdf:pdf},
issn = {10991166},
journal = {International Journal of Geriatric Psychiatry},
keywords = {Cognition,Dementia,Hypertension,Metabolic syndrome,Parkinson,Pulse pressure,Vascular risk factors},
pmid = {28509343},
title = {{The influence of vascular risk factors on cognitive function in early Parkinson's disease}},
year = {2017}
}
@inproceedings{Hoekstra1996,
author = {Hoekstra, Aarnoud and Duin, Robert P.W.},
booktitle = {Proceedings of the 13th International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hoekstra, Duin - 1996 - On the nonlinearity of pattern classifiers.pdf:pdf},
pages = {271--275},
title = {{On the nonlinearity of pattern classifiers}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=547429},
year = {1996}
}
@article{Hong2015a,
author = {Hong, Yi and Zhu, Weiping},
doi = {10.1016/j.patrec.2015.06.017},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hong, Zhu - 2015 - Spatial Co-Training for Semi-Supervised Image Classification.pdf:pdf;:Users/jkrijthe/Documents/Mendeley Desktop/Hong, Zhu - 2015 - Spatial Co-Training for Semi-Supervised Image Classification(2).pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Co-training,Image classif,Image classification,Semi-supervised learning},
pages = {59--65},
publisher = {Elsevier Ltd.},
title = {{Spatial Co-Training for Semi-Supervised Image Classification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865515001816},
volume = {63},
year = {2015}
}
@article{Ye2007a,
address = {New York, New York, USA},
author = {Ye, Jieping},
doi = {10.1145/1273496.1273633},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ye - 2007 - Least squares linear discriminant analysis.pdf:pdf},
isbn = {9781595937933},
journal = {Proceedings of the 24th International Conference on Machine Learning},
keywords = {18,3,8,are linear combinations of,class separability,derived features in lda,dimension reduction,least squares,linear discriminant anal-,linear regression,the,the data achieves maximum,the orig-,ysis},
pages = {1087--1093},
publisher = {ACM Press},
title = {{Least squares linear discriminant analysis}},
url = {http://portal.acm.org/citation.cfm?doid=1273496.1273633},
year = {2007}
}
@book{Shalev-Shwartz2014,
author = {Shalev-Shwartz, Shai and Ben-David, Shai},
publisher = {Cambridge University Press},
title = {{Understanding Machine Learning}},
year = {2014}
}
@article{King1995,
author = {King, R.D. and Feng, C and Sutherland, A},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/King, Feng, Sutherland - 1995 - Statlog comparison of classification algorithms on large real-world problems.pdf:pdf},
journal = {Applied Artificial Intelligence an International Journal},
number = {3},
pages = {289--333},
title = {{Statlog: comparison of classification algorithms on large real-world problems}},
url = {http://www.tandfonline.com/doi/abs/10.1080/08839519508945477},
volume = {9},
year = {1995}
}
@inproceedings{Hajizadeh2016,
author = {Hajizadeh, Siamak and N{\'{u}}{\~{n}}ez, Alfredo and Tax, David M.J.},
booktitle = {Proceedings of the 14th IFAC Symposium on Control in Transportation Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hajizadeh, N{\'{u}}{\~{n}}ez, Tax - 2016 - Semi-supervised Rail Defect Detection from Imbalanced Image Data.pdf:pdf},
keywords = {imbalance},
title = {{Semi-supervised Rail Defect Detection from Imbalanced Image Data}},
year = {2016}
}
@article{Dhillon2013,
author = {Dhillon, Paramveer S. and Foster, Dean P. and Kakade, Sham M. and Ungar, Lyle H.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dhillon et al. - 2013 - A Risk Comparison of Ordinary Least Squares vs Ridge Regression.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {pca,ridge regression,risk inflation},
pages = {1505--1511},
title = {{A Risk Comparison of Ordinary Least Squares vs Ridge Regression}},
url = {http://adsabs.harvard.edu/abs/2011arXiv1105.0875D},
volume = {14},
year = {2013}
}
@article{Loog2014a,
author = {Loog, Marco},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - 2014 - Semi-supervised linear discriminant analysis through moment-constraint parameter estimation.pdf:pdf},
journal = {Pattern Recognition Letters},
keywords = {linear discriminant analysis,semi-supervised learning},
pages = {24--31},
publisher = {Elsevier},
title = {{Semi-supervised linear discriminant analysis through moment-constraint parameter estimation}},
volume = {37},
year = {2014}
}
@article{Brown2015,
author = {Brown, Gavin},
doi = {10.1016/j.patrec.2015.04.014},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brown - 2015 - On Unifiers, Diversifiers, and the Nature of Pattern Recognition.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Nature of pattern recognition,Unifying,Diversifyin,nature of pattern recognition},
pages = {1--10},
publisher = {Elsevier Ltd.},
title = {{On Unifiers, Diversifiers, and the Nature of Pattern Recognition}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865515001312},
volume = {000},
year = {2015}
}
@article{Wallace2016,
author = {Wallace, Byron C and Marshall, Iain J},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wallace, Marshall - 2016 - Extracting PICO Sentences from Clinical Trial Reports using Supervised Distant Supervision.pdf:pdf},
pages = {1--25},
title = {{Extracting PICO Sentences from Clinical Trial Reports using Supervised Distant Supervision}},
volume = {17},
year = {2016}
}
@article{Nakamoto2008,
abstract = {A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution. Digital signatures provide part of the solution, but the main benefits are lost if a trusted third party is still required to prevent double-spending. We propose a solution to the double-spending problem using a peer-to-peer network. The network timestamps transactions by hashing them into an ongoing chain of hash-based proof-of-work, forming a record that cannot be changed without redoing the proof-of-work. The longest chain not only serves as proof of the sequence of events witnessed, but proof that it came from the largest pool of CPU power. As long as a majority of CPU power is controlled by nodes that are not cooperating to attack the network, they'll generate the longest chain and outpace attackers. The network itself requires minimal structure. Messages are broadcast on a best effort basis, and nodes can leave and rejoin the network at will, accepting the longest proof-of-work chain as proof of what happened while they were gone.},
author = {Nakamoto, Satoshi},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nakamoto - 2008 - Bitcoin A Peer-to-Peer Electronic Cash System.pdf:pdf},
pages = {1--9},
title = {{Bitcoin: A Peer-to-Peer Electronic Cash System}},
year = {2008}
}
@phdthesis{Hamers2012,
author = {Hamers, Adrian},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hamers - 2012 - The Evolution of Coeval Stellar Hierarchical Triple Systems.pdf:pdf},
school = {Utrecht University},
title = {{The Evolution of Coeval Stellar Hierarchical Triple Systems}},
year = {2012}
}
@inproceedings{Zhou2007a,
author = {Zhou, Zhi-hua and Xu, Jun-Ming},
booktitle = {Proceedings of the 24th International Conference on Machine learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Xu - 2007 - On the relation between multi-instance learning and semi-supervised learning.pdf:pdf},
number = {1997},
title = {{On the relation between multi-instance learning and semi-supervised learning}},
url = {http://dl.acm.org/citation.cfm?id=1273643},
year = {2007}
}
@article{Petris2010,
abstract = {Abstract We describe an R package focused on Bayesian analysis of dynamic linear models. The main features of the package are its flexibility to deal with a variety of constant or time-varying, univariate or multivariate models, and the numerically stable singular ... $\backslash$n},
archivePrefix = {arXiv},
arxivId = {arXiv:1501.0228},
author = {Petris, Giovanni},
doi = {10.1007/BF02887432},
eprint = {arXiv:1501.0228},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/v36i12.pdf:pdf},
isbn = {1548-7660},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {bayesian,forward filtering backward sampling,kalman filter,state space models},
number = {12},
pages = {1--16},
pmid = {18291371},
title = {{An R Package for Dynamic Linear Models}},
url = {http://www.jstatsoft.org/v36/i12/paper{\%}5Cnpapers2://publication/uuid/CBB963FA-1862-4D3C-BE12-8DD58B11DC28},
volume = {36},
year = {2010}
}
@unpublished{DeDeo2014,
abstract = {A recurring problem with statistical prediction for policy-making is that many useful variables are associated with others on which it would be ethically problematic to base decisions. This problem becomes particularly acute in the Big Data era, when predictions are often made in the absence of strong theories for the underlying causal mechanisms. Given this, we show how to use information theory to construct the distribution closest in predictive power to the full distribution, but in which predictions---and thus policy outcomes, provision of services, and so forth---are not correlated with protected variables.},
archivePrefix = {arXiv},
arxivId = {1412.4643},
author = {DeDeo, Simon},
eprint = {1412.4643},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/DeDeo - 2014 - Wrong side of the tracks Big Data and Protected Categories.pdf:pdf},
month = {dec},
pages = {3},
title = {{"Wrong side of the tracks": Big Data and Protected Categories}},
url = {http://arxiv.org/abs/1412.4643},
year = {2014}
}
@article{Culp2013,
abstract = {The cluster assumption had a significant impact on the reasoning behind semi-supervised classification methods in graph-based learning. The literature includes numerous applications where harmonic functions provided estimates that conformed to data satisfying this well-known assumption, but the relationship between this assumption and harmonic functions is not as well-understood theoretically. We investigate these matters from the perspective of supervised kernel classification and provide concrete answers to two fundamental questions. (i) Under what conditions do semi-supervised harmonic approaches satisfy this assumption? (ii) If such an assumption is satisfied then why precisely would an observation sacrifice its own supervised estimate in favor of the cluster? First, a harmonic function is guaranteed to assign labels to data in harmony with the cluster assumption if a specific condition on the boundary of the harmonic function is satisfied. Second, it is shown that any harmonic function estimate within the interior is a probability weighted average of supervised estimates, where the weight is focused on supervised kernel estimates near labeled cases. We demonstrate that the uniqueness criterion for harmonic estimators is sensitive when the graph is sparse or the size of the boundary is relatively small. This sets the stage for a third contribution, a new regularized joint harmonic function for semi-supervised learning based on a joint optimization criterion. Mathematical properties of this estimator, such as its uniqueness even when the graph is sparse or the size of the boundary is relatively small, are proven. A main selling point is its ability to operate in circumstances where the cluster assumption may not be fully satisfied on real data by compromising between the purely harmonic and purely supervised estimators. The competitive stature of the new regularized joint harmonic approach is established.},
author = {Culp, Mark Vere and Ryan, K J},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Culp, Ryan - 2013 - Joint Harmonic Functions and Their Supervised Connections.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {cluster assumption,harmonic function,joint training,semi-supervised learning},
pages = {3721--3752},
title = {{Joint Harmonic Functions and Their Supervised Connections}},
volume = {14},
year = {2013}
}
@article{Xue2015,
author = {Xue, Jing-hao and Hall, Peter},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Xue, Hall - 2015 - Why Does Rebalancing Class-Unbalanced Data Improve AUC for Linear Discriminant Analysis.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {5},
pages = {1109--1112},
title = {{Why Does Rebalancing Class-Unbalanced Data Improve AUC for Linear Discriminant Analysis?}},
volume = {37},
year = {2015}
}
@article{Hoogerbrugge1983,
author = {Hoogerbrugge, Ronald and Willig, Simon J. and Kistemaker, Piet G.},
doi = {10.1021/ac00261a016},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hoogerbrugge, Willig, Kistemaker - 1983 - Discriminant analysis by double stage principal component analysis.pdf:pdf},
issn = {0003-2700},
journal = {Analytical Chemistry},
month = {sep},
number = {11},
pages = {1710--1712},
title = {{Discriminant analysis by double stage principal component analysis}},
url = {http://pubs.acs.org/doi/abs/10.1021/ac00261a016},
volume = {55},
year = {1983}
}
@article{Gomez-Chova2008,
abstract = {This letter presents a semisupervised method based on kernel machines and graph theory for remote sensing image classification. The support vector machine (SVM) is regularized with the unnormalized graph Laplacian, thus leading to the Laplacian SVM (LapSVM). The method is tested in the challenging problems of urban monitoring and cloud screening, in which an adequate exploitation of the wealth of unlabeled samples is critical. Results obtained using different sensors, and with low number of training samples, demonstrate the potential of the proposed LapSVM for remote sensing image classification.},
author = {G{\'{o}}mez-Chova, Luis and Camps-Valls, Gustavo and Mu{\~{n}}oz-Mari, Jordi and Calpe, Javier},
doi = {10.1109/LGRS.2008.916070},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/G{\'{o}}mez-Chova et al. - 2008 - Semisupervised image classification with Laplacian support vector machines.pdf:pdf},
journal = {IEEE Geoscience and Remote Sensing Letters},
keywords = {Kernel methods,Manifold learning,Regularization,Semisupervised learning (SSL),Support vector machines (SVMs)},
number = {3},
pages = {336--340},
title = {{Semisupervised image classification with Laplacian support vector machines}},
volume = {5},
year = {2008}
}
@article{Gama1995,
author = {Gama, Joao and Brazdil, Pavel B.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gama, Brazdil - 1995 - Characterization of Classification Algorithms.pdf:pdf},
journal = {Progress in Artificial Intelligence},
pages = {189--200},
title = {{Characterization of Classification Algorithms}},
url = {http://link.springer.com/chapter/10.1007/3-540-60428-6{\_}16},
year = {1995}
}
@article{Yu2016,
author = {Yu, Hongbin and Lu, Hongtao},
doi = {10.1016/j.patcog.2016.07.005},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yu, Lu - 2016 - Orthogonal optimal reverse prediction for semi-supervised learning.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Optimal reverse prediction,Orthogonal constraint,Orthogonal optimal reverse prediction,Semi-supervised learning,optimal reverse prediction,orthogonal constraint,semi-supervised learning},
pages = {908--920},
publisher = {Elsevier},
title = {{Orthogonal optimal reverse prediction for semi-supervised learning}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0031320316301522},
volume = {60},
year = {2016}
}
@misc{Box1966,
author = {Box, George E P},
booktitle = {Technometrics},
doi = {10.1080/00401706.1966.10490407},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/BoxUseAbuse.pdf:pdf},
isbn = {0828520550},
issn = {0040-1706},
pages = {625--629},
pmid = {18934160},
title = {{Use and abuse of regression}},
volume = {8},
year = {1966}
}
@article{Forestier2007,
author = {Forestier, Germain and Wemmert, C{\'{e}}dric},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Forestier, Wemmert - 2016 - Semi-supervised learning using multiple clusterings with limited labeled data.pdf:pdf},
journal = {Information Sciences},
keywords = {semi-supervised learning},
pages = {48--65},
title = {{Semi-supervised learning using multiple clusterings with limited labeled data}},
volume = {361},
year = {2016}
}
@book{Zhu2009,
author = {Zhu, Xiaojin and Goldberg, Andrew B.},
editor = {Brachman, Ronald J. and Dietterich, Thomas},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhu, Goldberg - 2009 - Introduction to Semi-Supervised Learning.pdf:pdf},
isbn = {9781598295474},
pages = {1--130},
publisher = {Morgan {\&} Claypool},
title = {{Introduction to Semi-Supervised Learning}},
year = {2009}
}
@article{Robert2016,
abstract = {This note is made of comments on Watson and Holmes (2016) and about their proposals towards more robust decisions. While we acknowledge and commend the authors for setting new and all-encompassing principles of Bayesian robustness, we remain uncertain as to which extent such principles can be applied outside binary decision. We also wonder at the ultimate relevance of Kullback-Leibler neighbourhoods to characterise robustness.},
archivePrefix = {arXiv},
arxivId = {1603.09088},
author = {Robert, Christian P. and Rousseau, Judith},
eprint = {1603.09088},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Robert, Rousseau - 2016 - Some comments about James Watson's and Chris Holmes' Approximate Models and Robust Decisions.pdf:pdf},
keywords = {1,and phrases,decision-theory,decision-theory, prior selection, robust methodolo,first-hand,introduction,misspecification,ology,prior selection,robust method-,there is nothing like},
pages = {1--7},
title = {{Some comments about James Watson's and Chris Holmes' "Approximate Models and Robust Decisions"}},
url = {http://arxiv.org/abs/1603.09088},
year = {2016}
}
@article{Hernandez-Orallo2012,
abstract = {Many performance metrics have been introduced in the literature for the evaluation of classification performance, each of them with different origins and areas of application. These metrics include accuracy, unweighted accuracy, the area under the ROC curve or the ROC convex hull, the mean absolute error and the Brier score or mean squared error (with its decomposition into refinement and calibration). One way of understanding the relations among these metrics is by means of variable operating conditions (in the form of misclassification costs and/or class distributions). Thus, a metric may correspond to some expected loss over different operating conditions. One dimension for the analysis has been the distribution for this range of operating conditions, leading to some important connections in the area of proper scoring rules. We demonstrate in this paper that there is an equally important dimension which has so far received much less attention in the analysis of performance metrics. This dimension is given by the decision rule, which is typically implemented as a threshold choice method when using scoring models. In this paper, we explore many old and new threshold choice methods: fixed, score-uniform, score-driven, rate-driven and optimal, among others. By calculating the expected loss obtained with these threshold choice methods for a uniform range of operating conditions we give clear interpretations of the 0-1 loss, the absolute error, the Brier score, the AUC and the refinement loss respectively. Our analysis provides a comprehensive view of performance metrics as well as a systematic approach to loss minimisation which can be summarised as follows: given a model, apply the threshold choice methods that correspond with the available information about the operating condition, and compare their expected losses. In order to assist in this procedure we also derive several connections between the aforementioned performance metrics, and we highlight the role of calibration in choosing the threshold choice method. {\textcopyright} 2012 Jose Hernandez-Orallo, Peter Flach and Cesar Ferri.},
author = {Hern{\'{a}}ndez-Orallo, J and Flach, Peter and Ferri, C},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/hernandez-orallo12a.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {The Journal of Machine Learning Research},
pages = {2813--2869},
title = {{A unified view of performance metrics: translating threshold choice into expected classification loss}},
url = {http://dl.acm.org/citation.cfm?id=2503332},
volume = {13},
year = {2012}
}
@inproceedings{Cortes2011,
author = {Cortes, Corinna and Mohri, Mehryar},
booktitle = {Algorithmic Learning Theory (Lecture Notes in Computer Science Volume 6925)},
doi = {10.1007/978-3-642-24412-4_25},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cortes, Mohri - 2011 - Domain adaptation in regression.pdf:pdf},
pages = {308--323},
title = {{Domain adaptation in regression}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-24412-4{\_}25},
year = {2011}
}
@article{Zhang2017,
archivePrefix = {arXiv},
arxivId = {1611.03530},
author = {Zhang, Chiyuan and Recht, Benjamin and Bengio, Samy and Brain, Google and Vinyals, Oriol and Deepmind, Google and Hardt, Moritz and Brain, Google},
eprint = {1611.03530},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/a667dbd533e9f018c023e21d1e3efd86cd61c365.pdf:pdf},
pages = {1--15},
title = {{Nderstanding deep learning requires re - thinking generalization}},
year = {2017}
}
@inproceedings{Chaudhuri2009,
author = {Chaudhuri, Kamalika and Monteleoni, Claire},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chaudhuri, Monteleoni - 2009 - Privacy-Preserving Logistic Regression.pdf:pdf},
pages = {289--296},
title = {{Privacy-Preserving Logistic Regression}},
url = {https://papers.nips.cc/paper/3486-privacy-preserving-logistic-regression.pdf},
year = {2009}
}
@book{Kuncheva2004,
author = {Kuncheva, Ludmila I},
booktitle = {Methods and Algorithms. Wiley, Chichester},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kuncheva - 2004 - Combining Pattern Classifers.pdf:pdf},
isbn = {9786468600},
title = {{Combining Pattern Classifers}},
url = {http://www.tandfonline.com/doi/abs/10.1198/tech.2005.s320 http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Combining+Pattern+Classifiers{\#}3},
year = {2004}
}
@article{Gelman2010,
abstract = {We review some approaches and philosophies of causal inference coming from sociology, economics, computer science, cognitive science, and statistics},
archivePrefix = {arXiv},
arxivId = {1003.2619},
author = {Gelman, Andrew},
doi = {10.1086/662659},
eprint = {1003.2619},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman - 2010 - Causality and Statistical Learning.pdf:pdf},
issn = {00029602},
journal = {American Journal of Sociology},
number = {3},
pages = {955--966},
title = {{Causality and Statistical Learning}},
url = {http://arxiv.org/abs/1003.2619},
volume = {117},
year = {2010}
}
@inproceedings{Pfahringer2000,
author = {Pfahringer, Bernhard and Giraud-carrier, Christophe},
booktitle = {Proceedings of the 17th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pfahringer, Giraud-carrier - 2000 - Meta-Learning by Landmarking Various Learning Algorithms.pdf:pdf},
pages = {743--750},
title = {{Meta-Learning by Landmarking Various Learning Algorithms}},
year = {2000}
}
@book{Bertsekas1982,
author = {Bertsekas, Dimitri P.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bertsekas - 1982 - Constrained optimization and Lagrange multiplier methods.pdf:pdf},
publisher = {Academic Press},
title = {{Constrained optimization and Lagrange multiplier methods}},
url = {http://adsabs.harvard.edu/abs/1982colm.book.....b},
year = {1982}
}
@article{Healy1956,
author = {Healy, Michael and Westmacott, Michael},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Healy, Westmacott - 1956 - Missing Values in Experiments Analysed on Automatic Computers.pdf:pdf},
journal = {Journal of the Royal Statistical Society},
number = {3},
pages = {203--206},
title = {{Missing Values in Experiments Analysed on Automatic Computers}},
volume = {5},
year = {1956}
}
@article{Hartley1968b,
author = {Hartley, H.O. and Rao, J.N.K.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hartley, Rao - 1968 - Classification and Estimation in Analysis of Variance Problems.pdf:pdf},
journal = {Review of the International Statistical Institute},
number = {2},
pages = {141--147},
title = {{Classification and Estimation in Analysis of Variance Problems}},
volume = {36},
year = {1968}
}
@article{Mayo2011,
abstract = {Error statistics has a dual dimension involving philosophy and methodology. It refers to a standpoint regarding both: a cluster of statistical tools, their interpretation and justification; and a general philosophy of science, and the roles probability plays in inductive inference. Comparison of error statistical approach to other philosophies of statistics requires understanding the complex interconnections between the methodological and philosophical dimensions. This chapter focuses on the error statistical philosophy and concerns the interplay between the philosophical, methodological, and statistical issues, to elucidate long-standing conceptual, technical, and epistemological debates surrounding both dimensions. Standard explications of statistical methods focus on the formal mathematical tools without considering a general philosophy of science or of induction in which these tools best fit. Finally, this chapter deals with statistical methods of testing and estimation while simultaneously highlighting classic misunderstandings and fallacies. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Mayo, Deborah G. and Spanos, Aris},
doi = {10.1016/B978-0-444-51862-0.50005-8},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Error{\_}Statistics{\_}2011.pdf:pdf},
isbn = {9780444518620},
issn = {0080930964},
journal = {Philosophy of Statistics},
pages = {153--198},
pmid = {19589158},
title = {{Error Statistics}},
volume = {7},
year = {2011}
}
@article{Shipp2001,
author = {Shipp, Catherine A. and Kuncheva, Ludmila I},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shipp, Kuncheva - 2001 - Four Measures of Data Complexity for Bootstrapping, Splitting and Feature Sampling.pdf:pdf},
isbn = {0000000000000},
journal = {Proc. CIMA},
title = {{Four Measures of Data Complexity for Bootstrapping, Splitting and Feature Sampling}},
url = {http://www.bangor.ac.uk/{~}mas00a/papers/cslkAIDA01.pdf},
year = {2001}
}
@inproceedings{Giraud-Carrier2005,
author = {Giraud-carrier, Christophe and Provost, Foster},
booktitle = {In Proceedings of the ICML-2005 Workshop on Meta-learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Giraud-carrier, Provost - 2005 - Toward a justification of meta-learning Is the no free lunch theorem a show-stopper.pdf:pdf},
pages = {12--19},
title = {{Toward a justification of meta-learning: Is the no free lunch theorem a show-stopper}},
url = {http://dml.cs.byu.edu/{~}cgc/pubs/ICML2005WS.pdf},
year = {2005}
}
@incollection{Hansen2009,
author = {Hansen, Lars Kai},
booktitle = {Dataset shift in Machine Learning},
chapter = {4},
editor = {Quinonero-Candela, Joaquin and Sugiyama, Masashi and Schwaighofer, Anton and Lawrence, Neil D.},
pages = {65--72},
publisher = {MIT Press},
title = {{On Bayesian Transduction: Implications for the Covariate Shift Problem}},
year = {2009}
}
@article{Bareinboim2016,
abstract = {The exponential growth of electronically accessible information has led some to conjecture that data alone can replace sub- stantive knowledge in practical decision making and scientific explorations. In this paper, we argue that traditional scientific methodologies that have been successful in the natural and bio- medical sciences would still be necessary for big data applications, albeit tasked with new challenges: to go beyond predictions and, using information from multiple sources, provide users with rea- soned recommendations for actions and policies. The feasibility of meeting these challenges is demonstrated here using specific data fusion tasks, following a brief introduction to the structural causal model (SCM) framework (1–3).},
author = {Bareinboim, Elias and Pearl, Judea},
doi = {10.1073/pnas.1510507113},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/r450-reprint.pdf:pdf},
isbn = {10916490 (Electronic)},
issn = {0027-8424},
journal = {Pnas},
number = {27},
pages = {7345--7352},
pmid = {27382148},
title = {{Causal inference and the data-fusion problem}},
url = {www.pnas.org/cgi/doi/10.1073/pnas.1510507113},
volume = {113},
year = {2016}
}
@book{Webb2002,
author = {Webb, Andrew},
edition = {2},
publisher = {John Wiley {\&} Sons},
title = {{Statistical Pattern Recognition}},
year = {2002}
}
@article{Meng2014,
abstract = {Statistical inference is a field full of problems whose solutions require the same intellectual force needed to win a Nobel Prize in other scientific fields. Multi- resolution inference is the oldest of the trio. But emerging applications such as individualized medicine have challenged us to the limit: Infer estimands with resolution levels that far exceed those of any feasible estimator. Multi-phase inference is another reality because (big) data are almost never collected, processed, and analyzed in a single phase. The newest of the trio is multi- source inference, which aims to extract information in data coming from very different sources, some of which were never intended for inference purposes. All of these challenges call for an expanded paradigm with greater emphases on qualitative consistency and relative optimality than do our current inference paradigms.},
author = {Meng, Xiao-Li},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/COPSS{\_}50.pdf:pdf},
journal = {Past, Present, and Future of Statistical Science},
pages = {537--562},
title = {{A trio of inference problems that could win you a Nobel Prize in statistics (if you help fund it)}},
url = {http://www.stat.harvard.edu/Faculty{\_}Content/meng/COPSS{\_}50.pdf{\%}5Cnpapers2://publication/uuid/EF02EDBC-654A-4EB5-A617-9CB4504C714D},
year = {2014}
}
@inproceedings{Solomon2014,
abstract = {Probability distributions and histograms are nat-ural representations for product ratings, traffic measurements, and other data considered in many machine learning applications. Thus, this pa-per introduces a technique for graph-based semi-supervised learning of histograms, derived from the theory of optimal transportation. Our method has several properties making it suitable for this application; in particular, its behavior can be char-acterized by the moments and shapes of the his-tograms at the labeled nodes. In addition, it can be used for histograms on non-standard domains like circles, revealing a strategy for manifold-valued semi-supervised learning. We also extend this technique to related problems such as smoothing distributions on graph nodes.},
author = {Solomon, Justin and Rustamov, Raif M. and Guibas, Leonidas and Butscher, Adrian},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Solomon et al. - 2014 - Wasserstein Propagation for Semi-Supervised Learning.pdf:pdf},
pages = {306--314},
title = {{Wasserstein Propagation for Semi-Supervised Learning}},
year = {2014}
}
@article{Shiao2014,
author = {Shiao, Han-Tai and Cherkassky, Vladimir},
doi = {10.1109/IJCNN.2014.6889517},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shiao, Cherkassky - 2014 - Learning using privileged information (LUPI) for modeling survival data.pdf:pdf},
isbn = {978-1-4799-1484-5},
journal = {International Joint Conference on Neural Networks},
month = {jul},
pages = {1042--1049},
publisher = {Ieee},
title = {{Learning using privileged information (LUPI) for modeling survival data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6889517},
year = {2014}
}
@article{Buhlmann2014a,
author = {Buhlmann, Peter and Meier, Lukas and van de Geer, Sara},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Buhlmann, Meier, van de Geer - 2014 - Discussion “a significance test for the lasso”.pdf:pdf},
journal = {The Annals of Statistics},
number = {2},
pages = {469--477},
title = {{Discussion: “a significance test for the lasso”}},
volume = {42},
year = {2014}
}
@article{Vanschoren2012,
author = {Vanschoren, Joaquin and Blockeel, Hendrik and Pfahringer, Bernhard and Holmes, Geoffrey},
doi = {10.1007/s10994-011-5277-0},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vanschoren et al. - 2012 - Experiment databases.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = {jan},
number = {2},
pages = {127--158},
title = {{Experiment databases}},
url = {http://www.springerlink.com/index/10.1007/s10994-011-5277-0},
volume = {87},
year = {2012}
}
@article{Gelman2015,
abstract = {We argue that the words " objectivity " and " subjectivity " in statistics discourse are used in a mostly unhelpful way, and we propose to replace each of them with broader collections of attributes, with objectivity replaced by transparency, consensus, impartiality, and correspon-dence to observable reality, and subjectivity replaced by awareness of multiple perspectives and context dependence. The advantage of these reformulations is that the replacement terms do not oppose each other. Instead of debating over whether a given statistical method is subjec-tive or objective (or normatively debating the relative merits of subjectivity and objectivity in statistical practice), we can recognize desirable attributes such as transparency and acknowledg-ment of multiple perspectives as complementary goals. We demonstrate the implications of our proposal with recent applied examples from pharmacology, election polling, and socioeconomic stratification.},
archivePrefix = {arXiv},
arxivId = {arXiv:1508.05453v1},
author = {Gelman, Andrew and Hennig, Christian},
eprint = {arXiv:1508.05453v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Hennig - 2015 - Beyond subjective and objective in statistics.pdf:pdf},
keywords = {()},
number = {August},
title = {{Beyond subjective and objective in statistics}},
year = {2015}
}
@article{Lattimore2011,
archivePrefix = {arXiv},
arxivId = {1111.3846},
author = {Lattimore, Tor and Hutter, Marcus},
eprint = {1111.3846},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lattimore, Hutter - 2011 - No Free Lunch versus Occam's Razor in Supervised Learning.pdf:pdf},
journal = {arXiv preprint},
keywords = {kolmogorov complexity,no free lunch,occam,s razor,supervised learning},
title = {{No Free Lunch versus Occam's Razor in Supervised Learning}},
url = {http://arxiv.org/abs/1111.3846},
year = {2011}
}
@inproceedings{Shalev-Shwartz2007,
author = {Shalev-Shwartz, Shai and Singer, Yoram},
booktitle = {Proceedings of the 24th International Conference on Machine learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shalev-Shwartz, Singer - 2007 - Pegasos Primal estimated sub-gradient solver for svm.pdf:pdf},
pages = {807--814},
title = {{Pegasos: Primal estimated sub-gradient solver for svm}},
url = {http://link.springer.com/article/10.1007/s10107-010-0420-4},
year = {2007}
}
@article{Bengio2013,
abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
doi = {10.1109/TPAMI.2013.50},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bengio, Courville, Vincent - 2013 - Representation learning a review and new perspectives.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Artificial Intelligence: trends,Humans,Neural Networks (Computer)},
month = {aug},
number = {8},
pages = {1798--828},
pmid = {23787338},
title = {{Representation learning: a review and new perspectives.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23787338},
volume = {35},
year = {2013}
}
@article{Wickenberg-Bolin2006,
abstract = {Supervised learning for classification of cancer employs a set of design examples to learn how to discriminate between tumors. In practice it is crucial to confirm that the classifier is robust with good generalization performance to new examples, or at least that it performs better than random guessing. A suggested alternative is to obtain a confidence interval of the error rate using repeated design and test sets selected from available examples. However, it is known that even in the ideal situation of repeated designs and tests with completely novel samples in each cycle, a small test set size leads to a large bias in the estimate of the true variance between design sets. Therefore different methods for small sample performance estimation such as a recently proposed procedure called Repeated Random Sampling (RSS) is also expected to result in heavily biased estimates, which in turn translates into biased confidence intervals. Here we explore such biases and develop a refined algorithm called Repeated Independent Design and Test (RIDT).},
author = {Wickenberg-Bolin, Ulrika and G{\"{o}}ransson, Hanna and Frykn{\"{a}}s, M{\aa}rten and Gustafsson, Mats G and Isaksson, Anders},
doi = {10.1186/1471-2105-7-127},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wickenberg-Bolin et al. - 2006 - Improved variance estimation of classification performance via reduction of bias caused by small sample.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Analysis of Variance,Artificial Intelligence,Bias (Epidemiology),Diagnosis, Computer-Assisted,Diagnosis, Computer-Assisted: methods,Gene Expression Profiling,Gene Expression Profiling: methods,Humans,Models, Biological,Models, Statistical,Neoplasm Proteins,Neoplasm Proteins: analysis,Neoplasms,Neoplasms: diagnosis,Neoplasms: metabolism,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Reproducibility of Results,Sample Size,Sensitivity and Specificity,Tumor Markers, Biological,Tumor Markers, Biological: analysis},
month = {jan},
pages = {127},
pmid = {16533392},
title = {{Improved variance estimation of classification performance via reduction of bias caused by small sample size.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1435937{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {7},
year = {2006}
}
@article{Wilkinson1958,
author = {Wilkinson, G. N.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wilkinson - 1958 - Estimation of Missing Values for the Analysis of Incomplete Data.pdf:pdf},
issn = {0006-341X},
journal = {Biometrics},
number = {2},
pages = {257--286},
title = {{Estimation of Missing Values for the Analysis of Incomplete Data}},
volume = {14},
year = {1958}
}
@article{Hanczar2010a,
abstract = {The receiver operator characteristic (ROC) curves are commonly used in biomedical applications to judge the performance of a discriminant across varying decision thresholds. The estimated ROC curve depends on the true positive rate (TPR) and false positive rate (FPR), with the key metric being the area under the curve (AUC). With small samples these rates need to be estimated from the training data, so a natural question arises: How well do the estimates of the AUC, TPR and FPR compare with the true metrics?},
author = {Hanczar, Blaise and Hua, Jianping and Sima, Chao and Weinstein, John and Bittner, Michael and Dougherty, Edward R.},
doi = {10.1093/bioinformatics/btq037},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hanczar et al. - 2010 - Small-sample precision of ROC-related estimates.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,False Positive Reactions,Oligonucleotide Array Sequence Analysis,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,ROC Curve},
month = {mar},
number = {6},
pages = {822--30},
pmid = {20130029},
title = {{Small-sample precision of ROC-related estimates.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20130029},
volume = {26},
year = {2010}
}
@article{Astorino2007,
author = {Astorino, A and Fuduli, A},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Astorino, Fuduli - 2007 - Nonsmooth optimization techniques for Semi-Supervised Classification.pdf:pdf},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {nonsmooth optimization,semi,supervised learning},
number = {12},
pages = {2135--2142},
title = {{Nonsmooth optimization techniques for Semi-Supervised Classification}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4359288},
volume = {29},
year = {2007}
}
@article{Mhaskar2016,
abstract = {The paper briefy reviews several recent results on hierarchical architectures for learning from examples, that may formally explain the conditions under which Deep Convolutional Neural Networks perform much better in function approximation problems than shallow, one-hidden layer architectures. The paper announces new results for a non-smooth activation function - the ReLU function - used in present-day neural networks, as well as for the Gaussian networks. We propose a new definition of relative dimension to encapsulate different notions of sparsity of a function class that can possibly be exploited by deep networks but not by shallow ones to drastically reduce the complexity required for approximation and learning.},
archivePrefix = {arXiv},
arxivId = {1608.03287},
author = {Mhaskar, Hrushikesh and Poggio, Tomaso},
eprint = {1608.03287},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mhaskar, Poggio - 2016 - Deep vs. shallow networks An approximation theory perspective.pdf:pdf},
number = {054},
pages = {1--16},
title = {{Deep vs. shallow networks : An approximation theory perspective}},
url = {http://arxiv.org/abs/1608.03287},
year = {2016}
}
@inproceedings{Hernandez-reyes2005,
author = {Hern{\'{a}}ndez-reyes, Edith and Carrasco-Ochoa, J.A. and Mart{\'{i}}nez-trinidad, J Fco},
booktitle = {Proceedings of the 10th Iberoamerican Congress on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hern{\'{a}}ndez-reyes, Carrasco-Ochoa, Mart{\'{i}}nez-trinidad - 2005 - Classifier Selection Based on Data Complexity Measures.pdf:pdf},
pages = {586--592},
title = {{Classifier Selection Based on Data Complexity Measures}},
year = {2005}
}
@article{Schafer2002,
author = {Schafer, Joseph L. and Graham, John W.},
doi = {10.1037//1082-989X.7.2.147},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schafer, Graham - 2002 - Missing data Our view of the state of the art.pdf:pdf},
issn = {1082-989X},
journal = {Psychological Methods},
number = {2},
pages = {147--177},
title = {{Missing data: Our view of the state of the art.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/1082-989X.7.2.147},
volume = {7},
year = {2002}
}
@article{Heitjan1994,
author = {Heitjan, Daniel F and Landis, J Richard},
doi = {10.2307/2290900},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Heitjan, Landis - 1994 - Assessing Secular Trends in Blood Pressure {\{}A{\}} Multiple-imputation Approach.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {bayesian bootstrap,hot deck,incomplete data,missing data,observational study,predictive-mean matching},
number = {August 2015},
pages = {750--759},
title = {{Assessing Secular Trends in Blood Pressure: {\{}A{\}} Multiple-imputation Approach}},
volume = {89},
year = {1994}
}
@inproceedings{Corduneanu2002,
author = {Corduneanu, Adrian and Jaakkola, Tommi},
booktitle = {Proceedings of the 19th Conference on Uncertainty in Artificial Intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Corduneanu, Jaakkola - 2002 - On information regularization.pdf:pdf},
pages = {151--158},
title = {{On information regularization}},
url = {http://dl.acm.org/citation.cfm?id=2100602},
year = {2002}
}
@phdthesis{Rifkin2002,
author = {Rifkin, Ryan},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rifkin - 2002 - Everything Old Is New Again A Fresh Look at Historical Approaches in Machine Learning.pdf:pdf},
title = {{Everything Old Is New Again: A Fresh Look at Historical Approaches in Machine Learning}},
year = {2002}
}
@article{Guyon2003,
author = {Guyon, Isabelle and Elisseeff, Andre},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Guyon, Elisseeff - 2003 - An Introduction to Variable and Feature Selection.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {bioinformatics,clustering,computational biology,ery,feature selection,filters,gene expression,genomics,information retrieval,information theory,microarray,model selection,pattern discov-,proteomics,qsar,space dimensionality reduction,statistical testing,support vector machines,text classification,variable selection,wrappers},
pages = {1157--1182},
title = {{An Introduction to Variable and Feature Selection}},
volume = {3},
year = {2003}
}
@article{Hayashi2015,
author = {Hayashi, K. and Takai, K.},
doi = {10.1080/03610918.2014.957847},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hayashi, Takai - 2015 - Finite-sample analysis of impacts of unlabelled data and their labelling mechanisms in linear discriminant analy.pdf:pdf},
issn = {0361-0918},
journal = {Communications in Statistics - Simulation and Computation},
number = {June},
pages = {00--00},
title = {{Finite-sample analysis of impacts of unlabelled data and their labelling mechanisms in linear discriminant analysis}},
url = {http://www.tandfonline.com/doi/full/10.1080/03610918.2014.957847},
year = {2015}
}
@inproceedings{Ben-David2012,
author = {Ben-David, Shai and Loker, David and Srebro, Nathan and Sridharan, Karthik},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-David et al. - 2012 - Minimizing the misclassification error rate using a surrogate convex loss.pdf:pdf},
pages = {1863--1870},
title = {{Minimizing the misclassification error rate using a surrogate convex loss}},
year = {2012}
}
@article{Kullback1968,
author = {Kullback, S.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kullback - 1968 - Probability Densities with Given Marginals.pdf:pdf},
journal = {The Annals of Mathematical Statistics},
number = {4},
pages = {1236--1243},
title = {{Probability Densities with Given Marginals}},
url = {http://www.jstor.org/stable/10.2307/2239692},
volume = {39},
year = {1968}
}
@article{Wainberg2016,
abstract = {The JMLR study Do we need hundreds of classifiers to solve real world classification problems? benchmarks 179 classifiers in 17 families on 121 data sets from the UCI repository and claims that “the random forest is clearly the best family of classifier”. In this response, we show that the study's results are biased by the lack of a held-out test set and the exclusion of trials with errors. Further, the study's own statistical tests indicate that random forests do not have significantly higher percent accuracy than support vector machines and neural networks, calling into question the conclusion that random forests are the best classifiers.},
author = {Wainberg, Michael and Alipanahi, Babak and Frey, Brendan J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wainberg, Alipanahi, Frey - 2016 - Are Random Forests Truly the Best Classifiers.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {benchmarking,classification,neural,random forests,support vector machines},
number = {110},
pages = {1--5},
title = {{Are Random Forests Truly the Best Classifiers?}},
volume = {17},
year = {2016}
}
@inproceedings{Zhang2000a,
author = {Zhang, Tong and Oles, Frank J.},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang, Oles - 2000 - A Probability Analysis on the Value of Unlabeled Data for Classification Problems.pdf:pdf},
pages = {1191--1198},
title = {{A Probability Analysis on the Value of Unlabeled Data for Classification Problems}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.20.6025{\&}rep=rep1{\&}type=pdf},
year = {2000}
}
@article{Lockhart2014f,
author = {Lockhart, Richard and Taylor, Jonathan and Tibshirani, Ryan J. and Tibshirani, Robert},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lockhart et al. - 2014 - Rejoinder A significance test for the lasso.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {apr},
number = {2},
pages = {518--531},
title = {{Rejoinder: A significance test for the lasso}},
url = {http://projecteuclid.org/euclid.aos/1400592161},
volume = {42},
year = {2014}
}
@article{Rubin2005,
abstract = {Causal effects are defined as comparisons of potential outcomes under different treatments on a common set of units. Observed values of the potential outcomes are revealed by the assignment mechanism—a probabilistic model for the treatment each unit receives as a function of covariates and potential outcomes. Fisher made tremendous contributions to causal inference through his work on the design of randomized experiments, but the potential outcomes perspective applies to other complex experiments and nonrandomized studies as well. As noted by Kempthorne in his 1976 discussion of Savage's Fisher lecture, Fisher never bridged his work on experimental design and his work on parametric modeling, a bridge that appears nearly automatic with an appropriate view of the potential outcomes framework, where the potential outcomes and covariates are given a Bayesian distribution to complete the model specification. Also, this framework crisply separates scientific inference for causal effects and decisions based on s...},
author = {Rubin, Donald B},
doi = {10.1198/016214504000001880},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rubin - 2005 - Causal Inference Using Potential Outcomes.pdf:pdf},
isbn = {0162-1459$\backslash$r1537-274X},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {analysis of covariance,assignment mechanism,assignment-based causal inference,bayesian inference,creasy,direct causal,effects,fieller,fisher,neyman,observational studies,principal stratification,randomized experiments,rubin},
number = {469},
pages = {322--331},
title = {{Causal Inference Using Potential Outcomes}},
volume = {100},
year = {2005}
}
@techreport{Seeger2001,
author = {Seeger, Matthias},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Seeger - 2001 - Learning with labeled and unlabeled data.pdf:pdf},
pages = {1--62},
title = {{Learning with labeled and unlabeled data}},
year = {2001}
}
@article{Rajput2011,
author = {Rajput, a H and Voll, a and Rajput, M L},
doi = {10.1212/WNL.0b013e3181ae7af1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rajput{\_}Neurology (2009) Course in PD subtypes(2).pdf:pdf},
title = {{clinicopathologic study Course in Parkinson disease subtypes}},
year = {2011}
}
@article{Lindner1999,
author = {Lindner, Guido and Studer, Rudi},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lindner, Studer - 1999 - AST Support for algorithm selection with a CBR approach.pdf:pdf},
journal = {Principles of Data Mining and Knowledge Discovery},
title = {{AST: Support for algorithm selection with a CBR approach}},
url = {http://www.springerlink.com/index/QBDF3R2GKVW57LUF.pdf http://link.springer.com/chapter/10.1007/978-3-540-48247-5{\_}52},
year = {1999}
}
@inproceedings{Duin2000,
author = {Duin, Robert P.W.},
booktitle = {Proceedings of the 15th International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2000.906006},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Duin - 2000 - Classifiers in Almost Empty Spaces.pdf:pdf},
pages = {1--7},
title = {{Classifiers in Almost Empty Spaces}},
year = {2000}
}
@article{Kuncheva2002,
author = {Kuncheva, Ludmila I},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kuncheva - 2002 - A Theoretical Study on Six Classifier Fusion Strategies.pdf:pdf},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {2},
pages = {281--286},
title = {{A Theoretical Study on Six Classifier Fusion Strategies}},
volume = {24},
year = {2002}
}
@inproceedings{DeBie2003,
author = {de Bie, Tijl and Cristianini, Nello},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/de Bie, Cristianini - 2003 - Convex Methods for Transduction.pdf:pdf},
title = {{Convex Methods for Transduction}},
year = {2003}
}
@article{Tian2016a,
author = {Tian, Ye and Luo, Jian},
doi = {10.1007/s00500-016-2089-y},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tian, Luo - 2016 - A new branch-and-bound approach to semi-supervised support vector machine.pdf:pdf},
isbn = {0050001620},
issn = {14337479},
journal = {Soft Computing},
keywords = {Branch-and-bound scheme,Lower bound estimator,MIQP reformulation,Soft margin {\$}{\$}$\backslash$mathrm {\{}S{\^{}}3VM{\}}{\$}{\$}S3VMmodel},
number = {2001},
pages = {1--10},
publisher = {Springer Berlin Heidelberg},
title = {{A new branch-and-bound approach to semi-supervised support vector machine}},
year = {2016}
}
@article{Chandrasekaran2013,
abstract = {Modern massive datasets create a fundamental problem at the intersection of the computational and statistical sciences: how to provide guarantees on the quality of statistical inference given bounds on computational resources, such as time or space. Our approach to this problem is to define a notion of "algorithmic weakening," in which a hierarchy of algorithms is ordered by both computational efficiency and statistical efficiency, allowing the growing strength of the data at scale to be traded off against the need for sophisticated processing. We illustrate this approach in the setting of denoising problems, using convex relaxation as the core inferential tool. Hierarchies of convex relaxations have been widely used in theoretical computer science to yield tractable approximation algorithms to many computationally intractable tasks. In the current paper, we show how to endow such hierarchies with a statistical characterization and thereby obtain concrete tradeoffs relating algorithmic runtime to amount of data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.1073v2},
author = {Chandrasekaran, Venkat and Jordan, Michael I.},
doi = {10.1073/pnas.1302293110},
eprint = {arXiv:1211.1073v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chandrasekaran, Jordan - 2013 - Computational and statistical tradeoffs via convex relaxation.pdf:pdf},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {convex geometry,convex relaxation,high-dimensional statistics,massive datasets},
month = {mar},
number = {13},
pages = {E1181--90},
pmid = {23479655},
title = {{Computational and statistical tradeoffs via convex relaxation.}},
volume = {110},
year = {2013}
}
@inproceedings{Haitao2015,
address = {Hangzhou, China},
author = {Haitao, Gan and Ming, Meng and Yuliang, Ma and Yunyuan, Gao},
booktitle = {Proceedings of the 34th Chinese Control Conference},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Haitao et al. - 2015 - A safe semi-supervised kernel minimum squared error algorithm.pdf:pdf},
keywords = {kernel minimum squared error,laplacian regularized kernel mini-,safe mechanism,semi-supervised learning},
pages = {3723--3726},
title = {{A safe semi-supervised kernel minimum squared error algorithm}},
year = {2015}
}
@article{Shi2011,
author = {Shi, Mingguang and Zhang, Bing},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shi, Zhang - 2011 - Semi-supervised learning improves gene expression-based prediction of cancer recurrence.pdf:pdf},
journal = {Bioinformatics},
number = {21},
pages = {3017--3023},
title = {{Semi-supervised learning improves gene expression-based prediction of cancer recurrence}},
volume = {27},
year = {2011}
}
@article{scikit-learn,
author = {Pedregosa, F and Varoquaux, G and Gramfort, A and Michel, V and Thirion, B and Grisel, O and Blondel, M and Prettenhofer, P and Weiss, R and Dubourg, V and Vanderplas, J and Passos, A and Cournapeau, D and Brucher, M and Perrot, M and Duchesnay, E},
journal = {Journal of Machine Learning Research},
pages = {2825--2830},
title = {{Scikit-learn: Machine Learning in Python}},
volume = {12},
year = {2011}
}
@inproceedings{Karasuyama2013,
author = {Karasuyama, Masayuki and Mamitsuka, Hiroshi},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Karasuyama, Mamitsuka - 2013 - Manifold-based Similarity Adaptation for Label Propagation.pdf:pdf},
pages = {1547--1555},
title = {{Manifold-based Similarity Adaptation for Label Propagation}},
year = {2013}
}
@inproceedings{Ho2008,
author = {Ho, Tin Kam},
booktitle = {Proceedings of the 2008 Joint IAPR International Workshop on Structural, Syntactic, and Statistical Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho - 2008 - Data complexity analysis Linkage between context and solution in classification.pdf:pdf},
pages = {986--995},
title = {{Data complexity analysis: Linkage between context and solution in classification}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-89689-0{\_}102},
year = {2008}
}
@article{Kelejian1969,
author = {Kelejian, H.H.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kelejian - 1969 - Missing Observations in Multivaraite Regression Efficiency of a First-Order Method.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {328},
pages = {1609--1616},
title = {{Missing Observations in Multivaraite Regression: Efficiency of a First-Order Method}},
volume = {64},
year = {1969}
}
@article{Wainwright2008,
author = {Wainwright, Martin J. and Jordan, Michael I.},
doi = {10.1561/2200000001},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wainwright, Jordan - 2008 - Graphical models, exponential families, and variational inference.pdf:pdf;:Users/jkrijthe/Documents/Mendeley Desktop/Wainwright, Jordan - 2008 - Graphical models, exponential families, and variational inference(2).pdf:pdf},
journal = {Foundations and Trends in Machine Learning},
pages = {1--305},
title = {{Graphical models, exponential families, and variational inference}},
url = {http://discovery.ucl.ac.uk/185880/ http://dl.acm.org/citation.cfm?id=1498841},
volume = {1},
year = {2008}
}
@article{Seaman2013,
author = {Seaman, Shaun and Galati, John and Jackson, Dan and Carlin, John},
doi = {10.1214/13-STS415},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Seaman et al. - 2013 - What Is Meant by “Missing at Random”.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Ignorability, direct-likelihood inference, frequen,and phrases,direct-likelihood inference,frequen-,ignorability,missing completely at random,repeated sampling,tist inference},
month = {may},
number = {2},
pages = {257--268},
title = {{What Is Meant by “Missing at Random”?}},
url = {http://projecteuclid.org/euclid.ss/1369147915},
volume = {28},
year = {2013}
}
@inproceedings{Walt2007,
author = {Walt, Christiaan Van Der and Barnard, Etienne},
booktitle = {18th Annual Symposium of the Pattern Recognition Association of South Africa},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Walt, Barnard - 2007 - Measures for the characterisation of pattern-recognition data sets.pdf:pdf},
title = {{Measures for the characterisation of pattern-recognition data sets}},
url = {http://researchspace.csir.co.za/dspace/handle/10204/1979},
year = {2007}
}
@article{Ahmed2016,
abstract = {Focal cortical dysplasia (FCD) is the most common cause of pediatric epilepsy and the third most common cause in adults with treatment-resistant epilepsy. Surgical resection of the lesion is the most effective treatment to stop seizures. Technical advances in MRI have revolutionized the diagnosis of FCD, leading to high success rates for resective surgery. However, 45{\%} of histologically confirmed FCD patients have normal MRIs (MRI-negative). Without a visible lesion, the success rate of surgery drops from 66{\%} to 29{\%}. In this work, we cast the problem of detecting potential FCD lesions using MRI scans of MRI-negative patients in an image segmentation framework based on hierarchical conditional random fields (HCRF). We use surface based morphometry to model the cortical surface as a two-dimensional surface which is then segmented at multiple scales to extract superpixels of different sizes. Each superpixel is assigned an outlier score by comparing it to a control c Bilal Ahmed et al..},
author = {Ahmed, Bilal and Thesen, Thomas and Blackmon, Karen E and Kuzniekcy, Ruben and Devinsky, Orrin and Brodley, Carla E},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ahmed et al. - 2016 - Decrypting Cryptogenic Epilepsy Semi-supervised Hierarchical Conditional Random Fields For Detecting Cortical Le.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {1--30},
title = {{Decrypting " Cryptogenic " Epilepsy: Semi-supervised Hierarchical Conditional Random Fields For Detecting Cortical Lesions In MRI-Negative Patients}},
volume = {17},
year = {2016}
}
@article{Mann2007,
author = {Mann, Gideon S. and McCallum, Andrew Kachites},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mann, McCallum - 2007 - Efficient computation of entropy gradient for semi-supervised conditional random fields.pdf:pdf},
journal = {Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics},
title = {{Efficient computation of entropy gradient for semi-supervised conditional random fields}},
url = {http://dl.acm.org/citation.cfm?id=1614136},
year = {2007}
}
@article{Hernandez-Gonzalez2015,
author = {Hern{\'{a}}ndez-Gonz{\'{a}}lez, Jer{\'{o}}nimo and naki Inza and Lozano, Jose A.},
doi = {10.1016/j.patrec.2015.10.008},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hern{\'{a}}ndez-Gonz{\'{a}}lez, Inza, Lozano - 2015 - Weak supervision and other non-standard classification problems a taxonomy.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {degrees of supervision,partially supervised classification,weakly supervised classification},
pages = {49--55},
title = {{Weak supervision and other non-standard classification problems: a taxonomy}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865515003505},
volume = {69},
year = {2015}
}
@article{Wang2007,
author = {Wang, Junhui and Shen, Xiaotong},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Shen - 2007 - Large margin Semi-supervised Learning.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {generalization,grouping,sequential quadratic programming,support vectors},
pages = {1867--1891},
title = {{Large margin Semi-supervised Learning}},
volume = {8},
year = {2007}
}
@article{Calandriello2016,
abstract = {While the harmonic function solution performs well in many semi-supervised learning (SSL) tasks, it is known to scale poorly with the number of samples. Recent successful and scalable methods, such as the eigenfunction method focus on efficiently approximating the whole spectrum of the graph Laplacian constructed from the data. This is in contrast to various subsampling and quantization methods proposed in the past, which may fail in preserving the graph spectra. However, the impact of the approximation of the spectrum on the final generalization error is either unknown, or requires strong assumptions on the data. In this paper, we introduce Sparse-HFS, an efficient edge-sparsification algorithm for SSL. By constructing an edge-sparse and spectrally similar graph, we are able to leverage the approximation guarantees of spectral sparsification methods to bound the generalization error of Sparse-HFS. As a result, we obtain a theoretically-grounded approximation scheme for graph-based SSL that also empirically matches the performance of known large-scale methods.},
archivePrefix = {arXiv},
arxivId = {1601.05675},
author = {Calandriello, Daniele and Lazaric, Alessandro and Valko, Michal and Koutis, Ioannis},
eprint = {1601.05675},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Calandriello et al. - 2016 - Incremental Spectral Sparsification for Large-Scale Graph-Based Semi-Supervised Learning.pdf:pdf},
title = {{Incremental Spectral Sparsification for Large-Scale Graph-Based Semi-Supervised Learning}},
url = {http://arxiv.org/abs/1601.05675},
year = {2016}
}
@article{Gelman2014,
abstract = {We revisit expectation propagation (EP) as a prototype for scalable algorithms that partition big datasets into many parts and analyze each part in parallel to perform inference of shared parameters. The algorithm should be particularly efficient for hierarchical models, for which the EP algorithm works on the shared parameters (hyperparameters) of the model. The central idea of EP is to work at each step with a "tilted distribution" that combines the likelihood for a part of the data with the "cavity distribution," which is the approximate model for the prior and all other parts of the data. EP iteratively approximates the moments of the tilted distributions and incorporates those approximations into a global posterior approximation. As such, EP can be used to divide the computation for large models into manageable sizes. The computation for each partition can be made parallel with occasional exchanging of information between processes through the global posterior approximation. Moments of multivariate tilted distributions can be approximated in various ways, including, MCMC, Laplace approximations, and importance sampling.},
archivePrefix = {arXiv},
arxivId = {1412.4869},
author = {Gelman, Andrew and Vehtari, Aki and Jyl{\"{a}}nki, Pasi and Robert, Christian and Chopin, Nicolas and Cunningham, John P.},
eprint = {1412.4869},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/1412.4869.pdf:pdf},
journal = {arXiv:stat.CO},
keywords = {bayesian computation,big data,data partitioning,expectation propagation,hierarchical models,stan,statistical computing},
pages = {19},
title = {{Expectation propagation as a way of life}},
url = {http://arxiv.org/abs/1412.4869},
year = {2014}
}
@article{Dean,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.02531v1},
author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
eprint = {arXiv:1503.02531v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hinton, Vinyals, Dean - Unknown - Distilling the Knowledge in a Neural Network.pdf:pdf},
pages = {1--9},
title = {{Distilling the Knowledge in a Neural Network}}
}
@article{Hassanzadeh,
author = {Hassanzadeh, Aidin and Kaarna, Arto and Kauranne, Tuomo},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hassanzadeh, Kaarna, Kauranne - Unknown - Outlier Robust Geodesic K-means Algorithm for High Dimensional Data.pdf:pdf},
keywords = {clustering,geodesic dis-,high-dimensional data,k-means,local outlier factor,shared nearest neighbour,tance},
pages = {1--10},
title = {{Outlier Robust Geodesic K-means Algorithm for High Dimensional Data}}
}
@article{Doksum2007,
author = {Doksum, Kjell and Ozeki, Akichika and Kim, Jihoon and {Chaibub Neto}, Elias},
doi = {10.1016/j.spl.2007.03.005},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Doksum et al. - 2007 - Thinking outside the box Statistical inference based on Kullback–Leibler empirical projections.pdf:pdf},
issn = {01677152},
journal = {Statistics {\&} Probability Letters},
keywords = {bootstrap,box-cox transformation,classification,covariate,k-l divergence,klep,outside the box,sandwich formula},
month = {jul},
number = {12},
pages = {1201--1213},
title = {{Thinking outside the box: Statistical inference based on Kullback–Leibler empirical projections}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167715207000843},
volume = {77},
year = {2007}
}
@book{Hacking2001,
author = {Hacking, Ian},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/[Hacking{\_}I.]{\_}An{\_}introduction{\_}to{\_}probability{\_}and{\_}in(b-ok.org).pdf:pdf},
title = {{An Introduction to Probability and Inductive Logic}},
year = {2001}
}
@article{Senn2013,
author = {Senn, Stephen},
doi = {10.1111/j.2044-8317.2012.02065.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Senn-2013-British{\_}Journal{\_}of{\_}Mathematical{\_}and{\_}Statistical{\_}Psychology.pdf:pdf},
isbn = {0007-1102},
issn = {00071102},
journal = {British Journal of Mathematical and Statistical Psychology},
number = {1},
pages = {65--67},
pmid = {23017009},
title = {{Comment on Gelman and Shalizi}},
volume = {66},
year = {2013}
}
@inproceedings{Chu2007,
abstract = {We are at the beginning of the multicore era. Computers will have increasingly many cores (processors), but there is still no good programming framework for these architectures, and thus no simple and unified way for machine learning to take advantage of the potential speed up. In this paper, we develop a broadly ap- plicable parallel programming method, one that is easily applied to many different learning algorithms. Our work is in distinct contrast to the tradition in machine learning of designing (often ingenious) ways to speed up a single algorithm at a time. Specifically, we show that algorithms that fit the Statistical Query model [15] can be written in a certain “summation form,” which allows them to be easily par- allelized on multicore computers. We adapt Google's map-reduce [7] paradigm to demonstrate this parallel speed up technique on a variety of learning algorithms including locally weighted linear regression (LWLR), k-means, logistic regres- sion (LR), naive Bayes (NB), SVM, ICA, PCA, gaussian discriminant analysis (GDA), EM, and backpropagation (NN). Our experimental results show basically linear speedup with an increasing number of processors.},
author = {Chu, Cheng-tao and Kim, Sang Kyun and Lin, Yi-An and Yu, YuanYuan and Bradski, Gary and Ng, Andrew Y. and Olukotun, Kunle},
booktitle = {Advances in Neural Information Processing Systems 19},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chu et al. - 2007 - Map-Reduce for Machine Learning on Multicore.pdf:pdf},
pages = {281--288},
title = {{Map-Reduce for Machine Learning on Multicore}},
year = {2007}
}
@inproceedings{Kuncheva2001,
author = {Kuncheva, Ludmila I and Roli, Fabio and Marcialis, Gian Luca and Shipp, Catherine A.},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kuncheva et al. - 2001 - Complexity of Data Subsets Generated by the Random Subspace Method An Experimental Investigation.pdf:pdf},
pages = {349--358},
title = {{Complexity of Data Subsets Generated by the Random Subspace Method: An Experimental Investigation}},
year = {2001}
}
@article{Wolpert1996a,
abstract = {This is the first of two papers that use off-training set {\{}(OTS){\}} error to investigate the assumption-free relationship between learning algorithms. This first paper discusses the senses in which there are no {\{}$\backslash$textbackslash{\}}textita priori distinctions between learning algorithms. {\{}(The{\}} second paper discusses the senses in which there are such distinctions.) In this first paper it is shown, loosely speaking, that for any two algorithms A and B, there are “as many” targets (or priors over targets) for which A has lower expected {\{}OTS{\}} error than B as vice-versa, for loss functions like zero-one loss. In particular, this is true if A is cross-validation and B is “anti-cross-validation” (choose the learning algorithm with largest cross-validation error). This paper ends with a discussion of the implications of these results for computational learning theory. It is shown that one can not say: if empirical misclassification rate is low; the {\{}Vapnik-Chervonenkis{\}} dimension of your generalizer is small; and the training set is large, then with high probability your {\{}OTS{\}} error is small. Other implications for “membership queries” algorithms and “punting” algorithms are also discussed.},
author = {Wolpert, David H},
doi = {10.1162/neco.1996.8.7.1341},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wolpert - 1996 - The Lack of A Priori Distinctions Between Learning Algorithms.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
number = {7},
pages = {1341--1390},
title = {{The Lack of A Priori Distinctions Between Learning Algorithms}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/neco.1996.8.7.1341},
volume = {8},
year = {1996}
}
@article{Blanchard2010,
author = {Blanchard, Gilles and Lee, Gyemin and Scott, Clayton},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Blanchard, Lee, Scott - 2010 - Semi-supervised novelty detection.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {2973--3009},
title = {{Semi-supervised novelty detection}},
url = {http://dl.acm.org/citation.cfm?id=1953028},
volume = {11},
year = {2010}
}
@article{Yarowsky1995,
address = {Morristown, NJ, USA},
author = {Yarowsky, David},
doi = {10.3115/981658.981684},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yarowsky - 1995 - Unsupervised word sense disambiguation rivaling supervised methods.pdf:pdf},
journal = {Proceedings of the 33rd annual meeting on Association for Computational Linguistics},
pages = {189--196},
publisher = {Association for Computational Linguistics},
title = {{Unsupervised word sense disambiguation rivaling supervised methods}},
year = {1995}
}
@unpublished{Bresson2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1210.0699v1},
author = {Bresson, Xavier and Zhang, Ruiliang},
booktitle = {arXiv preprint},
eprint = {arXiv:1210.0699v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bresson, Zhang - 2012 - TV-SVM Total Variation Support Vector Machine for Semi-Supervised Data Classification.pdf:pdf},
title = {{TV-SVM: Total Variation Support Vector Machine for Semi-Supervised Data Classification}},
url = {http://arxiv.org/abs/1210.0699},
year = {2012}
}
@article{Lin2011a,
abstract = {We design a class of submodular functions meant for document summarization tasks. These functions each combine two terms, one which encourages the summary to be representative of the corpus, and the other which positively rewards diversity. Critically, our functions are monotone nondecreasing and submodular, which means that an efficient scalable greedy optimization scheme has a constant factor guarantee of optimality. When evaluated on DUC 2004-2007 corpora, we obtain better than existing state-of-art results in both generic and query-focused document summarization. Lastly, we show that several well-established methods for document summarization correspond, in fact, to submodular function optimization, adding further evidence that submodular functions are a natural fit for document summarization.},
author = {Lin, Hui and Bilmes, Jeff},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lin, Bilmes - 2011 - A Class of Submodular Functions for Document Summarization.pdf:pdf},
isbn = {978-1-932432-87-9},
journal = {Computational Linguistics},
pages = {510--520},
title = {{A Class of Submodular Functions for Document Summarization}},
url = {http://ssli.ee.washington.edu/people/hlin/papers/lin-acl11-summ.pdf},
volume = {1},
year = {2011}
}
@article{Demsar2006,
author = {Demsar, Janez},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Demsar - 2006 - Statistical Comparisons of Classifiers over Multiple Data Sets.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {comparative studies,friedman test,multiple comparisons tests,statistical methods,wilcoxon signed ranks test},
pages = {1--30},
title = {{Statistical Comparisons of Classifiers over Multiple Data Sets}},
volume = {7},
year = {2006}
}
@book{Hastie2009,
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H.},
edition = {2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hastie, Tibshirani, Friedman - 2009 - The Elements of Statistical Learning.pdf:pdf},
isbn = {0387848576},
publisher = {Spinger},
title = {{The Elements of Statistical Learning}},
year = {2009}
}
@article{Bartlett2007,
abstract = {One of the nice properties of kernel classifiers such as SVMs is that they often produce sparse solutions. However, the decision functions of these classifiers cannot always be used to estimate the conditional probability of the class label. We investigate the relationship between these two properties and show that these are intimately related: sparseness does not occur when the conditional probabilities can be unambiguously estimated. We consider a family of convex loss functions and derive sharp asymptotic results for the fraction of data that becomes support vectors. This enables us to characterize the exact trade-off between sparseness and the ability to estimate conditional probabilities for these loss functions.},
author = {Bartlett, Peter L and Bartlett, Peter L and Tewari, Ambuj and Tewari, Ambuj},
doi = {10.1007/978-3-540-27819-1_39},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bartlett et al. - 2007 - Sparseness vs Estimating Conditional Probabilities Some Asymptotic Results.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {calibration,estimating conditional proba-,kernel methods,sparseness,support vector machines},
pages = {775--790},
title = {{Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results}},
volume = {8},
year = {2007}
}
@inproceedings{Jordan2002,
abstract = {We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widely held belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better. This stems from the observation - which is borne out in repeated experiments - that while discriminative learning has lower asymptotic error, a generative classifier may also approach its (higher) asymptotic error much faster.},
author = {Jordan, Michael I. and Ng, Andrew Y},
booktitle = {Advances in neural information processing systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jordan, Ng - 2002 - On Discriminative vs. Generative classifiers comparison of logistic regression and naive Bayes.pdf:pdf},
number = {14},
pages = {841--848},
title = {{On Discriminative vs. Generative classifiers: comparison of logistic regression and naive Bayes}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=GbC8cqxGR7YC{\&}oi=fnd{\&}pg=PA841{\&}dq=On+Discriminative+vs.+Generative+classifiers:+comparison+of+logistic+regression+and+naive+Bayes{\&}ots=ZvO0F2{\_}vx9{\&}sig=0nMLd-CWMsb8-jyrI6YetIH6ZZU},
volume = {2},
year = {2002}
}
@article{Rosenbaum2010,
abstract = {Journal of the American Statistical Association, Vol.105, No.490, 2010, 692-702},
author = {Rosenbaum, Paul R},
doi = {10.1198/jasa.2010.tm09570},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Design Sensitivity and Efficiency in Observational Studies.pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of American Statistical Association},
keywords = {1,Sensitivity analysis,introduction,motivating example,sensitivity analysis},
number = {490},
pages = {692--702},
title = {{Design Sensitivity and Efficiency in Observational Studies}},
volume = {105},
year = {2010}
}
@book{Pearl,
abstract = {Written by one of the pre-eminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, philosophy, cognitive science, and the health and social sciences. Pearl presents a unified account of the probabilistic, manipulative, counterfactual and structural approaches to causation, and devises simple mathematical tools for analyzing the relationships between causal connections, statistical associations, actions and observations. The book will open the way for including causal analysis in the standard curriculum of statistics, artifical intelligence, business, epidemiology, social science and economics. Students in these areas will find natural models, simple identification procedures, and precise mathematical definitions of causal concepts that traditional texts have tended to evade or make unduly complicated. This book will be of interest to professionals and students in a wide variety of fields. Anyone who wishes to elucidate meaningful relationships from data, predict effects of actions and policies, assess explanations of reported events, or form theories of causal understanding and causal speech will find this book stimulating and invaluable. Professor of Computer Science at the UCLA, Judea Pearl is the winner of the 2008 Benjamin Franklin Award in Computers and Cognitive Science.},
author = {Pearl, Judea},
doi = {10.1215/00318108-110-4-639},
edition = {Second},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pearl - 2009 - Causality Models, Reasoning, and Inference.pdf:pdf},
isbn = {0521773628},
issn = {00323470},
pmid = {11768929},
publisher = {Cambridge University Press},
title = {{Causality: Models, Reasoning, and Inference}},
year = {2009}
}
@article{Leemis2008,
author = {Leemis, Lawrence M and McQueston, Jacquelyn T},
doi = {10.1198/000313008X270448},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Leemis, McQueston - 2008 - Univariate Distribution Relationships.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {asymptotic relationships,distribution proper-,limiting distributions,stochastic parameters,ties,transforma-},
month = {feb},
number = {1},
pages = {45--53},
title = {{Univariate Distribution Relationships}},
url = {http://www.tandfonline.com/doi/abs/10.1198/000313008X270448},
volume = {62},
year = {2008}
}
@article{Vehtari2012,
abstract = {To date, several methods exist in the statistical literature for model assessment, which purport themselves specifically as Bayesian predictive methods. The decision theoretic assumptions on which these methods are based are not always clearly stated in the original articles, however. The aim of this survey is to provide a unified review of Bayesian predictive model assessment and selection methods, and of methods closely related to them. We review the various assumptions that are made in this context and discuss the connections between different approaches, with an emphasis on how each method approximates the expected utility of using a Bayesian model for the purpose of predicting future data.},
author = {Vehtari, Aki and Ojanen, Janne},
doi = {10.1214/12-SS102},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/euclid.ssu.1356628931.pdf:pdf},
isbn = {1935-7516},
issn = {1935-7516},
journal = {Statistics Surveys},
keywords = {62-02, 62C10Bayesian, predictive, model assessment,and phrases,bayesian,cross-validation,decision theory,expected utility,information cri-,model,model assessment,predictive,selection},
number = {0},
pages = {142--228},
title = {{A survey of Bayesian predictive methods for model assessment, selection and comparison}},
url = {http://projecteuclid.org/euclid.ssu/1356628931},
volume = {6},
year = {2012}
}
@article{Janson2015,
author = {Janson, L. and Fithian, W. and Hastie, T. J.},
doi = {10.1093/biomet/asv019},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Janson, Fithian, Hastie - 2015 - Effective degrees of freedom a flawed metaphor.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
number = {May},
pages = {479--485},
title = {{Effective degrees of freedom: a flawed metaphor}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/asv019},
year = {2015}
}
@article{Gelman2013c,
abstract = {A substantial school in the philosophy of science identifies Bayesian inference with inductive inference and even rationality as such, and seems to be strengthened by the rise and practical success of Bayesian statistics. We argue that the most successful forms of Bayesian statistics do not actually support that particular philosophy but rather accord much better with sophisticated forms of hypothetico-deductivism. We examine the actual role played by prior distributions in Bayesian models, and the crucial aspects of model checking and model revision, which fall outside the scope of Bayesian confirmation theory. We draw on the literature on the consistency of Bayesian updating and also on our experience of applied work in social science. Clarity about these matters should benefit not just philosophy of science, but also statistical practice. At best, the inductivist view has encouraged researchers to fit and compare models without checking them; at worst, theorists have actively discouraged practitioners from performing model checking because it does not fit into their framework.},
author = {Gelman, Andrew and Shalizi, Cosma},
doi = {10.1111/j.2044-8317.2012.02066.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/philosophy{\_}rejoinder.pdf:pdf},
isbn = {0007-1102},
issn = {00071102},
journal = {British Journal of Mathematical and Statistical Psychology},
month = {feb},
number = {1},
pages = {76--80},
pmid = {23050946},
title = {{Rejoinder to discussion of ‘Philosophy and the practice of Bayesian statistics'}},
url = {http://doi.wiley.com/10.1111/j.2044-8317.2012.02066.x},
volume = {66},
year = {2013}
}
@article{Fox-Roberts2014,
abstract = {Reliable semi-supervised learning, where a small amount of labelled data is complemented by a large body of unlabelled data, has been a long-standing goal of the machine learning community. However, while it seems intuitively obvious that unlabelled data can aid the learning process, in practise its performance has often been disappointing. We investigate this by examining generative maximum likelihood semi-supervised learning and derive novel upper and lower bounds on the degree of bias introduced by the unlabelled data. These bounds improve upon those provided in previous work, and are specifically applicable to the challenging case where the model is unable to exactly fit to the underlying distribution a situation which is common in practise, but for which fewer guarantees of semi-supervised performance have been found. Inspired by this new framework for analysing bounds, we propose a new, simple reweighing scheme which provides a provably unbiased estimator for arbitrary model/distribution pairs|an unusual property for a semi-supervised algorithm. This reweighing introduces no additional computational complexity and can be applied to very many models. Additionally, we provide specific conditions demonstrating the circumstance under which the unlabelled data will lower the estimator variance, thereby improving convergence. {\textcopyright} 2014 Patrick Fox-Roberts and Edward Rosten.},
author = {Fox-Roberts, Patrick and Rosten, Edward},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fox-Roberts, Rosten - 2014 - Unbiased Generative Semi-Supervised Learning.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {asymptotic bounds,bias,generative model,kullback-leibler,semi-supervised},
pages = {367--443},
title = {{Unbiased Generative Semi-Supervised Learning}},
volume = {15},
year = {2014}
}
@article{Taskesen2016,
author = {Taskesen, Erdogan and Huisman, Sjoerd M H and Mahfouz, Ahmed and Krijthe, Jesse H and de Ridder, Jeroen and van de Stolpe, Anja and van den Akker, Erik and Verheagh, Wim and Reinders, Marcel J T},
doi = {10.1038/srep24949},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Taskesen et al. - 2016 - Pan-cancer subtyping in a 2D-map shows substructures that are driven by specific combinations of molecular char.pdf:pdf},
journal = {Scientific Reports},
month = {apr},
pages = {24949},
publisher = {The Author(s)},
title = {{Pan-cancer subtyping in a 2D-map shows substructures that are driven by specific combinations of molecular characteristics}},
volume = {6},
year = {2016}
}
@article{Weston2005,
abstract = {MOTIVATION: Building an accurate protein classification system depends critically upon choosing a good representation of the input sequences of amino acids. Recent work using string kernels for protein data has achieved state-of-the-art classification performance. However, such representations are based only on labeled data--examples with known 3D structures, organized into structural classes--whereas in practice, unlabeled data are far more plentiful. RESULTS: In this work, we develop simple and scalable cluster kernel techniques for incorporating unlabeled data into the representation of protein sequences. We show that our methods greatly improve the classification performance of string kernels and outperform standard approaches for using unlabeled data, such as adding close homologs of the positive examples to the training data. We achieve equal or superior performance to previously presented cluster kernel methods and at the same time achieving far greater computational efficiency. AVAILABILITY: Source code is available at www.kyb.tuebingen.mpg.de/bs/people/weston/semiprot. The Spider matlab package is available at www.kyb.tuebingen.mpg.de/bs/people/spider. SUPPLEMENTARY INFORMATION: www.kyb.tuebingen.mpg.de/bs/people/weston/semiprot.},
author = {Weston, Jason and Leslie, Christina and Ie, Eugene and Zhou, Dengyong and Elisseeff, Andre and Noble, William Stafford},
doi = {10.1093/bioinformatics/bti497},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Weston et al. - 2005 - Semi-supervised protein classification using cluster kernels.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Cluster Analysis,Pattern Recognition,Protein,Protein: methods,Proteins,Proteins: analysis,Proteins: chemistry,Proteins: classification,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis,Software},
month = {aug},
number = {15},
pages = {3241--7},
pmid = {15905279},
title = {{Semi-supervised protein classification using cluster kernels.}},
volume = {21},
year = {2005}
}
@inproceedings{Balcan2013,
author = {Balcan, Maria-Florina and Berlind, Christopher and Ehrlich, Steven and Liang, Yingyu},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balcan et al. - 2013 - Efficient Semi-supervised and Active Learning of Disjunctions.pdf:pdf},
pages = {633--641},
title = {{Efficient Semi-supervised and Active Learning of Disjunctions}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/ICML2013{\_}balcan13},
year = {2013}
}
@article{Blundell2015,
abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learn-ing a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a com-pression cost, known as the variational free en-ergy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforce-ment learning.},
archivePrefix = {arXiv},
arxivId = {arXiv:1505.05424v2},
author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
eprint = {arXiv:1505.05424v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Blundell et al. - 2015 - Weight Uncertainty in Neural Networks.pdf:pdf},
title = {{Weight Uncertainty in Neural Networks}},
volume = {37},
year = {2015}
}
@article{Sharma2014,
author = {Sharma, Aastha and Chaturvedi, Setu and Gour, Bhupesh},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sharma, Chaturvedi, Gour - 2014 - A Semi- Supervised Technique for Weather Condition Prediction using DBSCAN and KNN.pdf:pdf},
journal = {International Journal of Computer Applications},
keywords = {clustering,data mining,dbscan,knn,semi-supervised},
number = {10},
pages = {21--26},
title = {{A Semi- Supervised Technique for Weather Condition Prediction using DBSCAN and KNN}},
volume = {95},
year = {2014}
}
@misc{Tibshirani,
author = {Tibshirani, Robert},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tibshirani - Unknown - Machine Learning vs. Statistics.pdf:pdf},
title = {{Machine Learning vs. Statistics}}
}
@inproceedings{Sokolovska2011,
address = {Greece},
author = {Sokolovska, Nataliya},
booktitle = {ECML PKDD},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sokolovska - 2011 - Aspects of semi-supervised and active learning in conditional random fields.pdf:pdf},
keywords = {active learn-,conditional random fields,ing,probability of observations,semi-supervised learning},
title = {{Aspects of semi-supervised and active learning in conditional random fields}},
url = {http://www.springerlink.com/index/3308764R6251J70P.pdf},
year = {2011}
}
@article{Soudry2016,
abstract = {We use smoothed analysis techniques to provide guarantees on the training loss of Multilayer Neural Networks (MNNs) at differentiable local minima. Specifically, we examine MNNs with piecewise linear activation functions, quadratic loss and a single output, under mild over-parametrization. We prove that for a MNN with one hidden layer, the training error is zero at every differentiable local minimum, for almost every dataset and dropout-like noise realization. We then extend these results to the case of more than one hidden layer. Our theoretical guarantees assume essentially nothing on the training data, and are verified numerically. These results suggest why the highly non-convex loss of such MNNs can be easily optimized using local updates (e.g., stochastic gradient descent), as observed empirically.},
archivePrefix = {arXiv},
arxivId = {1605.08361},
author = {Soudry, Daniel and Carmon, Yair},
eprint = {1605.08361},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Soudry, Carmon - 2016 - No bad local minima Data independent training error guarantees for multilayer neural networks.pdf:pdf},
number = {DLM},
pages = {1--12},
title = {{No bad local minima: Data independent training error guarantees for multilayer neural networks}},
url = {http://arxiv.org/abs/1605.08361},
year = {2016}
}
@inproceedings{Jaakkola2002,
author = {Jaakkola, MST and Szummer, Martin},
booktitle = {Advances in Neural Information Processing Systems 14},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jaakkola, Szummer - 2002 - Partially labeled classification with Markov random walks.pdf:pdf},
pages = {945--952},
title = {{Partially labeled classification with Markov random walks}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=GbC8cqxGR7YC{\&}oi=fnd{\&}pg=PA945{\&}dq=Partially+labeled+classification+with+Markov+random+walks{\&}ots=ZvP5J{\_}YBx6{\&}sig=dk27TWzUdp9G-e9OyvfYcGR14ro},
year = {2002}
}
@article{Kuncheva2003,
author = {Kuncheva, Ludmila I and Whitaker, Christopher J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kuncheva, Whitaker - 2003 - Measures of Diversity in Classifier Ensembles.pdf:pdf},
journal = {Machine Learning},
keywords = {committee of learners,dependency and diversity,multiple classifiers ensemble,pattern recognition},
pages = {181--207},
title = {{Measures of Diversity in Classifier Ensembles}},
volume = {51},
year = {2003}
}
@article{Cox2016,
author = {Cox, D. R.},
doi = {10.1093/biomet/asw052},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cox - 2016 - Some pioneers of modern statistical theory a personal reflection.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
number = {4},
pages = {747--759},
title = {{Some pioneers of modern statistical theory: a personal reflection}},
url = {http://biomet.oxfordjournals.org/lookup/doi/10.1093/biomet/asw052},
volume = {103},
year = {2016}
}
@inproceedings{Roli2002,
author = {Roli, Fabio and Raudys, {\v{S}}arūnas and Marcialis, Gian Luca},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Roli, Raudys, Marcialis - 2002 - An experimental comparison of fixed and trained fusion rules for crisp classifier outputs.pdf:pdf},
title = {{An experimental comparison of fixed and trained fusion rules for crisp classifier outputs}},
url = {http://link.springer.com/chapter/10.1007/3-540-45428-4{\_}23},
year = {2002}
}
@inproceedings{Ji2012,
author = {Ji, Ming and Yang, Tianbao and Lin, Binbin and Jin, Rong and Han, Jiawei},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ji et al. - 2012 - A simple algorithm for semi-supervised learning with improved generalization error bound.pdf:pdf},
number = {2},
title = {{A simple algorithm for semi-supervised learning with improved generalization error bound}},
url = {http://arxiv.org/abs/1206.6412},
year = {2012}
}
@article{Jones2012,
author = {Jones, Emrys A and Deininger, S{\"{o}}ren-oliver and Hogendoorn, Pancras C W and Deelder, Andr{\'{e}} M and Mcdonnell, Liam A},
doi = {10.1016/j.jprot.2012.06.014},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jones et al. - 2012 - Imaging mass spectrometry statistical analysis.pdf:pdf},
issn = {1874-3919},
journal = {Journal of Proteomics},
keywords = {Biomarker discovery,Data analysis,Molecular histology,imaging mass spectrometry},
number = {16},
pages = {4962--4989},
publisher = {Elsevier B.V.},
title = {{Imaging mass spectrometry statistical analysis}},
url = {http://dx.doi.org/10.1016/j.jprot.2012.06.014},
volume = {75},
year = {2012}
}
@article{Culp2008,
author = {Culp, Mark and Michailidis, George},
doi = {10.1198/106186008X344748},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Culp, Michailidis - 2008 - An iterative algorithm for extending learners to a semi-supervised setting.pdf:pdf;:Users/jkrijthe/Documents/Mendeley Desktop/Culp, Michailidis - 2008 - An iterative algorithm for extending learners to a semi-supervised setting(2).pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {convergence,iterative algorithm,linear smoothers,semi-supervised learning},
month = {sep},
number = {3},
pages = {545--571},
title = {{An iterative algorithm for extending learners to a semi-supervised setting}},
volume = {17},
year = {2008}
}
@inproceedings{Yoshiyama2011,
author = {Yoshiyama, Kazuki and Sakurai, Akito},
booktitle = {IAPR International Workshop on Partially Supervised Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yoshiyama, Sakurai - 2011 - Manifold-Regularized Minimax Probability Machine.pdf:pdf},
pages = {42--51},
title = {{Manifold-Regularized Minimax Probability Machine}},
year = {2011}
}
@article{Skurichina1999,
author = {Skurichina, Marina and Duin, Robert P. W.},
doi = {10.1007/s100440050013},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Skurichina, Duin - 1999 - Regularisation of Linear Classifiers by Adding Redundant Features.pdf:pdf},
issn = {1433-7541},
journal = {Pattern Analysis {\&} Applications},
keywords = {critical sample size,generalisation error,noise injection,peaking behaviour,pseudo fisher linear discriminant,regularisation},
number = {1},
pages = {44--52},
title = {{Regularisation of Linear Classifiers by Adding Redundant Features}},
volume = {2},
year = {1999}
}
@inproceedings{McDowell2012,
author = {McDowell, Luke and Aha, David W.},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/McDowell, Aha - 2012 - Semi-supervised collective classification via hybrid label regularization.pdf:pdf},
pages = {975--982},
title = {{Semi-supervised collective classification via hybrid label regularization}},
url = {http://arxiv.org/abs/1206.6467},
year = {2012}
}
@article{Culp2008b,
abstract = {Graph-based learning provides a useful approach for modeling data in classification problems. In this modeling scenario, the relationship between labeled and unlabeled data impacts the construction and performance of classifiers, and therefore a semi-supervised learning framework is adopted. We propose a graph classifier based on kernel smoothing. A regularization framework is also introduced, and it is shown that the proposed classifier optimizes certain loss functions. Its performance is assessed on several synthetic and real benchmark data sets with good results, especially in settings where only a small fraction of the data are labeled.},
author = {Culp, Mark and Michailidis, George},
doi = {10.1109/TPAMI.2007.70765},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Culp, Michailidis - 2008 - Graph-based semisupervised learning.pdf:pdf},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Cluster Analysis,Computer Simulation,Data Interpretation,Models,Pattern Recognition,Statistical},
month = {jan},
number = {1},
pages = {174--9},
pmid = {18000333},
title = {{Graph-based semisupervised learning.}},
volume = {30},
year = {2008}
}
@article{Henderson1981,
author = {Henderson, H.V. and Searle, S.R.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Henderson, Searle - 1981 - On Deriving the Inverse of a Sum of Matrices.pdf:pdf},
journal = {SIAM Review},
number = {1},
pages = {53--60},
title = {{On Deriving the Inverse of a Sum of Matrices}},
volume = {23},
year = {1981}
}
@techreport{Bensusan2000,
author = {Bensusan, H. and Giraud-carrier, Christophe and Kennedy, C.J.},
booktitle = {ILP Work-in-progress {\ldots}},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bensusan, Giraud-carrier, Kennedy - 2000 - A Higher-order Approach to Meta-learning.pdf:pdf},
institution = {University of Bristol},
title = {{A Higher-order Approach to Meta-learning}},
url = {http://137.222.102.8/Publications/Papers/1000471.pdf},
year = {2000}
}
@article{Bartlett2003a,
author = {Bartlett, Peter L and Mendelson, S},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bartlett, Mendelson - 2003 - Rademacher and Gaussian complexities Risk bounds and structural results.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {data-dependent complexity,error bounds,maxi-,rademacher averages},
pages = {463--482},
title = {{Rademacher and Gaussian complexities: Risk bounds and structural results}},
url = {http://dl.acm.org/citation.cfm?id=944944},
volume = {3},
year = {2003}
}
@inproceedings{Taigman2014,
author = {Taigman, Yaniv and Ranzato, Marc Aurelio and Aviv, Tel and Park, Menlo},
booktitle = {Computer Vision and Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Taigman et al. - 2014 - DeepFace Closing the Gap to Human-Level Performance in Face Verification.pdf:pdf},
title = {{DeepFace : Closing the Gap to Human-Level Performance in Face Verification}},
year = {2014}
}
@article{Ho2002a,
author = {Ho, Tin Kam},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho - 2002 - A Data Complexity Analysis of Comparative Advantages of Decision Forest Constructors.pdf:pdf},
journal = {Pattern Analysis and Applications},
number = {2},
pages = {102--112},
title = {{A Data Complexity Analysis of Comparative Advantages of Decision Forest Constructors}},
volume = {5},
year = {2002}
}
@inproceedings{Szummer2000,
author = {Szummer, Martin and Jaakkola, Tommi},
booktitle = {Advances in Neural Information Processing Systems 13},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Szummer, Jaakkola - 2000 - Kernel expansions with unlabeled examples.pdf:pdf},
pages = {626--632},
title = {{Kernel expansions with unlabeled examples}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.222{\&}rep=rep1{\&}type=pdf},
year = {2000}
}
@article{Yates1933,
author = {Yates, Frank},
journal = {Empire Journal of Experimental Agriculture},
number = {2},
pages = {129--142},
title = {{The analysis of replicated experiments when the field results are incomplete}},
volume = {1},
year = {1933}
}
@article{Louppe2015a,
abstract = {Author name disambiguation in bibliographic databases is the problem of grouping together scientific publications written by the same person, accounting for potential homonyms and/or synonyms. Among solutions to this problem, digital libraries are increasingly offering tools for authors to manually curate their publications and claim those that are theirs. Indirectly, these tools allow for the inexpensive collection of large annotated training data, which can be further leveraged to build a complementary automated disambiguation system capable of inferring patterns for identifying publications written by the same person. Building on more than 1 million publicly released crowdsourced annotations, we propose an automated author disambiguation solution exploiting this data (i) to learn an accurate classifier for identifying coreferring authors and (ii) to guide the clustering of scientific publications by distinct authors in a semi-supervised way. To the best of our knowledge, our analysis is the first to be carried out on data of this size and coverage. With respect to the state of the art, we validate the general pipeline used in most existing solutions, and improve by: (i) proposing phonetic-based blocking strategies, thereby increasing recall; and (ii) adding strong ethnicity-sensitive features for learning a linkage function, thereby tailoring disambiguation to non-Western author names whenever necessary.},
archivePrefix = {arXiv},
arxivId = {1508.07744},
author = {Louppe, Gilles and Al-Natsheh, Hussein and Susik, Mateusz and Maguire, Eamonn},
eprint = {1508.07744},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Louppe et al. - 2015 - Ethnicity sensitive author disambiguation using semi-supervised learning.pdf:pdf},
pages = {1--14},
title = {{Ethnicity sensitive author disambiguation using semi-supervised learning}},
url = {http://arxiv.org/abs/1508.07744},
year = {2015}
}
@article{Mirowski2008,
author = {Mirowski, Piotr W. and LeCun, Yann and Madhavan, Deepak and Kuzniecky, Ruben},
doi = {10.1109/MLSP.2008.4685487},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mirowski et al. - 2008 - Comparing SVM and convolutional networks for epileptic seizure prediction from intracranial EEG.pdf:pdf},
isbn = {978-1-4244-2375-0},
journal = {2008 IEEE Workshop on Machine Learning for Signal Processing},
month = {oct},
pages = {244--249},
publisher = {Ieee},
title = {{Comparing SVM and convolutional networks for epileptic seizure prediction from intracranial EEG}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4685487},
year = {2008}
}
@article{Yu2013a,
author = {Yu, Bin},
doi = {10.3150/13-BEJSP14},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yu - 2013 - Stability.pdf:pdf},
issn = {1350-7265},
journal = {Bernoulli},
keywords = {cross-validation,double exponential error,estimation stability,fmri,high-dim regression,lasso,movie reconstruction,robust statistics,stability},
number = {4},
pages = {1484--1500},
title = {{Stability}},
url = {http://projecteuclid.org/euclid.bj/1377612862},
volume = {19},
year = {2013}
}
@article{Ireland1968a,
author = {Ireland, C.T. and Kullback, S.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ireland, Kullback - 1968 - Contingence tables with given marginals.pdf:pdf},
journal = {Biometrika},
number = {1},
pages = {179--188},
title = {{Contingence tables with given marginals}},
volume = {55},
year = {1968}
}
@inproceedings{Moutafis2014,
author = {Moutafis, Panagiotis and Kakadiaris, Ioannis A},
booktitle = {Pacific-Asia Conference on Knowledge Discovery and Data Mining},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Moutafis, Kakadiaris - 2014 - GS4 Generating synthetic samples for semi-supervised nearest neighbor classification.pdf:pdf},
isbn = {9783319131856},
issn = {16113349},
keywords = {Classification,K-nearest neighbor,Semi-supervised learning,Synthetic samples},
number = {13},
pages = {393--403},
title = {{GS4: Generating synthetic samples for semi-supervised nearest neighbor classification}},
volume = {8643},
year = {2014}
}
@article{Smith-Miles2008,
author = {Smith-Miles, Kate},
doi = {10.1145/1456650.1456656},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Smith-Miles - 2008 - Cross-disciplinary perspectives on meta-learning for algorithm selection.pdf:pdf},
issn = {03600300},
journal = {ACM Computing Surveys},
month = {dec},
number = {1},
pages = {1--25},
title = {{Cross-disciplinary perspectives on meta-learning for algorithm selection}},
url = {http://portal.acm.org/citation.cfm?doid=1456650.1456656},
volume = {41},
year = {2008}
}
@inproceedings{Niu2013,
author = {Niu, G and Jitkrittum, W and Dai, Bo and Hachiya, H and Sugiyama, M},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Niu et al. - 2013 - Squared-loss Mutual Information Regularization A Novel Information-theoretic Approach to Semi-supervised Learning.pdf:pdf},
pages = {10--18},
title = {{Squared-loss Mutual Information Regularization: A Novel Information-theoretic Approach to Semi-supervised Learning}},
url = {http://sugiyama-www.cs.titech.ac.jp/{~}gang/paper/niu{\_}icml13.pdf},
year = {2013}
}
@techreport{Zhu2005,
author = {Zhu, Xiaojin},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhu - 2005 - Semi-supervised learning literature survey.pdf:pdf},
institution = {University of Wisconsin - Madison},
pages = {1--59},
title = {{Semi-supervised learning literature survey}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.99.9681{\&}rep=rep1{\&}type=pdf},
year = {2005}
}
@article{Halevy2009,
author = {Halevy, Alon and Norvig, Peter and Pereira, Fernando},
doi = {10.1109/MIS.2009.36},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Halevy, Norvig, Pereira - 2009 - The Unreasonable Effectiveness of Data.pdf:pdf},
issn = {1541-1672},
journal = {IEEE Intelligent Systems},
month = {mar},
number = {2},
pages = {8--12},
title = {{The Unreasonable Effectiveness of Data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4804817},
volume = {24},
year = {2009}
}
@article{An2016,
author = {An, H Aitao G and Uo, Z Hizeng L and An, Y Ingle F and Ang, N O N G S},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/An et al. - 2016 - Enhanced manifold regularization for semi-supervised classification.pdf:pdf},
journal = {Journal of the Optical Society of America},
number = {6},
pages = {1207--1213},
title = {{Enhanced manifold regularization for semi-supervised classification}},
volume = {33},
year = {2016}
}
@inproceedings{Goldman2000,
author = {Goldman, Sally and Zhou, Yan},
booktitle = {Proceedings of the 17th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Goldman, Zhou - 2000 - Enhancing supervised learning with unlabeled data.pdf:pdf},
pages = {327--334},
title = {{Enhancing supervised learning with unlabeled data}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Enhancing+Supervised+Learning+with+Unlabeled+Data{\#}0},
volume = {3},
year = {2000}
}
@article{Kulesza2012,
abstract = {Determinantal point processes (DPPs) are elegant probabilistic models of repulsion that arise in quantum physics and random matrix theory. In contrast to traditional structured models like Markov random fields, which become intractable and hard to approximate in the presence of negative correlations, DPPs offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. We provide a gentle introduction to DPPs, focusing on the intuitions, algorithms, and extensions that are most relevant to the machine learning community, and show how DPPs can be applied to real-world applications like finding diverse sets of high-quality search results, building informative summaries by selecting diverse sentences from documents, modeling non-overlapping human poses in images or video, and automatically building timelines of important news stories.},
archivePrefix = {arXiv},
arxivId = {1207.6083},
author = {Kulesza, Alex and Taskar, Ben},
doi = {10.1561/2200000044},
eprint = {1207.6083},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kulesza, Taskar - 2012 - Determinantal Point Processes for Machine Learning.pdf:pdf},
isbn = {9781601986283},
issn = {1935-8237},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
number = {2-3},
pages = {123--286},
title = {{Determinantal Point Processes for Machine Learning}},
url = {http://arxiv.org/abs/1207.6083{\%}5Cnhttp://www.nowpublishers.com/product.aspx?product=MAL{\&}doi=2200000044},
volume = {5},
year = {2012}
}
@article{Gelman1999,
abstract = {Maps are frequently used to display spatial distributions of parameters of interest, such as cancer rates or average pollutant concentrations by county. It is well known that plotting observed rates can have serious drawbacks when sample sizes vary by area, since very high (and low) observed rates are found disproportionately in poorly-sampled areas. Unfortunately, adjusting the observed rates to account for the effects of small-sample noise can introduce an opposite effect, in which the highest adjusted rates tend to be found disproportionately in well-sampled areas. In either case, the maps can be difficult to interpret because the display of spatial variation in the underlying parameters of interest is confounded with spatial variation in sample sizes. As a result, spatial patterns occur in adjusted rates even if there is no spatial structure in the underlying parameters of interest, and adjusted rates tend to look too uniform in areas with little data. We introduce two models (normal and Poisson) in which parameters of interest have no spatial patterns, and demonstrate the existence of spatial artefacts in inference from these models. We also discuss spatial models and the extent to which they are subject to the same artefacts. We present examples from Bayesian modelling, but, as we explain, the artefacts occur generally.},
author = {Gelman, Andrew and Price, Phlllip N.},
doi = {10.1002/(SICI)1097-0258(19991215)18:23<3221::AID-SIM312>3.0.CO;2-M},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Price - 1999 - All maps of parameter estimates are misleading.pdf:pdf},
isbn = {0277-6715},
issn = {02776715},
journal = {Statistics in Medicine},
pages = {3221--3234},
pmid = {10602147},
title = {{All maps of parameter estimates are misleading}},
volume = {18},
year = {1999}
}
@inproceedings{Sindhwani2005,
author = {Sindhwani, Vikas and Niyogi, Partha and Belkin, Mikhail},
booktitle = {Proceedings of the 22nd international conference on Machine learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sindhwani, Niyogi, Belkin - 2005 - Beyond the point cloud from transductive to semi-supervised learning.pdf:pdf},
number = {0},
pages = {824--831},
title = {{Beyond the point cloud: from transductive to semi-supervised learning}},
url = {http://dl.acm.org/citation.cfm?id=1102455},
year = {2005}
}
@article{Williams1998,
author = {Williams, Christopher K. I.},
doi = {10.1162/089976698300017412},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Williams - 1998 - Computation with Infinite Neural Networks.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
month = {jul},
number = {5},
pages = {1203--1216},
title = {{Computation with Infinite Neural Networks}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/089976698300017412},
volume = {10},
year = {1998}
}
@article{Papadopoulos2016,
abstract = {Training object class detectors typically requires a large set of images in which objects are annotated by bounding-boxes. However, manually drawing bounding-boxes is very time consuming. We propose a new scheme for training object detectors which only requires annotators to verify bounding-boxes produced automatically by the learning algorithm. Our scheme iterates between re-training the detector, re-localizing objects in the training images, and human verification. We use the verification signal both to improve re-training and to reduce the search space for re-localisation, which makes these steps different to what is normally done in a weakly supervised setting. Extensive experiments on PASCAL VOC 2007 show that (1) using human verification to update detectors and reduce the search space leads to the rapid production of high-quality bounding-box annotations; (2) our scheme delivers detectors performing almost as good as those trained in a fully supervised setting, without ever drawing any bounding-box; (3) as the verification task is very quick, our scheme substantially reduces total annotation time by a factor 6x-9x.},
archivePrefix = {arXiv},
arxivId = {1602.08405},
author = {Papadopoulos, Dim P. and Uijlings, Jasper R. R. and Keller, Frank and Ferrari, Vittorio},
eprint = {1602.08405},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Papadopoulos et al. - 2016 - We don't need no bounding-boxes Training object class detectors using only human verification.pdf:pdf},
number = {1},
title = {{We don't need no bounding-boxes: Training object class detectors using only human verification}},
url = {http://arxiv.org/abs/1602.08405},
year = {2016}
}
@incollection{Cozman2006,
author = {Cozman, F and Cohen, Ira},
booktitle = {Semi-Supervised Learning},
chapter = {4},
editor = {Chapelle, Olivier and Sch{\"{o}}lkopf, Bernhard and Zien, A},
pages = {56--72},
publisher = {MIT press},
title = {{Risks of Semi-Supervised Learning}},
year = {2006}
}
@book{Pearl2014,
author = {Pearl, Judea and Glymour, Madelyn and Jewell, Nicholas P.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pearl, Glymour, Jewell - 2016 - Causal Inference in Statistics A Primer.pdf:pdf},
publisher = {Wiley},
title = {{Causal Inference in Statistics: A Primer}},
year = {2016}
}
@article{Lockhart2014d,
author = {Lockhart, Richard and Taylor, Jonathan and Tibshirani, Ryan J. and Tibshirani, Robert},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lockhart et al. - 2014 - A significance test for the lasso.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {apr},
number = {2},
pages = {413--468},
title = {{A significance test for the lasso}},
url = {http://projecteuclid.org/euclid.aos/1400592161},
volume = {42},
year = {2014}
}
@article{Nguyen2009,
author = {Nguyen, XuanLong and Wainwright, Martin J. and Jordan, Michael I.},
doi = {10.1214/08-AOS595},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nguyen, Wainwright, Jordan - 2009 - On surrogate loss functions and f -divergences.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {apr},
number = {2},
pages = {876--904},
title = {{On surrogate loss functions and f -divergences}},
url = {http://projecteuclid.org/euclid.aos/1236693153},
volume = {37},
year = {2009}
}
@book{Imbens2015,
author = {Imbens, Guido W. and Rubin, Donald B.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Imbens, Rubin - 2015 - Causal Inference for Statistics , Social , and Biomedical Sciences.pdf:pdf},
isbn = {9781139025751},
publisher = {Cambridge University Press},
title = {{Causal Inference for Statistics , Social , and Biomedical Sciences}},
year = {2015}
}
@inproceedings{Krijthe2014,
address = {Stockholm},
archivePrefix = {arXiv},
arxivId = {1411.4521},
author = {Krijthe, Jesse Hendrik and Loog, Marco},
booktitle = {Proceedings of the 22nd International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2014.646},
eprint = {1411.4521},
pages = {3762--3767},
title = {{Implicitly Constrained Semi-Supervised Linear Discriminant Analysis}},
year = {2014}
}
@article{Reif2012,
author = {Reif, Matthias and Shafait, Faisal and Goldstein, Markus and Breuel, Thomas and Dengel, Andreas},
doi = {10.1007/s10044-012-0280-z},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Reif et al. - 2012 - Automatic classifier selection for non-experts.pdf:pdf},
issn = {1433-7541},
journal = {Pattern Analysis and Applications},
keywords = {classifier recommendation,classifier selection,landmarking,meta-features,meta-learning,regression},
month = {jul},
title = {{Automatic classifier selection for non-experts}},
url = {http://www.springerlink.com/index/10.1007/s10044-012-0280-z},
year = {2012}
}
@article{Liu2015,
abstract = {Doctors use statistics to advance medical knowledge; we use a medical analogy to introduce statistical inference "from scratch" and to highlight an improvement. Your doctor, perhaps implicitly, predicts the effectiveness of a treatment for you based on its performance in a clinical trial; the trial patients serve as controls for you. The same logic underpins statistical inference: to identify the best statistical procedure to use for a problem, we simulate a set of control problems and evaluate candidate procedures on the controls. Now for the improvement: recent interest in personalized/individualized medicine stems from the recognition that some clinical trial patients are better controls for you than others. Therefore, treatment decisions for you should depend only on a subset of relevant patients. Individualized statistical inference implements this idea for control problems (rather than patients). Its potential for improving data analysis matches personalized medicine's for improving healthcare. The central issue--for both individualized medicine and individualized inference--is how to make the right relevance robustness trade-off: if we exercise too much judgement in determining which controls are relevant, our inferences will not be robust. How much is too much? We argue that the unknown answer is the Holy Grail of statistical inference.},
archivePrefix = {arXiv},
arxivId = {1510.08539},
author = {Liu, Keli and Meng, Xiao-Li},
doi = {10.1146/annurev-statistics-010814-020310},
eprint = {1510.08539},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/annurev-statistics-010814-020310.pdf:pdf},
issn = {2326-8298},
keywords = {bayes-frequentist dichotomy,conditional inference,fiducial inference,objective inference,relevance-robustness tradeoff},
title = {{There is Individualized Treatment. Why Not Individualized Inference?}},
url = {http://arxiv.org/abs/1510.08539},
year = {2015}
}
@inproceedings{Cai2007,
author = {Cai, Deng and He, Xiaofei and Han, Jiawei},
booktitle = {The IEEE 11th International Conference on Computer Vision},
doi = {10.1109/ICCV.2007.4408856},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cai, He, Han - 2007 - Semi-supervised Discriminant Analysis.pdf:pdf},
pages = {1--7},
title = {{Semi-supervised Discriminant Analysis}},
year = {2007}
}
@inproceedings{Kulesza2010,
abstract = {We present a novel probabilistic model for distributions over sets of structures— for example, sets of sequences, trees, or graphs. The critical characteristic of our model is a preference for diversity: sets containing dissimilar structures are more likely. Our model is a marriage of},
author = {Kulesza, Alex and Taskar, Ben},
booktitle = {Advances in Neural Information Processing Systems 23},
doi = {10.1080/00036840500405656},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kulesza, Taskar - 2010 - Structured Determinantal Point Processes.pdf:pdf},
isbn = {0003684050040},
issn = {{\textless}null{\textgreater}},
pages = {1--9},
title = {{Structured Determinantal Point Processes}},
year = {2010}
}
@article{OpenScienceCollaboration2015,
author = {{Open Science Collaboration}},
doi = {10.1126/science.aac4716},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Open Science Collaboration - 2015 - Estimating the reproducibility of psychological science.pdf:pdf},
issn = {0036-8075},
journal = {Science},
number = {6251},
title = {{Estimating the reproducibility of psychological science}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.aac4716},
volume = {349},
year = {2015}
}
@article{Kleinberg2000,
author = {Kleinberg, E.M.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kleinberg - 2000 - On the algorithmic implementation of stochastic discrimination.pdf:pdf},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {5},
pages = {473--490},
publisher = {IEEE},
title = {{On the algorithmic implementation of stochastic discrimination}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=857004},
volume = {22},
year = {2000}
}
@article{Reitmaier2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.04033v2},
author = {Reitmaier, Tobias and Sick, Bernhard},
eprint = {arXiv:1502.04033v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Reitmaier, Sick - 2015 - The Responsibility Weighted Mahalanobis Kernel for Semi-Supervised Training of Support Vector Machines for Clas.pdf:pdf},
journal = {Information Sciences},
keywords = {kernel function,pattern classification,responsibility weighted mahalanobis kernel,semi-supervised learning,support vector machine},
pages = {179--198},
title = {{The Responsibility Weighted Mahalanobis Kernel for Semi-Supervised Training of Support Vector Machines for Classification}},
volume = {323},
year = {2015}
}
@article{Gelman2013,
abstract = {A substantial school in the philosophy of science identifies Bayesian inference with inductive inference and even rationality as such, and seems to be strengthened by the rise and practical success of Bayesian statistics. We argue that the most successful forms of Bayesian statistics do not actually support that particular philosophy but rather accord much better with sophisticated forms of hypothetico-deductivism. We examine the actual role played by prior distributions in Bayesian models, and the crucial aspects of model checking and model revision, which fall outside the scope of Bayesian confirmation theory. We draw on the literature on the consistency of Bayesian updating and also on our experience of applied work in social science. Clarity about these matters should benefit not just philosophy of science, but also statistical practice. At best, the inductivist view has encouraged researchers to fit and compare models without checking them; at worst, theorists have actively discouraged practitioners from performing model checking because it does not fit into their framework.},
author = {Gelman, Andrew and Shalizi, Cosma Rohilla},
doi = {10.1111/j.2044-8317.2011.02037.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Shalizi - 2013 - Philosophy and the practice of Bayesian statistics.pdf:pdf},
issn = {2044-8317},
journal = {The British journal of mathematical and statistical psychology},
month = {feb},
number = {1},
pages = {8--38},
pmid = {22364575},
title = {{Philosophy and the practice of Bayesian statistics.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22364575},
volume = {66},
year = {2013}
}
@article{McLachlan1977,
author = {McLachlan, Geoffrey John},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/McLachlan - 1977 - Estimating the Linear Discriminant Function from Initial Samples Containing a Small Number of Unclassified Observatio.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {358},
pages = {403--406},
title = {{Estimating the Linear Discriminant Function from Initial Samples Containing a Small Number of Unclassified Observations}},
volume = {72},
year = {1977}
}
@article{Balcan2005,
abstract = {Semi Supervised Learning;},
author = {Balcan, M.F. and Blum, a. and Choi, P.P. and Lafferty, J. and Pantano, B. and Rwebangira, M.R. and Zhu, X.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balcan et al. - 2005 - Person identification in webcam images An application of semi-supervised learning.pdf:pdf},
journal = {ICML 2005 Workshop on Learning with Partially Classified Training Data},
pages = {6},
title = {{Person identification in webcam images: An application of semi-supervised learning}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.61.1706{\&}rep=rep1{\&}type=pdf},
volume = {2},
year = {2005}
}
@inproceedings{Arthur2007,
abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, ran- domized seeding technique, we obtain an algorithm that is $\Theta$(log k)-competitive with the optimal clustering. Prelim- inary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
author = {Arthur, David and Vassilvitskii, Sergei},
booktitle = {Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Arthur, Vassilvitskii - 2007 - k-means The Advantages of Careful Seeding.pdf:pdf},
pages = {1027--1035},
title = {{k-means ++ : The Advantages of Careful Seeding}},
year = {2007}
}
@article{Finney1990,
abstract = {This paper expresses a personal view of experiments involving repeated measurements; it attempts to classify types of experiment and approaches to statistical analysis: no novel procedures are propsed, only a systematic outlook in objectives and methods.},
author = {Finney, David J.},
doi = {10.1002/sim.4780090610},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/finney1990.pdf:pdf},
isbn = {1097-0258},
issn = {10970258},
journal = {Statistics in Medicine},
number = {6},
pages = {639--644},
title = {{Repeated measurements: What is measured and what repeats?}},
volume = {9},
year = {1990}
}
@article{VanRooden2010,
abstract = {The clinical variability between patients with Parkinson's disease (PD) may point at the existence of subtypes of the disease. Identification of subtypes is important, since a focus on homogeneous groups may enhance the chance of success of research on mechanisms of disease and may also lead to tailored treatment strategies. Cluster analysis (CA) is an objective method to classify patients into subtypes. We systematically reviewed the methodology and results of CA studies in PD to gain a better understanding of the robustness of identified subtypes. We found seven studies that fulfilled the inclusion criteria. Studies were limited by incomplete reporting and methodological limitations. Differences between studies rendered comparisons of the results difficult. However, it appeared that studies which applied a comparable design identified similar subtypes. The cluster profiles "old age-at-onset and rapid disease progression" and "young age-at-onset and slow disease progression" emerged from the majority of studies. Other cluster profiles were less consistent across studies. Future studies with a rigorous study design that is standardized with respect to the included variables, data processing, and CA technique may advance the knowledge on subtypes in PD.},
author = {van Rooden, Stephanie M and Heiser, Willem J and Kok, Joost N and Verbaan, Dagmar and van Hilten, Jacobus J and Marinus, Johan},
doi = {10.1002/mds.23116},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/van Rooden et al. - 2010 - The identification of Parkinson's disease subtypes using cluster analysis a systematic review.pdf:pdf},
issn = {1531-8257},
journal = {Movement disorders : official journal of the Movement Disorder Society},
keywords = {Algorithms,Cluster Analysis,Humans,Parkinson Disease,Parkinson Disease: classification,PubMed,PubMed: statistics {\&} numerical data},
month = {jun},
number = {8},
pages = {969--78},
pmid = {20535823},
title = {{The identification of Parkinson's disease subtypes using cluster analysis: a systematic review.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20535823},
volume = {25},
year = {2010}
}
@article{Buja2014,
author = {Buja, A and Brown, L},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Buja, Brown - 2014 - Discussion of A significance test for the lasso''.pdf:pdf},
journal = {The Annals of Statistics},
number = {2},
pages = {509--517},
title = {{Discussion of "A significance test for the lasso''}},
volume = {42},
year = {2014}
}
@inproceedings{Wolpert2002,
author = {Wolpert, David H},
booktitle = {Soft Computing and Industry},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wolpert - 2002 - The Supervised Learning No-Free-Lunch Theorems.pdf:pdf},
pages = {25--42},
title = {{The Supervised Learning No-Free-Lunch Theorems}},
url = {http://link.springer.com/chapter/10.1007/978-1-4471-0123-9{\_}3},
year = {2002}
}
@inproceedings{Pitelis2014,
author = {Pitelis, Nikolaos and Russell, Chris and Agapito, Lourdes},
booktitle = {ECML PKDD},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pitelis, Russell, Agapito - 2014 - Semi-supervised Learning Using an Unsupervised Atlas.pdf:pdf},
pages = {565--580},
title = {{Semi-supervised Learning Using an Unsupervised Atlas}},
year = {2014}
}
@article{Gelman2011,
author = {Gelman, Andrew},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman - 2011 - Induction and deduction in Bayesian data analysis.pdf:pdf},
journal = {Rationality, Markets and Morals (RMM)},
pages = {67--78},
title = {{Induction and deduction in Bayesian data analysis}},
url = {http://www.stat.columbia.edu/{~}gelman/research/unpublished/philosophy{\_}online4.pdf},
volume = {2},
year = {2011}
}
@article{Bishop1995,
author = {Bishop, Chris M},
doi = {10.1162/neco.1995.7.1.108},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bishop - 1995 - Training with Noise is Equivalent to Tikhonov Regularization.pdf:pdf},
journal = {Neural Computation},
number = {1},
pages = {108--116},
title = {{Training with Noise is Equivalent to Tikhonov Regularization}},
volume = {7},
year = {1995}
}
@article{Keogh2005,
author = {Keogh, Eamonn and Lin, Jessica},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Keogh, Lin - 2005 - Clustering of time-series subsequences is meaningless implications for previous and future research.pdf:pdf},
journal = {Knowledge and information systems},
number = {2},
pages = {154--177},
title = {{Clustering of time-series subsequences is meaningless: implications for previous and future research}},
url = {http://link.springer.com/article/10.1007/s10115-004-0172-7},
volume = {8},
year = {2005}
}
@inproceedings{Matti2006,
author = {Kaariainen, Matti},
booktitle = {International Joint Conference on Neural Networks},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kaariainen - 2006 - Semi-Supervised Model Selection Based on Cross-Validation.pdf:pdf},
number = {510},
title = {{Semi-Supervised Model Selection Based on Cross-Validation}},
year = {2006}
}
@article{Berrar2012,
author = {Berrar, Daniel},
doi = {10.1007/s10994-016-5612-6},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/art{\%}3A10.1007{\%}2Fs10994-016-5612-6.pdf:pdf},
issn = {1573-0565},
journal = {Machine Learning},
keywords = {Confidence curve,Significance test,p value,Multipl,confidence curve,multiple comparisons,p value,performance evaluation,significance test},
publisher = {Springer US},
title = {{Confidence curves : an alternative to null hypothesis significance testing for the comparison of classifiers}},
year = {2016}
}
@book{RamonyCajal1897a,
abstract = {Santiago Ramon y Cajal was a mythic figure in science. Hailed as the father of modernanatomy and neurobiology, he was largely responsible for the modern conception of the brain. Hisgroundbreaking works were New Ideas on the Structure of the Nervous System and Histology of the Nervous System in Man and Vertebrates. In addition to leaving alegacy of unparalleled scientific research, Cajal sought to educate the novice scientist about howscience was done and how he thought it should be done. This recently rediscovered classic, firstpublished in 1897, is an anecdotal guide for the perplexed new investigator as well as a refreshingresource for the old pro.Cajal was a pragmatist, aware of the pitfalls of beingtoo idealistic -- and he had a sense of humor, particularly evident in his diagnoses of variousstereotypes of eccentric scientists. The book covers everything from valuable personality traits foran investigator to social factors conducive to scientific work.},
author = {{Ram{\'{o}}n y Cajal}, Santiago},
booktitle = {Advice for a Young Investigator},
doi = {10.1016/S0166-2236(00)01546-0},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ram{\'{o}}n y Cajal - 1897 - Advice for a Young Investigator (translated by Neely Swanson and Larry W. Swanson).pdf:pdf},
isbn = {0262181916},
issn = {01662236},
number = {7},
pages = {1--150},
title = {{Advice for a Young Investigator (translated by Neely Swanson and Larry W. Swanson)}},
volume = {23},
year = {1897}
}
@inproceedings{Huang2006,
author = {Huang, Jiayuan and Smola, Alex and Gretton, Arthur and Borgwardt, Karsten M. and Sch{\"{o}}lkopf, Bernhard},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huang et al. - 2006 - Correcting sample selection bias by unlabeled data.pdf:pdf},
pages = {601--608},
title = {{Correcting sample selection bias by unlabeled data}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2006{\_}915.pdf},
year = {2006}
}
@article{Nguyen2010,
author = {Nguyen, XuanLong and Wainwright, Martin J. and Jordan, Michael I.},
doi = {10.1109/TIT.2010.2068870},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nguyen, Wainwright, Jordan - 2010 - Estimating Divergence Functionals and the Likelihood Ratio by Convex Risk Minimization.pdf:pdf},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
month = {nov},
number = {11},
pages = {5847--5861},
title = {{Estimating Divergence Functionals and the Likelihood Ratio by Convex Risk Minimization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5605355},
volume = {56},
year = {2010}
}
@article{Bian2014,
author = {Bian, Wei and Tao, Dacheng and Member, Senior},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bian, Tao, Member - 2014 - Asymptotic Generalization Bound of Fisher's Linear Discriminant Analysis.pdf:pdf},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {12},
pages = {2325--2337},
title = {{Asymptotic Generalization Bound of Fisher's Linear Discriminant Analysis}},
volume = {36},
year = {2014}
}
@unpublished{Hennig2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.02555v1},
author = {Hennig, Christian},
eprint = {arXiv:1502.02555v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hennig - 2015 - What are the true clusters.pdf:pdf},
keywords = {active scientific realism,cat-,comparison of clustering methods,constructivism,egorization,mixture models,natural kinds,variable},
title = {{What are the true clusters?}},
year = {2015}
}
@misc{RCoreTeam2016,
address = {Vienna, Austria},
author = {{R Core Team}},
institution = {R Foundation for Statistical Computing},
title = {{R: A Language and Environment for Statistical Computing}},
url = {https://www.r-project.org/},
year = {2016}
}
@article{Efron1977,
author = {Efron, Bradley and Morris, Carl},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Efron, Morris - 1977 - Stein's paradox in statistics.pdf:pdf},
journal = {Scientific American},
pages = {119--127},
title = {{Stein's paradox in statistics}},
url = {https://www.cs.nyu.edu/{~}roweis/csc2515-2006/readings/stein{\_}sciam.pdf},
year = {1977}
}
@inproceedings{Ho2000,
author = {Ho, Tin Kam},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho - 2000 - Complexity of Classification Problems and Comparative Advantages of Combined Classifiers.pdf:pdf},
pages = {97--106},
title = {{Complexity of Classification Problems and Comparative Advantages of Combined Classifiers}},
year = {2000}
}
@article{Freedman1983,
author = {Freedman, David A},
doi = {10.1080/00031305.1983.10482729},
isbn = {9780471463764},
issn = {0003-1305},
journal = {The American Statistician},
number = {2},
pages = {152--155},
title = {{A Note on Screening Regression Equations}},
url = {http://dx.doi.org/10.1002/0471463760.app1},
volume = {37},
year = {1983}
}
@article{Doiron2017,
author = {Doiron, Maxime and Langlois, M{\'{e}}lanie and Dupr{\'{e}}, Nicolas and Simard, Martine},
doi = {10.1002/gps.4735},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Doiron{\_}2017{\_}The influence of vascular risk factors on cognitive function in early Parkinson's disease.pdf:pdf},
issn = {10991166},
journal = {International Journal of Geriatric Psychiatry},
keywords = {Cognition,Dementia,Hypertension,Metabolic syndrome,Parkinson,Pulse pressure,Vascular risk factors},
pmid = {28509343},
title = {{The influence of vascular risk factors on cognitive function in early Parkinson's disease}},
year = {2017}
}
@inproceedings{Loog2012b,
author = {Loog, Marco and Jensen, Are C},
booktitle = {Structural, Syntactic, and Statistical Pattern Recognition (Lecture Notes in Computer Science Volume 7626)},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog, Jensen - 2012 - Constrained log-likelihood-based semi-supervised linear discriminant analysis.pdf:pdf},
pages = {327--335},
publisher = {Springer},
title = {{Constrained log-likelihood-based semi-supervised linear discriminant analysis}},
url = {http://www.springerlink.com/index/U16X1L3015777162.pdf},
year = {2012}
}
@inproceedings{Zhang2008,
abstract = {Linear discriminant analysis (LDA) is commonly used for dimensionality$\backslash$nreduction. In real-world applications where labeled data are scarce,$\backslash$nLDA does not work very well. However, unlabeled data are often available$\backslash$nin large quantities. We propose a novel semi-supervised discriminant$\backslash$nanalysis algorithm called SSDA . We utilize unlabeled data to maximize$\backslash$nan optimality criterion of LDA and use the constrained concave-convex$\backslash$nprocedure to solve the optimization problem. The optimization procedure$\backslash$nleads to estimation of the class labels for the unlabeled data. We$\backslash$npropose a novel confidence measure for selecting those unlabeled$\backslash$ndata points with high confidence. The selected unlabeled data can$\backslash$nthen be used to augment the original labeled data set for performing$\backslash$nLDA. We also propose a variant of SSDA , called M-SSDA , which adopts$\backslash$nthe manifold assumption to utilize the unlabeled data. Extensive$\backslash$nexperiments on many benchmark data sets demonstrate the effectiveness$\backslash$nof our proposed methods.},
author = {Zhang, Yu and Yeung, Dit Yan},
booktitle = {ECML PKDD},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang, Yeung - 2008 - Semi-supervised discriminant analysis via CCCP.pdf:pdf},
isbn = {3540874801},
issn = {03029743},
number = {PART 2},
pages = {644--659},
title = {{Semi-supervised discriminant analysis via CCCP}},
volume = {5212 LNAI},
year = {2008}
}
@article{Ho2002,
author = {Ho, Tin Kam and Basu, Mitra},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho, Basu - 2002 - Complexity Measures of Supervised Classification Problems.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {3},
pages = {289--300},
title = {{Complexity Measures of Supervised Classification Problems}},
volume = {24},
year = {2002}
}
@inproceedings{Lafferty2007,
author = {Lafferty, John D. and Wasserman, Larry},
booktitle = {Advances in Neural Information Processing Systems 20},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lafferty, Wasserman - 2007 - Statistical analysis of semi-supervised regression.pdf:pdf},
pages = {801----808},
title = {{Statistical analysis of semi-supervised regression}},
year = {2007}
}
@article{Goel2003,
author = {Goel, Prem K. and Ginebra, Josep},
doi = {10.1046/j.1467-9884.2003.00376.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Goel, Ginebra - 2003 - When is one experiment 'always better than' another.pdf:pdf},
issn = {00390526},
journal = {Journal of the Royal Statistical Society Series D: The Statistician},
keywords = {Comparison of experiments,Location experiments,Loewner ordering,Optimal design,Statistical information,Stochastic ordering,Sufficiency},
number = {4},
pages = {515--537},
title = {{When is one experiment 'always better than' another?}},
volume = {52},
year = {2003}
}
@article{Grunwald2007a,
abstract = {We show that forms of Bayesian and MDL inference that are often applied to classification problems can be {\{}$\backslash$em inconsistent{\}}.  This means that there exists a learning problem such that for all amounts of data the generalization errors of the MDL classifier and the Bayes classifier relative to the Bayesian posterior both remain bounded away from the smallest achievable generalization error. We extensively discuss the result from both a Bayesian and an MDL perspective.},
archivePrefix = {arXiv},
arxivId = {math/0406221},
author = {Gr{\"{u}}nwald, Peter and Langford, John},
doi = {10.1007/s10994-007-0716-7},
eprint = {0406221},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gr{\"{u}}nwald, Langford - 2007 - Suboptimal behavior of Bayes and MDL in classification under misspecification.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Bayesian statistics,Classification,Consistency,Inconsistency,Minimum description length,Misspecification},
number = {2-3},
pages = {119--149},
primaryClass = {math},
title = {{Suboptimal behavior of Bayes and MDL in classification under misspecification}},
volume = {66},
year = {2007}
}
@inproceedings{Cohen2014,
author = {Cohen, Taco and Welling, Max},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cohen, Welling - 2014 - Learning the Irreducible Representations of Commutative Lie Groups.pdf:pdf},
pages = {1755--1763},
title = {{Learning the Irreducible Representations of Commutative Lie Groups}},
year = {2014}
}
@article{Eddelbuettel2011,
author = {Eddelbuettel, Dirk and Francois, Romain},
doi = {10.18637/jss.v040.i08},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Eddelbuettel, Francois - 2011 - Rcpp Seamless R and C Integration.pdf:pdf},
journal = {Journal of Statistical Software},
number = {1},
pages = {1--18},
title = {{Rcpp : Seamless R and C ++ Integration}},
volume = {40},
year = {2011}
}
@article{Stigler2013,
author = {Stigler, Stephen M.},
doi = {10.1214/13-STS438},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Stigler - 2013 - The True Title of Bayes's Essay.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Thomas Bayes, Richard Price, Bayes's theorem, hist,and phrases},
month = {aug},
number = {3},
pages = {283--288},
title = {{The True Title of Bayes's Essay}},
url = {http://projecteuclid.org/euclid.ss/1377696937},
volume = {28},
year = {2013}
}
@inproceedings{VanderMaaten2013,
author = {{Van der Maaten}, Laurens and Chen, Minmin and Tyree, Stephen and Weinberger, Kilian Q.},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Van der Maaten et al. - 2013 - Learning with Marginalized Corrupted Features.pdf:pdf},
pages = {410--418},
title = {{Learning with Marginalized Corrupted Features}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/ICML2013{\_}vandermaaten13},
year = {2013}
}
@inproceedings{Kopf2000,
author = {K{\"{o}}pf, Christian and Taylor, Charles and Keller, Jorg},
booktitle = {Proceedings of the PKDD-00 workshop on data mining, decision support, meta-learning and ILP},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/K{\"{o}}pf, Taylor, Keller - 2000 - Meta-analysis from data characterisation for meta-learning to meta-regression.pdf:pdf},
number = {Ml},
title = {{Meta-analysis: from data characterisation for meta-learning to meta-regression}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.26.8159},
year = {2000}
}
@article{Vehtari2016,
abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparing of predictive errors between two models. We implement the computations in an R package called 'loo' and demonstrate using models fit with the Bayesian inference package Stan.},
archivePrefix = {arXiv},
arxivId = {arXiv:1507.04544v4},
author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
doi = {10.1007/s11222-016-9696-4},
eprint = {arXiv:1507.04544v4},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/1507.04544v5.pdf:pdf},
issn = {15731375},
journal = {Statistics and Computing},
keywords = {Bayesian computation,K-fold cross-validation,Leave-one-out cross-validation (LOO),Pareto smoothed importance sampling (PSIS),Stan,Widely applicable information criterion (WAIC)},
number = {September},
pages = {1--20},
title = {{Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC}},
year = {2016}
}
@article{Kawakita2014a,
abstract = {We are interested in developing a safe semi-supervised learning that works in any situation. Semi-supervised learning postulates that n(') unlabeled data are available in addition to n labeled data. However, almost all of the previous semi-supervised methods require additional assumptions (not only unlabeled data) to make improvements on supervised learning. If such assumptions are not met, then the methods possibly perform worse than supervised learning. Sokolovska, Capp{\'{e}}, and Yvon (2008) proposed a semi-supervised method based on a weighted likelihood approach. They proved that this method asymptotically never performs worse than supervised learning (i.e., it is safe) without any assumption. Their method is attractive because it is easy to implement and is potentially general. Moreover, it is deeply related to a certain statistical paradox. However, the method of Sokolovska et al. (2008) assumes a very limited situation, i.e., classification, discrete covariates, n(')→∞ and a maximum likelihood estimator. In this paper, we extend their method by modifying the weight. We prove that our proposal is safe in a significantly wide range of situations as long as n≤n('). Further, we give a geometrical interpretation of the proof of safety through the relationship with the above-mentioned statistical paradox. Finally, we show that the above proposal is asymptotically safe even when n('){\textless}n by modifying the weight. Numerical experiments illustrate the performance of these methods.},
author = {Kawakita, Masanori and Takeuchi, Jun'ichi Jun'ichi},
doi = {10.1016/j.neunet.2014.01.016},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kawakita, Takeuchi - 2014 - Safe semi-supervised learning based on weighted likelihood.pdf:pdf;:Users/jkrijthe/Documents/Mendeley Desktop/Kawakita, Takeuchi - 2014 - Safe semi-supervised learning based on weighted likelihood(2).pdf:pdf},
journal = {Neural Networks},
keywords = {semi-supervised learning},
month = {may},
pages = {146--164},
publisher = {Elsevier Ltd},
title = {{Safe semi-supervised learning based on weighted likelihood}},
volume = {53},
year = {2014}
}
@article{Vilalta2002,
author = {Vilalta, Ricardo and Drissi, Youssef},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vilalta, Drissi - 2002 - A perspective view and survey of meta-learning.pdf:pdf},
journal = {Artificial Intelligence Review},
keywords = {classification,inductive learning,meta-knowledge},
number = {1997},
pages = {77--95},
title = {{A perspective view and survey of meta-learning}},
url = {http://link.springer.com/article/10.1023/A:1019956318069},
year = {2002}
}
@unpublished{Liu2014a,
author = {Liu, Mingxia and Zhang, Daoqiang},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Liu, Zhang - 2014 - CGS A Novel Pairwise Constraint-Guided Sparse Feature Selection Method.pdf:pdf},
title = {{CGS: A Novel Pairwise Constraint-Guided Sparse Feature Selection Method}},
year = {2014}
}
@article{Minka2005,
abstract = {This paper presents a unifying view of message-passing algorithms, as methods to approximate a complex Bayesian network by a simpler network with minimum information divergence. In this view, the difference between mean-field methods and belief propagation is not the amount of structure they model, but only the measure of loss they minimize (`exclusive' versus `inclusive' Kullback-Leibler divergence). In each case, message-passing arises by minimizing a localized version of the divergence, local to each factor. By examining these divergence measures, we can intuit the types of solution they prefer (symmetry-breaking, for example) and their suitability for different tasks. Furthermore, by considering a wider variety of divergence measures (such as alpha-divergences), we can achieve different complexity and performance goals.},
author = {Minka, Thomas},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Minka - 2005 - Divergence measures and message passing.pdf:pdf},
pages = {MSR--TR--2005--173},
title = {{Divergence measures and message passing}},
year = {2005}
}
@unpublished{Graves2014,
archivePrefix = {arXiv},
arxivId = {1410.5401},
author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
eprint = {1410.5401},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Graves, Wayne, Danihelka - 2014 - Neural Turing Machines.pdf:pdf},
month = {oct},
pages = {1--26},
title = {{Neural Turing Machines}},
url = {http://arxiv.org/abs/1410.5401v1},
year = {2014}
}
@article{Zeiler2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1311.2901v3},
author = {Zeiler, Matthew D and Fergus, Rob},
eprint = {arXiv:1311.2901v3},
title = {{Visualizing and Understanding Convolutional Networks}},
year = {2013}
}
@article{Gui2016,
author = {Gui, Jie and Liu, Tongliang and Tao, Dacheng and Sun, Zhenan and Tan, Tieniu},
doi = {10.1109/TCYB.2015.2457234},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gui et al. - 2016 - Representative Vector Machines A Unified Framework for Classical Classifiers.pdf:pdf},
issn = {21682267},
journal = {IEEE Transactions on Cybernetics},
keywords = {Discriminant vector machine (DVM),pattern classification,representative vector machines (RVMs),sparse representation,support vector machines (SVMs)},
number = {8},
pages = {1877--1888},
title = {{Representative Vector Machines: A Unified Framework for Classical Classifiers}},
volume = {46},
year = {2016}
}
@article{Moen2016,
abstract = {The application of social network analysis to the organization of healthcare delivery is a relatively new area of research that may not be familiar to health services statisticians and other methodologists. We present a methodological introduction to social network analysis with a case study of physicians' adherence to clinical guidelines regarding use of implantable cardioverter defibrillators (ICDs) for the prevention of sudden cardiac death. We focus on two hospital referral regions in Indiana, Gary and South Bend, characterized by different rates of evidence-based ICD use (86 and 66 {\%}, respectively). Using Medicare Part B claims, we construct a network of physicians who care for cardiovascular disease patients based on patient-sharing relationships. Approaches for weighting physician dyads and aggregating physician dyads by hospital are discussed. Then, we obtain a set of weighted network statistics for the positions of hospitals in their referral region, global statistics for the physician network within each hospital, and of the network positions of individual physicians within hospitals, providing the mathematical specification and sociological intuition underlying each measure. We find that adjusting for network measures can reduce the observed differences between referral regions for evidence-based ICD therapy. This study supports previous reports on how variation in physician network structure relates to utilization of care, and motivates future work using physician network measures to examine variation in evidence-based medicine. Copyright {\textcopyright} 2016, Springer Science+Business Media New York.},
author = {Moen, Erika L. and Austin, Andrea M. and Bynum, Julie P. and Skinner, Jonathan S. and O'Malley, A. James},
doi = {10.1007/s10742-016-0152-x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/10.1007-s10742-016-0152-x.pdf:pdf},
isbn = {1387-3741},
issn = {15729400},
journal = {Health Services and Outcomes Research Methodology},
keywords = {Centrality,Degree distribution,Evidence-based medicine,Exponential random graph model,Implantable cardioverter defibrillators,Social network analysis,Structural equivalence},
number = {3},
pages = {132--153},
pmid = {27597812},
publisher = {Springer US},
title = {{An analysis of patient-sharing physician networks and implantable cardioverter defibrillator therapy}},
volume = {16},
year = {2016}
}
@article{Breiman1996,
author = {Breiman, Leo},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Breiman - 1996 - Bagging predictors.pdf:pdf},
journal = {Machine learning},
keywords = {aggregation,averaging,bootstrap,combining},
pages = {123--140},
title = {{Bagging predictors}},
url = {http://www.springerlink.com/index/L4780124W2874025.pdf},
volume = {140},
year = {1996}
}
@article{Belkin2006,
author = {Belkin, Mikhail and Niyogi, Partha and Sindhwani, Vikas},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Belkin, Niyogi, Sindhwani - 2006 - Manifold regularization A geometric framework for learning from labeled and unlabeled examples.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {fold learning,graph transduction,kernel methods,mani-,regularization,semi-supervised learning,spectral graph theory,support vector machines,unlabeled data},
pages = {2399--2434},
title = {{Manifold regularization: A geometric framework for learning from labeled and unlabeled examples}},
volume = {7},
year = {2006}
}
@article{Dempster1977,
author = {Dempster, AP and Laird, NM and Rubin, DB},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dempster, Laird, Rubin - 1977 - Maximum likelihood from incomplete data via the EM algorithm.pdf:pdf},
isbn = {0000000779},
journal = {Journal of the Royal Statistical Society. Series B},
keywords = {incomplete,likelihood,maximum},
number = {1},
pages = {1--38},
title = {{Maximum likelihood from incomplete data via the EM algorithm}},
volume = {39},
year = {1977}
}
@article{Marcum2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1512.02914v1},
author = {Marcum, Christopher Steven},
eprint = {arXiv:1512.02914v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Marcum - 2015 - Yet Another Statistical Analysis of Bob Ross Paintings.pdf:pdf},
keywords = {art,bob ross,linear subspace,paintings},
pages = {1--16},
title = {{Yet Another Statistical Analysis of Bob Ross Paintings}},
year = {2015}
}
@inproceedings{Loogc,
author = {Loog, M. and van Ginneken, B.},
booktitle = {Proceedings of the International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2002.1048456},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog, van Ginneken - 2002 - Supervised Segmentation by Iterated Contextual Pixel Classificatio.pdf:pdf},
pages = {925--928},
title = {{Supervised Segmentation by Iterated Contextual Pixel Classificatio}},
year = {2002}
}
@phdthesis{Druck2011,
author = {Druck, Gregory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Druck - 2011 - Generalized Expectation Criteria for Lightly Supervised Learning.pdf:pdf},
number = {September},
title = {{Generalized Expectation Criteria for Lightly Supervised Learning}},
url = {http://scholarworks.umass.edu/open{\_}access{\_}dissertations/440/},
year = {2011}
}
@unpublished{Blum2001,
author = {Blum, Avrim and Chawla, S},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Blum, Chawla - 2001 - Learning from labeled and unlabeled data using graph mincuts.pdf:pdf},
institution = {Carnegie Mellon University, Computer Science Department},
title = {{Learning from labeled and unlabeled data using graph mincuts}},
url = {http://repository.cmu.edu/compsci/163/?utm{\_}source=repository.cmu.edu{\%}2Fcompsci{\%}2F163{\&}utm{\_}medium=PDF{\&}utm{\_}campaign=PDFCoverPages},
year = {2001}
}
@article{Brazdil2003a,
author = {Brazdil, Pavel B. and Soares, Carlos and Costa, JP Da},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brazdil, Soares, Costa - 2003 - Ranking learning algorithms Using IBL and meta-learning on accuracy and time results.pdf:pdf},
journal = {Machine Learning},
keywords = {algorithm recommendation,data characterization,meta-learning,ranking},
pages = {251--277},
title = {{Ranking learning algorithms: Using IBL and meta-learning on accuracy and time results}},
url = {http://link.springer.com/article/10.1023/A:1021713901879},
volume = {50},
year = {2003}
}
@article{Ma2015,
author = {Ma, Jianping and Jiang, Jin},
doi = {10.1016/j.net.2014.12.005},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ma, Jiang - 2015 - Semisupervised classification for fault diagnosis in nuclear power plants.pdf:pdf},
issn = {17385733},
journal = {Nuclear Engineering and Technology},
number = {2},
pages = {176--186},
publisher = {Elsevier B.V},
title = {{Semisupervised classification for fault diagnosis in nuclear power plants}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1738573315000054},
volume = {47},
year = {2015}
}
@article{Zhang2004a,
author = {Zhang, Tong},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang - 2004 - Statistical Behavior and Consistency of Classification Methods Based on Convex Risk Minimization.pdf:pdf},
journal = {The Annals of Statistics},
number = {1},
pages = {56--134},
title = {{Statistical Behavior and Consistency of Classification Methods Based on Convex Risk Minimization}},
url = {http://www.jstor.org/stable/10.2307/3448494},
volume = {32},
year = {2004}
}
@article{Ben-David2015a,
abstract = {It is well known that most of the common clustering objectives are NP-hard to optimize. In practice, however, clustering is being routinely carried out. One approach for providing theoretical understanding of this seeming discrepancy is to come up with notions of clusterability that distinguish realistically interesting input data from worst-case data sets. The hope is that there will be clustering algorithms that are provably efficient on such "clusterable" instances. This paper addresses the thesis that the computational hardness of clustering tasks goes away for inputs that one really cares about. In other words, that "Clustering is difficult only when it does not matter" (the $\backslash$emph{\{}CDNM thesis{\}} for short). I wish to present a a critical bird's eye overview of the results published on this issue so far and to call attention to the gap between available and desirable results on this issue. A longer, more detailed version of this note is available as arXiv:1507.05307. I discuss which requirements should be met in order to provide formal support to the the CDNM thesis and then examine existing results in view of these requirements and list some significant unsolved research challenges in that direction.},
archivePrefix = {arXiv},
arxivId = {1510.05336},
author = {Ben-David, Shai},
eprint = {1510.05336},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-David - 2015 - Clustering is Easy When ....What.pdf:pdf},
pages = {2--7},
title = {{Clustering is Easy When ....What?}},
url = {http://arxiv.org/abs/1510.05336},
year = {2015}
}
@misc{Hanczar2015,
author = {Hanczar, Blaise and Zucker, Jean-Daniel},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hanczar, Zucker - 2015 - A generic approach to optimizing abstaining area for small sample data classification.pdf:pdf},
keywords = {abstract,accuracy relies on the,an approach to improve,classifiers,given a classification task,in the feature space,not,reject option,reliable enough,small-sample setting,supervised learning,these classifiers are trained,these rejected observations belong,to an abstaining area,to reject observations for,two,use of abstaining,which predicted values are},
title = {{A generic approach to optimizing abstaining area for small sample data classification}},
year = {2015}
}
@incollection{Jain1982,
author = {Jain, A K and Chandrasekaran, B},
booktitle = {Handbook of Statistics},
editor = {Krishnaiah, P. R. and Kanal, L.N.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jain, Chandrasekaran - 1982 - Dimensionality and Sample Size Considerations in Pattern Recognition Practice.pdf:pdf},
pages = {835--855},
publisher = {North-Holland Publishing Company},
title = {{Dimensionality and Sample Size Considerations in Pattern Recognition Practice}},
volume = {2},
year = {1982}
}
@inproceedings{Szummer2001,
author = {Szummer, Martin and Jaakkola, Tommi},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Szummer, Jaakkola - 2001 - Clustering and efficient use of unlabeled examples.pdf:pdf},
title = {{Clustering and efficient use of unlabeled examples}},
url = {http://www.ai.mit.edu/projects/ntt/projects/MIT2000-08/documents/SzummerJaakkola.pdf},
year = {2001}
}
@unpublished{Looga,
author = {Loog, Marco and Jensen, Are C},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog, Jensen - Unknown - A Constrained Log-Likelihood Formulation for Semi-Supervised Nearest Mean Classification.pdf:pdf},
keywords = {constrained estimation,log-likelihood,nearest mean classifier,semi-supervised learning},
number = {1},
title = {{A Constrained Log-Likelihood Formulation for Semi-Supervised Nearest Mean Classification}},
volume = {1}
}
@article{Witten2011,
abstract = {We consider the supervised classification setting, in which the data consist of p features measured on n observations, each of which belongs to one of K classes. Linear discriminant analysis (LDA) is a classical method for this problem. However, in the high-dimensional setting where p ≫ n, LDA is not appropriate for two reasons. First, the standard estimate for the within-class covariance matrix is singular, and so the usual discriminant rule cannot be applied. Second, when p is large, it is difficult to interpret the classification rule obtained from LDA, since it involves all p features. We propose penalized LDA, a general approach for penalizing the discriminant vectors in Fisher's discriminant problem in a way that leads to greater interpretability. The discriminant problem is not convex, so we use a minorization-maximization approach in order to efficiently optimize it when convex penalties are applied to the discriminant vectors. In particular, we consider the use of L(1) and fused lasso penalties. Our proposal is equivalent to recasting Fisher's discriminant problem as a biconvex problem. We evaluate the performances of the resulting methods on a simulation study, and on three gene expression data sets. We also survey past methods for extending LDA to the high-dimensional setting, and explore their relationships with our proposal.},
author = {Witten, Daniela M. and Tibshirani, Robert},
doi = {10.1111/j.1467-9868.2011.00783.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Witten, Tibshirani - 2011 - Penalized classification using Fisher's linear discriminant.pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society. Series B, Statistical methodology},
keywords = {classification,discriminant analysis,feature selection,high dimensional problems,lasso,linear,supervised learning},
month = {nov},
number = {5},
pages = {753--772},
pmid = {22323898},
title = {{Penalized classification using Fisher's linear discriminant.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3272679{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {73},
year = {2011}
}
@article{Bengio2004,
author = {Bengio, Yoshua and Grandvalet, Yves},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bengio, Grandvalet - 2004 - No unbiased estimator of the variance of k-fold cross-validation.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {cross-validation,k-fold cross-validation,statistical comparisons,variance estimators},
pages = {1089--1105},
title = {{No unbiased estimator of the variance of k-fold cross-validation}},
volume = {5},
year = {2004}
}
@article{Eddelbuettel2014,
author = {Eddelbuettel, Dirk and Sanderson, Conrad},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Eddelbuettel, Sanderson - 2014 - Rcpparmadillo Accelerating R with high-performance C linear algebra.pdf:pdf},
journal = {Computational Statistics {\&} Data Analysis},
keywords = {c,linear algebra,r,software},
pages = {1054--1063},
title = {{Rcpparmadillo: Accelerating R with high-performance C++ linear algebra}},
url = {http://www.sciencedirect.com/science/article/pii/S0167947313000492},
volume = {71},
year = {2014}
}
@article{Kingma2014,
abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
archivePrefix = {arXiv},
arxivId = {1406.5298},
author = {Kingma, Diederik P. and Rezende, Danilo J. and Mohamed, Shakir and Welling, Max},
eprint = {1406.5298},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kingma et al. - 2014 - Semi-Supervised Learning with Deep Generative Models.pdf:pdf},
month = {jun},
pages = {1--9},
title = {{Semi-Supervised Learning with Deep Generative Models}},
url = {http://arxiv.org/abs/1406.5298},
year = {2014}
}
@book{Jaynes,
author = {Jaynes, E.T.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jaynes - 2003 - Probability Theory The Logic of Science.pdf:pdf},
title = {{Probability Theory: The Logic of Science}},
year = {2003}
}
@article{Azizyan2013,
author = {Azizyan, Martin and Singh, Aarti and Wasserman, Larry},
doi = {10.1214/13-AOS1092},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Azizyan, Singh, Wasserman - 2013 - Density-sensitive semisupervised inference.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {apr},
number = {2},
pages = {751--771},
title = {{Density-sensitive semisupervised inference}},
url = {http://projecteuclid.org/euclid.aos/1368018172},
volume = {41},
year = {2013}
}
@inproceedings{Krijthe2016a,
archivePrefix = {arXiv},
arxivId = {1610.03713},
author = {Krijthe, Jesse Hendrik and Loog, Marco},
booktitle = {Proceedings of the 23rd International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2016.7899877},
eprint = {1610.03713},
pages = {1677--1682},
title = {{Optimistic Semi-supervised Least Squares Classification}},
year = {2016}
}
@inproceedings{Rosset2004,
author = {Rosset, Saharon and Zhu, Ji and Zou, Hui and Hastie, Trevor J},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rosset et al. - 2004 - A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning.pdf:pdf},
pages = {1161--1168},
title = {{A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning}},
year = {2004}
}
@article{Witten2010,
abstract = {We consider the problem of clustering observations using a potentially large set of features. One might expect that the true underlying clusters present in the data differ only with respect to a small fraction of the features, and will be missed if one clusters the observations using the full set of features. We propose a novel framework for sparse clustering, in which one clusters the observations using an adaptively chosen subset of the features. The method uses a lasso-type penalty to select the features. We use this framework to develop simple methods for sparse K-means and sparse hierarchical clustering. A single criterion governs both the selection of the features and the resulting clusters. These approaches are demonstrated on simulated data and on genomic data sets.},
author = {Witten, Daniela M. and Tibshirani, Robert},
doi = {10.1198/jasa.2010.tm09415},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Witten, Tibshirani - 2010 - A framework for feature selection in clustering.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {hierarchical clustering,high-dimensional,k-means clustering,lasso,model selection,sparsity,unsupervised learning},
month = {jun},
number = {490},
pages = {713--726},
pmid = {20811510},
title = {{A framework for feature selection in clustering.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2930825{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {105},
year = {2010}
}
@article{Li2011a,
abstract = {Classifying biological data into different groups is a central task of bioinformatics: for instance, to predict the function of a gene or protein, the disease state of a patient or the phenotype of an individual based on its genotype. Support Vector Machines are a wide spread approach for classifying biological data, due to their high accuracy, their ability to deal with structured data such as strings, and the ease to integrate various types of data. However, it is unclear how to correct for confounding factors such as population structure, age or gender or experimental conditions in Support Vector Machine classification.},
author = {Li, Limin and Rakitsch, Barbara and Borgwardt, Karsten},
doi = {10.1093/bioinformatics/btr204},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li, Rakitsch, Borgwardt - 2011 - ccSVM Correcting Support Vector Machines for confounding factors in biological data classification.pdf:pdf},
issn = {13674803},
journal = {Bioinformatics},
number = {13},
pmid = {21685091},
title = {{ccSVM: Correcting Support Vector Machines for confounding factors in biological data classification}},
volume = {27},
year = {2011}
}
@inproceedings{Singh2008,
author = {Singh, Aarti and Nowak, Robert D. and Zhu, Xiaojin},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Singh, Nowak, Zhu - 2008 - Unlabeled data Now it helps, now it doesn't.pdf:pdf},
pages = {1513--1520},
title = {{Unlabeled data: Now it helps, now it doesn't}},
year = {2008}
}
@article{Walker2016,
abstract = {The eukaryotic RNA exosome is an essential, multi-subunit complex that catalyzes RNA turnover, maturation, and quality control processes. Its non-catalytic donut-shaped core includes 9 subunits that associate with the 3' to 5' exoribonucleases Rrp6, and Rrp44/Dis3, a subunit that also catalyzes endoribonuclease activity. Although recent structures and biochemical studies of RNA bound exosomes from S. cerevisiae revealed that the Exo9 central channel guides RNA to either Rrp6 or Rrp44 using partially overlapping and mutually exclusive paths, several issues related to RNA recruitment remain. Here, we identify activities for the highly basic Rrp6 C-terminal tail that we term the 'lasso' because it binds RNA and stimulates ribonuclease activities associated with Rrp44 and Rrp6 within the 11-subunit nuclear exosome. Stimulation is dependent on the Exo9 central channel, and the lasso contributes to degradation and processing activities of exosome substrates in vitro and in vivo. Finally, we present evidence that the Rrp6 lasso may be a conserved feature of the eukaryotic RNA exosome.},
archivePrefix = {arXiv},
arxivId = {1611.06654},
author = {Walker, S G},
doi = {10.1093/tropej/fmw080},
eprint = {1611.06654},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/asx010.pdf:pdf},
isbn = {2076792171},
issn = {0142-6338},
journal = {Journal Of Tropical Pediatrics},
number = {March},
pages = {497--503},
pmid = {27899565},
title = {{OUP accepted manuscript}},
url = {https://academic.oup.com/tropej/article-lookup/doi/10.1093/tropej/fmw080},
year = {2016}
}
@article{VanderKooi2013,
abstract = {OBJECTIVES: We investigated how much the Human Development Index (HDI), a global measure of development, modifies the effect of education on self-reported health.

METHODS: We analyzed cross-sectional World Health Survey data on 217,642 individuals from 49 countries, collected in 2002 to 2005, with random-intercept multilevel linear regression models.

RESULTS: We observed greater positive associations between educational levels and self-reported good health with increasing HDI. The magnitude of this effect modification of the education-health relation tended to increase with educational attainment. For example, before adjustment for effect modification, at comparable HDI, on average, finishing primary school was associated with better general health (b = 1.49; 95{\%} confidence interval [CI] = 1.18, 1.80). With adjustment for effect modification by HDI, the impact became 4.63 (95{\%} CI = 3.63, 5.62) for every 0.1 increase in HDI. Among those who completed high school, these associations were, respectively, 5.59 (95{\%} CI = 5.20, 5.98) and 9.95 (95{\%} CI = 8.89, 11.00).

CONCLUSIONS: The health benefits of educational attainment are greater in countries with greater human development. Health inequalities attributable to education are, therefore, larger in more developed countries.},
author = {van der Kooi, Anne L F and Stronks, Karien and Thompson, Caroline a and DerSarkissian, Maral and Arah, Onyebuchi a},
doi = {10.2105/AJPH.2013.301593},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/van der Kooi et al. - 2013 - The modifying influence of country development on the effect of individual educational attainment on self-r.pdf:pdf},
issn = {1541-0048},
journal = {American journal of public health},
keywords = {Adult,Aged,Cross-Sectional Studies,Developed Countries,Developed Countries: statistics {\&} numerical data,Developing Countries,Developing Countries: statistics {\&} numerical data,Educational Status,Female,Health Status,Health Status Disparities,Health Surveys,Humans,Male,Middle Aged,Self Report,Young Adult},
month = {nov},
number = {11},
pages = {e49--54},
pmid = {24028233},
title = {{The modifying influence of country development on the effect of individual educational attainment on self-rated health.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24028233},
volume = {103},
year = {2013}
}
@article{Hunt1989,
author = {Hunt, D. N. and Triggs, C. M.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hunt, Triggs - 1989 - Iterative Missing Value Estimation Authors.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
number = {2},
pages = {293--300},
title = {{Iterative Missing Value Estimation Authors}},
volume = {38},
year = {1989}
}
@article{Loog2016,
abstract = {Improvement guarantees for semi-supervised classifiers can currently only be given under restrictive conditions on the data. We propose a general way to perform semi-supervised parameter estimation for likelihood-based classifiers for which, on the full training set, the estimates are never worse than the supervised solution in terms of the log-likelihood. We argue, moreover, that we may expect these solutions to really improve upon the supervised classifier in particular cases. In a worked-out example for LDA, we take it one step further and essentially prove that its semi-supervised version is strictly better than its supervised counterpart. The two new concepts that form the core of our estimation principle are contrast and pessimism. The former refers to the fact that our objective function takes the supervised estimates into account, enabling the semi-supervised solution to explicitly control the potential improvements over this estimate. The latter refers to the fact that our estimates are conservative and therefore resilient to whatever form the true labeling of the unlabeled data takes on. Experiments demonstrate the improvements in terms of both the log-likelihood and the classification error rate on independent test sets.},
archivePrefix = {arXiv},
arxivId = {1503.00269},
author = {Loog, Marco},
eprint = {1503.00269},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - 2016 - Contrastive Pessimistic Likelihood Estimation for Semi-Supervised Classification.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {contrast,ear discriminant analysis,lin-,maximum likelihood,pessimism,semi-supervised learning},
number = {3},
pages = {462--475},
title = {{Contrastive Pessimistic Likelihood Estimation for Semi-Supervised Classification}},
volume = {38},
year = {2016}
}
@techreport{Dixon1973,
author = {Dixon, John K.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dixon - 1973 - Pattern Recognition with Partly Missing Data.pdf:pdf},
title = {{Pattern Recognition with Partly Missing Data}},
year = {1973}
}
@article{Gelman2013d,
abstract = {Posterior predictive p-values do not in general have uniform distributions under the null hypothesis (except in the special case of ancillary test variables) but instead tend to have distributions more concentrated near 0.5. From different perspectives, such nonuniform distributions have been portrayed as desirable (as reflecting an ability of vague prior distributions to nonetheless yield accurate posterior predictions) or undesirable (as making it more difficult to reject a false model). We explore this tension through two simple normal-distribution examples. In one example, we argue that the low power of the posterior predictive check is desirable from a statistical perspective; in the other, the posterior predictive check seems inappropriate. Our conclusion is that the relevance of the p-value depends on the applied context, a point which (ironically) can be seen even in these two toy examples.},
archivePrefix = {arXiv},
arxivId = {math.PR/0000000},
author = {Gelman, Andrew},
doi = {10.1214/13-EJS854},
eprint = {0000000},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/ppc{\_}understand3.pdf:pdf},
isbn = {1935-7524},
issn = {19357524},
journal = {Electronic Journal of Statistics},
keywords = {Bayesian inference,Model checking,P-value,Posterior predictive check,U-value},
number = {1},
pages = {2595--2602},
primaryClass = {math.PR},
title = {{Two simple examples for understanding posterior p-values whose distributions are far from unform}},
volume = {7},
year = {2013}
}
@phdthesis{Hillebrand2012,
author = {Hillebrand, Arne},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hillebrand - 2012 - Separating a polygonal environment into a multi-layered environment.pdf:pdf},
keywords = {branch,explicit corridor map,ge-,graphs,local search,multi-layered environment,multicut,netic algorithm,price},
school = {Utrecht University},
title = {{Separating a polygonal environment into a multi-layered environment}},
year = {2012}
}
@article{Patil2016,
author = {Patil, Prasad and Peng, Roger D. and Leek, Jeffrey T.},
doi = {10.1101/066803},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Patil, Peng, Leek - 2016 - A statistical definition for reproducibility and replicability.pdf:pdf},
journal = {Biorxiv},
title = {{A statistical definition for reproducibility and replicability}},
url = {http://dx.doi.org/10.1101/066803},
year = {2016}
}
@techreport{Poggio2001,
author = {Poggio, T and Mukherjee, S and Rifkin, R and Rakhlin, A and Verri, A},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Poggio et al. - 2001 - b.pdf:pdf},
title = {b},
year = {2001}
}
@article{Gelman2012a,
author = {Gelman, Andrew and Hill, Jennifer and Yajima, Masanao},
doi = {10.1080/19345747.2011.618213},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Hill, Yajima - 2012 - Why We (Usually) Don't Have to Worry About Multiple Comparisons.pdf:pdf},
issn = {1934-5747},
journal = {Journal of Research on Educational Effectiveness},
keywords = {bayesian inference,hierarchical modeling,multiple comparisons,statis-,type s error},
month = {apr},
number = {2},
pages = {189--211},
title = {{Why We (Usually) Don't Have to Worry About Multiple Comparisons}},
url = {http://www.tandfonline.com/doi/abs/10.1080/19345747.2011.618213},
volume = {5},
year = {2012}
}
@article{Senn2011a,
abstract = {It is a common belief that individual variation in response to treatment is an important explanation for the variation in observed outcomes in clinical trials. If such variation is large, it seems reasonable to suppose that progress in treating disease will be advanced by classifying patients according to their abilities or not to ‘respond' to particular treatments. We consider that there is currently a lost opportunity in drug development. There is a great deal of talk about individual response to treatment and tailor-made drugs. However, relatively little work is being done to formally investigate, using suitable designs, where individual response to treatment may be important. Through a case study from a replicate cross-over study we show how, given suitable replication, it is possible to isolate the component of variation corresponding to patient- by-treatment interaction and hence investigate the possibility of individual response to treatment.},
author = {Senn, S. and Rolfe, K. and Julious, S. A.},
doi = {10.1177/0962280210379174},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/0962280210379174.pdf:pdf},
issn = {0962-2802},
journal = {Statistical Methods in Medical Research},
keywords = {components of,personalised medicine,replicate cross-over,treatment response,within subject variability},
number = {6},
pages = {657--666},
pmid = {20739334},
title = {{Investigating variability in patient response to treatment - a case study from a replicate cross-over study}},
url = {http://smm.sagepub.com/cgi/doi/10.1177/0962280210379174},
volume = {20},
year = {2011}
}
@inproceedings{Cervone2014,
abstract = {Basketball is a game of decisions; at any moment, a player can change the character of a possession by choosing to pass, dribble, or shoot. The current state of basketball analytics, however, provides no way to quantitatively evaluate the vast majority of decisions that players make, as most metrics are driven by events that occur at or near the end of a possession, such as points, turnovers, and assists. We propose a framework for using plater-tracking data to assign a point value to each moment of a possession by computing how many points the offense is expected to score by the end of the possession, a quantity we call expected possession value (EPV). EPV allows analysts to evaluate every decision made during a basketball game - whether it is to pass, dribble, or shoot - opening the door for a multitude of new metrics and analyses of basketball that quantify value in terms of points. In this paper, we propose a modeling framework for estimating EPV, present results of EPV computations performed using player-tracking data from the 2012-2013 season, adn provide several examples of EPV-derived metrics that answer real basketball questions.},
author = {Cervone, Dan and D'Amour, Alexander and Bornn, Luke and Goldsberry, Kirk},
booktitle = {SLOAN Sports Analytics Conference},
pages = {1--9},
title = {{POINTWISE: Predicting Points and Valuing Decisions in Real Time with NBA Optical Tracking Data}},
url = {http://www.sloansportsconference.com/wp-content/uploads/2014/02/2014{\_}SSAC{\_}Pointwise-Predicting-Points-and-Valuing-Decisions-in-Real-Time.pdf http://dl.frz.ir/FREE/papers-we-love/sports{\_}analytics/2014-ssac-pointwise-predicting-points-and-valuing-decisions-},
year = {2014}
}
@article{Friedman2001,
author = {Friedman, Jerome H.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Friedman - 2001 - Greedy Function Approximation A Gradient Boosting Machine.pdf:pdf},
journal = {The Annals of Statistics},
number = {5},
pages = {1189--1232},
title = {{Greedy Function Approximation: A Gradient Boosting Machine}},
url = {http://home.olemiss.edu/{~}xdang/676/Greedy{\_}function{\_}approximation{\_}a{\_}gradient{\_}bossting{\_}machine.pdf http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Greedy+Function+Approximation:+A+Gradient+Boosting+Machine{\#}3},
volume = {29},
year = {2001}
}
@inproceedings{Suzuki2008,
author = {Suzuki, Jun and Isozaki, Hideki},
booktitle = {ACL},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Suzuki, Isozaki - 2008 - Semi-Supervised Sequential Labeling and Segmentation Using Giga-Word Scale Unlabeled Data.pdf:pdf},
pages = {665--673},
title = {{Semi-Supervised Sequential Labeling and Segmentation Using Giga-Word Scale Unlabeled Data.}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.164.5597{\&}rep=rep1{\&}type=pdf},
year = {2008}
}
@phdthesis{Macia2011,
author = {Macia, Nuria},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Macia - 2011 - Data Complexity in Supervised Learning A Far-Reaching Implication.pdf:pdf},
title = {{Data Complexity in Supervised Learning: A Far-Reaching Implication}},
year = {2011}
}
@article{Adams,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.01344v1},
author = {Adams, Ryan P},
eprint = {arXiv:1504.01344v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Adams - Unknown - Early Stopping is Nonparametric Variational Inference.pdf:pdf},
title = {{Early Stopping is Nonparametric Variational Inference}}
}
@inproceedings{Blum1998,
author = {Blum, Avrim and Mitchell, Tom},
booktitle = {Proceedings of the 11th Annual Conference on Computational Learning Theory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Blum, Mitchell - 1998 - Combining labeled and unlabeled data with co-training.pdf:pdf},
pages = {92--100},
title = {{Combining labeled and unlabeled data with co-training}},
url = {http://dl.acm.org/citation.cfm?id=279962},
year = {1998}
}
@article{Smola2005,
abstract = {We present methods for dealing with missing variables in the context
  of Gaussian Processes and Support Vector Machines. This solves an
  important problem which has largely been ignored by kernel methods:
  How to systematically deal with incomplete data? Our method can also
  be applied to problems with partially observed labels as well as to
  the transductive setting where we view the labels as missing data.
  
  Our approach relies on casting kernel methods as an estimation
  problem in exponential families. Hence, estimation with missing
  variables becomes a problem of computing marginal distributions, and
  finding efficient optimization methods. To that extent we propose an
  optimization scheme which extends the Concave Convex Procedure (CCP)
  of Yuille and Rangarajan, and present a simplified and intuitive
  proof of its convergence. We show how our algorithm can be
  specialized to various cases in order to efficiently solve the
  optimization problems that arise. Encouraging preliminary
  experimental results on the USPS dataset are also presented.},
author = {Smola, Alex and Vishwanathan, S V N and Hoffman, Thomas},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Smola, Vishwanathan, Hoffman - 2005 - Kernel Methods for Missing Variables.pdf:pdf},
isbn = {097273581X},
keywords = {Learning/Statistics {\&} Optimisation,Theory {\&} Algorithms},
title = {{Kernel Methods for Missing Variables}},
url = {http://eprints.pascal-network.org/archive/00002053/},
year = {2005}
}
@article{Friedman2000,
author = {Friedman, Jerome H. and Hastie, Trevor and Tibshirani, Robert},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Friedman, Hastie, Tibshirani - 2000 - Additive Logistic Regression A Statistical View of Boosting.pdf:pdf},
journal = {The Annals of Statistics},
number = {2},
pages = {337--407},
title = {{Additive Logistic Regression: A Statistical View of Boosting}},
volume = {28},
year = {2000}
}
@inproceedings{Mey2016,
author = {Mey, Alexander and Loog, Marco},
booktitle = {Proceedings of the 23rd International Conference on Pattern Recognition (To Appear)},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mey, Loog - 2016 - A Soft-Labeled Self-Training Approach.pdf:pdf},
title = {{A Soft-Labeled Self-Training Approach}},
year = {2016}
}
@incollection{Loog2016a,
archivePrefix = {arXiv},
arxivId = {1707.04025},
author = {Loog, Marco and Krijthe, Jesse Hendrik and Jensen, A C},
booktitle = {Handbook of Pattern Recognition and Computer Vision},
chapter = {1.3},
edition = {5},
editor = {Chen, C H},
eprint = {1707.04025},
publisher = {World Scientific},
title = {{On Measuring and Quantifying Performance: Error Rates, Surrogate Loss, and an Example in SSL}},
year = {2016}
}
@article{Engelhardt2017,
author = {Engelhardt, Ellen G. and van den Broek, Alexandra J. and Linn, Sabine C. and Wishart, Gordon C. and Rutgers, Emiel J. Th. and van de Velde, Anthonie O. and Smit, Vincent T.H.B.M. and Voogd, Adri C. and Siesling, Sabine and Brinkhuis, Mari{\"{e}}l and Seynaeve, Caroline and Westenend, Pieter J. and Stiggelbout, Anne M. and Tollenaar, Rob A.E.M. and van Leeuwen, Flora E. and {van 't Veer}, Laura J. and Ravdin, Peter M. and Pharaoh, Paul D.P. and Schmidt, Marjanka K.},
doi = {10.1016/j.ejca.2017.03.015},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/PIIS095980491730833X.pdf:pdf},
issn = {09598049},
journal = {European Journal of Cancer},
number = {August 2016},
pages = {37--44},
title = {{Accuracy of the online prognostication tools PREDICT and Adjuvant! for early-stage breast cancer patients younger than 50 years}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S095980491730833X},
volume = {78},
year = {2017}
}
@article{Dean2006,
abstract = {An authentic food is one that is what it purports to be. Food processors and consumers need to be assured that, when they pay for a specific product or ingredient, they are receiving exactly what they pay for. Classification methods are an important tool in food authenticity studies where they are used to assign food samples of unknown type to known types. A classification method is developed where the classification rule is estimated by using both the labelled and the unlabelled data, in contrast with many classical methods which use only the labelled data for estimation. This methodology models the data as arising from a Gaussian mixture model with parsimonious covariance structure, as is done in model-based clustering. A missing data formulation of the mixture model is used and the models are fitted by using the EM and classification EM algorithms. The methods are applied to the analysis of spectra of food-stuffs recorded over the visible and near infra-red wavelength range in food authenticity studies. A comparison of the performance of model-based discriminant analysis and the method of classification proposed is given. The classification method proposed is shown to yield very good misclassification rates. The correct classification rate was observed to be as much as 15{\%} higher than the correct classification rate for model-based discriminant analysis.},
author = {Dean, Nema and Murphy, Thomas Brendan and Downey, Gerard},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dean, Murphy, Downey - 2006 - Using unlabelled data to update classification rules with applications in food authenticity studies.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
keywords = {Classification,Discriminant analysis,Food authenticity studies,Model-based clustering,Near infra-red spectroscopy},
number = {1},
pages = {1--14},
title = {{Using unlabelled data to update classification rules with applications in food authenticity studies}},
volume = {55},
year = {2006}
}
@article{Rubin1976,
abstract = {SUMMARY When making sampling distribution inferences about the parameter of the data, $\theta$, it is appropriate to ignore the process that causes missing data if the missing data are ‘missing at random' and the observed data are ‘observed at random', but these inferences are generally conditional on the observed pattern of missing data. When making direct-likelihood or Bayesian inferences about $\theta$, it is appropriate to ignore the process that causes missing data if the missing data are missing at random and the parameter of the missing data process is ‘distinct' from $\theta$. These conditions are the weakest general conditions under which ignoring the process that causes missing data always leads to correct inferences.},
author = {Rubin, Donald B.},
doi = {10.2307/2335739},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rubin - 1976 - Inference and Missing Data.pdf:pdf},
isbn = {0006344414643510},
issn = {00063444},
journal = {Biometrika},
number = {3},
pages = {581},
pmid = {86},
title = {{Inference and Missing Data}},
url = {http://biomet.oxfordjournals.org.libproxy1.nus.edu.sg/content/63/3/581{\%}5Cnhttp://www.jstor.org/stable/2335739?origin=crossref},
volume = {63},
year = {1976}
}
@article{Kanamori2012,
author = {Kanamori, Takafumi and Takeda, A and Suzuki, T},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kanamori, Takeda, Suzuki - 2012 - A Conjugate Property between Loss Functions and Uncertainty Sets in Classification Problems.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {consistency,convex conjugate,loss function,uncertainty set},
pages = {1461--1504},
title = {{A Conjugate Property between Loss Functions and Uncertainty Sets in Classification Problems}},
url = {http://arxiv.org/abs/1204.6583},
volume = {14},
year = {2012}
}
@article{Yuille2003,
abstract = {The concave-convex procedure (CCCP) is a way to construct discrete-time iterative dynamical systems that are guaranteed to decrease global optimization and energy functions monotonically. This procedure can be applied to almost any optimization problem, and many existing algorithms can be interpreted in terms of it. In particular, we prove that all expectation-maximization algorithms and classes of Legendre minimization and variational bounding algorithms can be reexpressed in terms of CCCP. We show that many existing neural network and mean-field theory algorithms are also examples of CCCP. The generalized iterative scaling algorithm and Sinkhorn's algorithm can also be expressed as CCCP by changing variables. CCCP can be used both as a new way to understand, and prove the convergence of, existing optimization algorithms and as a procedure for generating new algorithms.},
author = {Yuille, a L and Rangarajan, Anand},
doi = {10.1162/08997660360581958},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yuille, Rangarajan - 2003 - The concave-convex procedure.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Energy Metabolism,Neural Networks (Computer)},
month = {apr},
number = {4},
pages = {915--36},
pmid = {12689392},
title = {{The concave-convex procedure.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12689392},
volume = {15},
year = {2003}
}
@article{Dietterich1998,
author = {Dietterich, Thomas G},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dietterich - 1998 - Approximate statistical tests for comparing supervised classification learning algorithms.pdf:pdf},
journal = {Neural computation},
title = {{Approximate statistical tests for comparing supervised classification learning algorithms}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/089976698300017197},
year = {1998}
}
@article{Trapp,
abstract = {In several domains obtaining class annotations is expensive while at the same time unlabelled data are abundant. While most semi-supervised approaches enforce restrictive assumptions on the data distribution, recent work has man-aged to learn semi-supervised models in a non-restrictive regime. However, so far such ap-proaches have only been proposed for linear models. In this work, we introduce semi-supervised parameter learning for Sum-Product Networks (SPNs). SPNs are deep probabilis-tic models admitting inference in linear time in number of network edges. Our approach has several advantages, as it (1) allows genera-tive and discriminative semi-supervised learn-ing, (2) guarantees that adding unlabelled data can increase, but not degrade, the performance (safe), and (3) is computationally efficient and does not enforce restrictive assumptions on the data distribution We show on a variety of data sets that safe semi-supervised learning with SPNs is competitive compared to state-of-the-art and can lead to a better generative and dis-criminative objective value than a purely super-vised approach.},
author = {Trapp, Martin and Madl, Tamas and Peharz, Robert and Pernkopf, Franz and Trappl, Robert},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/108.pdf:pdf},
title = {{Safe Semi-Supervised Learning of Sum-Product Networks}},
url = {https://www.spsc.tugraz.at/sites/default/files/cameraready{\_}version{\_}0.pdf}
}
@article{Leisch2004,
author = {Leisch, F},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Leisch - 2004 - {\{}FlexMix{\}} A General Framework for Finite Mixture Models and Latent Class Regression in {\{}R{\}}.pdf:pdf},
keywords = {finite mixture models,latent class regression,model based clustering,r},
number = {8},
pages = {1--18},
title = {{{\{}FlexMix{\}}: A General Framework for Finite Mixture Models and Latent Class Regression in {\{}R{\}}}},
volume = {11},
year = {2004}
}
@article{Dwork2015,
author = {Dwork, Cynthia and Feldman, Vitaly and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Roth, Aaron},
doi = {10.1126/science.aaa9375},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dwork et al. - 2015 - The reusable holdout Preserving validity in adaptive data analysis.pdf:pdf;:Users/jkrijthe/Documents/Mendeley Desktop/Dwork et al. - 2015 - The reusable holdout Preserving validity in adaptive data analysis(2).pdf:pdf},
issn = {0036-8075},
journal = {Science},
number = {6248},
pages = {636--638},
title = {{The reusable holdout: Preserving validity in adaptive data analysis}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.aaa9375},
volume = {349},
year = {2015}
}
@article{Caspell-Garcia2017,
author = {Caspell-garcia, Chelsea and Simuni, Tanya and Tosun-turgut, Duygu and Wu, I-wei and Zhang, Yu and Nalls, Mike and Singleton, Andrew and Shaw, Leslie A and Kang, Ju-hee and Trojanowski, John Q and Siderowf, Andrew and Coffey, Christopher and Lasch, Shirley and Aarsland, Dag and Burn, David and Chahine, Lana M and Espay, Alberto J and Foster, Eric D and Hawkins, Keith A and Litvan, Irene and Richard, Irene and Weintraub, Daniel},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Caspell-Garcia{\_}PlosOne (2017) PPMI Biomarker prediction of cognitive impairment in de novo PD.pdf:pdf},
isbn = {1111111111},
pages = {1--18},
title = {{Multiple modality biomarker prediction of cognitive impairment in prospectively followed de novo Parkinson disease}},
year = {2017}
}
@inproceedings{Foulds2011,
author = {Foulds, James and Smyth, Padhraic},
booktitle = {SIAM International Conference on Data Mining},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Foulds, Smyth - 2011 - Multi-instance mixture models and semi-supervised learning.pdf:pdf},
number = {Mi},
title = {{Multi-instance mixture models and semi-supervised learning}},
url = {http://siam.omnibooksonline.com/2011datamining/data/papers/256.pdf},
year = {2011}
}
@article{Loog2014b,
author = {Loog, Marco and Jensen, Are C},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog, Jensen - 2014 - Semi-Supervised Nearest Mean Classification through a constrained Log-Likelihood.pdf:pdf},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
number = {5},
pages = {995 -- 1006},
title = {{Semi-Supervised Nearest Mean Classification through a constrained Log-Likelihood}},
volume = {26},
year = {2014}
}
@article{Shalev-ShwartzSHAIS2010,
abstract = {The problem of characterizing learnability is the most basic question of statistical learning theory. A fun-damental and long-standing answer, at least for the case of supervised classification and regression, is that learnability is equivalent to uniform convergence of the empirical risk to the population risk, and that if a problem is learnable, it is learnable via empirical risk minimization. In this paper, we consider the General Learning Setting (introduced by Vapnik), which includes most statistical learning problems as special cases. We show that in this setting, there are non-trivial learning problems where uniform convergence does not hold, empirical risk minimization fails, and yet they are learnable using alternative mechanisms. Instead of uniform convergence, we identify stability as the key necessary and sufficient condition for learnability. More-over, we show that the conditions for learnability in the general setting are significantly more complex than in supervised classification and regression.},
author = {{Shalev-Shwartz SHAIS}, Shai and Shamir, Ohad and Srebro, Nathan and {Sridharan KARTHIK}, Karthik},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shalev-Shwartz SHAIS et al. - 2010 - Learnability, Stability and Uniform Convergence.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {learnability,stability,statistical learning theory,stochastic convex opti-mization,uniform convergence},
pages = {2635--2670},
title = {{Learnability, Stability and Uniform Convergence}},
volume = {11},
year = {2010}
}
@inproceedings{Ratsaby1995,
author = {Ratsaby, Joel and Venkatesht, Santosh S.},
booktitle = {Proceedings of the 8th Annual conference on Computational learning theory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ratsaby, Venkatesht - 1995 - Learning from a mixture of labeled and unlabeled examples with parametric side information.pdf:pdf},
pages = {412--417},
title = {{Learning from a mixture of labeled and unlabeled examples with parametric side information}},
url = {http://dl.acm.org/citation.cfm?id=225348},
year = {1995}
}
@article{Bach2010,
abstract = {Sparse methods for supervised learning aim at finding good linear predictors from as few variables as possible, i.e., with small cardinality of their supports. This combinatorial selection problem is often turned into a convex optimization problem by replacing the cardinality function by its convex envelope (tightest convex lower bound), in this case the L1-norm. In this paper, we investigate more general set-functions than the cardinality, that may incorporate prior knowledge or structural constraints which are common in many applications: namely, we show that for nondecreasing submodular set-functions, the corresponding convex envelope can be obtained from its $\backslash$lova extension, a common tool in submodular analysis. This defines a family of polyhedral norms, for which we provide generic algorithmic tools (subgradients and proximal operators) and theoretical results (conditions for support recovery or high-dimensional inference). By selecting specific submodular functions, we can give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups; we also define new norms, in particular ones that can be used as non-factorial priors for supervised learning.},
archivePrefix = {arXiv},
arxivId = {1008.4220},
author = {Bach, Francis},
eprint = {1008.4220},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bach - 2010 - Structured sparsity-inducing norms through submodular functions.pdf:pdf},
isbn = {9781617823800},
journal = {Advances in Neural Information Processing Systems NIPS'2010},
pages = {1--9},
title = {{Structured sparsity-inducing norms through submodular functions}},
url = {http://arxiv.org/abs/1008.4220},
year = {2010}
}
@article{Little1992,
abstract = {The literature of regression analysis with missing values of the independent variables is reviewed. Six classes of procedures are distinguished: complete case analysis, available case methods, least squares on imputed data, maximum likelihood, Bayesian methods, and multiple imputation. Methods are compared and illustrated when missing data are confined to one independent variable, and extensions to more general patterns are indicated. Attention is paid to the performance of methods when the missing data are not missing completely at random. Least squares methods that fill in missing X's using only data on the X's are contrasted with likelihood-based methods that use data on the X's and Y. The latter approach is preferred and provides methods for elaboration of the basic normal linear regression model. It is suggested that more widely distributed software is needed that advances beyond complete-case analysis, available-case analysis, and naive imputation methods. Bayesian simulation methods and multiple imputation are reviewed; these provide fruitful avenues for future research.},
author = {Little, Roderick J. a.},
doi = {10.2307/2290664},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Little - 1992 - Regression with missing x's A review.pdf:pdf},
isbn = {01621459},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {bayesian inference,imputation,incomplete data,multiple imputation},
number = {420},
pages = {1227--1237},
pmid = {318},
title = {{Regression with missing x's: A review}},
url = {http://www.jstor.org.libproxy1.nus.edu.sg/stable/2290664},
volume = {87},
year = {1992}
}
@inproceedings{Ho1996,
author = {Ho, Tin Kam and Kleinberg, Eeugene M.},
booktitle = {International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho, Kleinberg - 1996 - Building projectable classifiers of arbitrary complexity.pdf:pdf},
pages = {880--885},
title = {{Building projectable classifiers of arbitrary complexity}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=547202},
volume = {2},
year = {1996}
}
@inproceedings{Duin1995,
author = {Duin, Robert P W},
booktitle = {Proceedings of the Scandinavian Conference on Image Analysis},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Duin - 1995 - Small sample size generalization.pdf:pdf},
keywords = {classification error,linear discriminants,small sample size},
pages = {957--964},
title = {{Small sample size generalization}},
year = {1995}
}
@article{Burman1989,
author = {Burman, Prabir},
doi = {10.2307/2336116},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Burman - 1989 - A Comparative Study of Ordinary Cross-Validation, v-Fold Cross-Validation and the Repeated Learning-Testing Methods.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
month = {sep},
number = {3},
pages = {503},
title = {{A Comparative Study of Ordinary Cross-Validation, v-Fold Cross-Validation and the Repeated Learning-Testing Methods}},
url = {http://www.jstor.org/stable/2336116?origin=crossref},
volume = {76},
year = {1989}
}
@article{Lerch2015,
abstract = {In public discussions of the quality of forecasts, attention typically focuses on the predictive performance in cases of extreme events. However, the restriction of conventional forecast evaluation methods to subsets of extreme observations has unexpected and undesired effects, and is bound to discredit skillful forecasts when the signal-to-noise ratio in the data generating process is low. Conditioning on outcomes is incompatible with the theoretical assumptions of established forecast evaluation methods, thereby confronting forecasters with what we refer to as the forecaster's dilemma. For probabilistic forecasts, proper weighted scoring rules have been proposed as decision theoretically justifiable alternatives for forecast evaluation with an emphasis on extreme events. Using theoretical arguments, simulation experiments, and a real data study on probabilistic forecasts of U.S. inflation and gross domestic product growth, we illustrate and discuss the forecaster's dilemma along with potential remedies.},
archivePrefix = {arXiv},
arxivId = {1512.09244},
author = {Lerch, Sebastian and Thorarinsdottir, Thordis L. and Ravazzolo, Francesco and Gneiting, Tilmann},
doi = {10.1214/16-STS588},
eprint = {1512.09244},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/euclid.ss.1491465630.pdf:pdf},
issn = {0883-4237},
keywords = {ancus,and phrases,diebold-mariano test,feliciter,likelihood ratio test,neyman-pearson lemma,predictive performance,probabilistic forecast,proper weighted scoring rule,quod male consultum cecidit,rare and extreme events},
number = {1607},
pages = {1--30},
title = {{Forecaster's Dilemma: Extreme Events and Forecast Evaluation}},
url = {http://arxiv.org/abs/1512.09244},
volume = {32},
year = {2015}
}
@inproceedings{Dasgupta2002,
author = {Dasgupta, Sanjoy and Littman, Michael L. and McAlles},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dasgupta, Littman, McAlles - 2002 - PAC generalization bounds for co-training.pdf:pdf},
pages = {375--382},
title = {{PAC generalization bounds for co-training}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=PGrlRWV5-v0C{\&}oi=fnd{\&}pg=PA375{\&}dq=PAC+Generalization+Bounds+for+Co-training{\&}ots=auaN1CGPip{\&}sig=0dID1oXJYgeENxwSzfsntvwz{\_}oU},
year = {2002}
}
@inproceedings{Kim2014,
author = {Kim, Do-kyum and Der, Matthew and Saul, Lawrence K.},
booktitle = {AISTATS},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kim, Der, Saul - 2014 - A Gaussian Latent Variable Model for Large Margin Classification of Labeled and Unlabeled Data.pdf:pdf},
title = {{A Gaussian Latent Variable Model for Large Margin Classification of Labeled and Unlabeled Data}},
url = {http://jmlr.org/proceedings/papers/v33/kim14a.pdf},
volume = {33},
year = {2014}
}
@article{ONeill1978,
author = {O'Neill, Terence J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/O'Neill - 1978 - Normal discrimination with unclassified observations.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {unclassified},
number = {364},
pages = {821--826},
title = {{Normal discrimination with unclassified observations}},
url = {http://amstat.tandfonline.com/doi/full/10.1080/01621459.1978.10480106},
volume = {73},
year = {1978}
}
@inproceedings{Ben-David2011,
author = {Ben-David, Shai and Srebro, Nathan and Urner, R},
booktitle = {Philosophy and Machine Learning - Workshop at NIPS},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-David, Srebro, Urner - 2011 - Universal learning vs. no free lunch results.pdf:pdf},
title = {{Universal learning vs. no free lunch results}},
url = {http://www.dsi.unive.it/PhiMaLe2011/Abstract/Ben-David{\_}Srebro{\_}Urner.pdf},
year = {2011}
}
@article{Huggins2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1505.04984v1},
author = {Huggins, Jonathan H and Tenenbaum, Joshua B},
eprint = {arXiv:1505.04984v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huggins, Tenenbaum - 2015 - Risk and Regret of Hierarchical Bayesian Learners.pdf:pdf},
journal = {Proceedings of the 32nd International Conference on Machine Learning},
title = {{Risk and Regret of Hierarchical Bayesian Learners}},
volume = {37},
year = {2015}
}
@book{Brazdil2010,
author = {Brazdil, Pavel B. and Bernstein, Abraham},
editor = {Brazdil, Pavel and Bernstein, Abraham},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brazdil, Bernstein - 2010 - Proceedings of 3rd Planning to Learn Workshop at ECAI 2010.pdf:pdf},
number = {Ecai},
title = {{Proceedings of 3rd Planning to Learn Workshop at ECAI 2010}},
year = {2010}
}
@article{Wyman1990,
author = {Wyman, Frank J. and Young, Dean M. and Turner, Danny W.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wyman, Young, Turner - 1990 - A Comparison of Asymptotic Error Rate Expansions for the Sample Linear Discriminant Function.pdf:pdf},
journal = {Pattern Recognition},
number = {7},
pages = {775--783},
title = {{A Comparison of Asymptotic Error Rate Expansions for the Sample Linear Discriminant Function}},
volume = {23},
year = {1990}
}
@unpublished{Balakrishnan,
abstract = {We develop a general framework for proving rigorous guarantees on the performance of the EM algorithm and a variant known as gradient EM. Our analysis is divided into two parts: a treatment of these algorithms at the population level (in the limit of infinite data), followed by results that apply to updates based on a finite set of samples. First, we characterize the domain of attraction of any global maximizer of the population likelihood. This characterization is based on a novel view of the EM updates as a perturbed form of likelihood ascent, or in parallel, of the gradient EM updates as a perturbed form of standard gradient ascent. Leveraging this characterization, we then provide non-asymptotic guarantees on the EM and gradient EM algorithms when applied to a finite set of samples. We develop consequences of our general theory for three canonical examples of incomplete-data problems: mixture of Gaussians, mixture of regressions, and linear regression with covariates missing completely at random. In each case, our theory guarantees that with a suitable initialization, a relatively small number of EM (or gradient EM) steps will yield (with high probability) an estimate that is within statistical error of the MLE. We provide simulations to confirm this theoretically predicted behavior.},
archivePrefix = {arXiv},
arxivId = {1408.2156},
author = {Balakrishnan, Sivaraman and Wainwright, Martin J. and Yu, Bin},
eprint = {1408.2156},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balakrishnan, Wainwright, Yu - Unknown - Statistical guarantees for the EM algorithm From population to sample-based analysis.pdf:pdf},
title = {{Statistical guarantees for the EM algorithm: From population to sample-based analysis}}
}
@article{Marchand2004,
author = {Marchand, E and Strawderman, WE},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Marchand, Strawderman - 2004 - Estimation in Restricted Parameter Spaces A Review.pdf:pdf},
journal = {Institute of Mathematical Statistics Lecture Notes-Monograph Series},
number = {2004},
pages = {21--44},
title = {{Estimation in Restricted Parameter Spaces: A Review}},
url = {http://www.jstor.org/stable/10.2307/4356296},
volume = {45},
year = {2004}
}
@article{Nishimura2015a,
abstract = {Hamiltonian Monte Carlo and related algorithms have become routinely used in Bayesian computation. The utility of such approaches is highlighted in the software package STAN, which provides a platform for automatic implementation of general Bayesian models. Hence, methods for improving the efficiency of general Hamiltonian Monte Carlo algorithms can have a substantial impact on practice. We propose such a method in this article by recycling the intermediate leap-frog steps used in approximating the Hamiltonian trajectories. Current algorithms use only the final step, and wastefully discard all the intermediate steps. We propose a simple and provably accurate approach for using these intermediate samples, boosting the effective sample size with little programming effort and essentially no extra computational cost. We show that our recycled Hamiltonian Monte Carlo algorithm can lead to substantial gains in computational efficiency in a variety of experiments.},
archivePrefix = {arXiv},
arxivId = {1511.06925},
author = {Nishimura, Akihiko and Dunson, David},
eprint = {1511.06925},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nishimura, Dunson - 2015 - Recycling intermediate steps to improve Hamiltonian Monte Carlo.pdf:pdf},
number = {1},
title = {{Recycling intermediate steps to improve Hamiltonian Monte Carlo}},
url = {http://arxiv.org/abs/1511.06925},
year = {2015}
}
@article{Greenland2016,
abstract = {Misinterpretation and abuse of statistical tests, confidence intervals, and statistical power have been decried for decades, yet remain rampant. A key problem is that there are no interpretations of these concepts that are at once simple, intuitive, correct, and foolproof. Instead, correct use and interpretation of these statistics requires an attention to detail which seems to tax the patience of working scientists. This high cognitive demand has led to an epidemic of shortcut definitions and interpretations that are simply wrong, sometimes disastrously so-and yet these misinterpretations dominate much of the scientific literature. In light of this problem, we provide definitions and a discussion of basic statistics that are more general and critical than typically found in traditional introductory expositions. Our goal is to provide a resource for instructors, researchers, and consumers of statistics whose knowledge of statistical theory and technique may be limited but who wish to avoid and spot misinterpretations. We emphasize how violation of often unstated analysis protocols (such as selecting analyses for presentation based on the P values they produce) can lead to small P values even if the declared test hypothesis is correct, and can lead to large P values even if that hypothesis is incorrect. We then provide an explanatory list of 25 misinterpretations of P values, confidence intervals, and power. We conclude with guidelines for improving statistical interpretation and reporting.},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Greenland, Sander and Senn, Stephen J. and Rothman, Kenneth J. and Carlin, John B. and Poole, Charles and Goodman, Steven N. and Altman, Douglas G.},
doi = {10.1007/s10654-016-0149-3},
eprint = {1011.1669},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/art{\%}3A10.1007{\%}2Fs10654-016-0149-3.pdf:pdf},
isbn = {1573-7284 (Electronic)$\backslash$r0393-2990 (Linking)},
issn = {15737284},
journal = {European Journal of Epidemiology},
keywords = {Confidence intervals,Hypothesis testing,Null testing,P value,Power,Significance tests,Statistical testing},
number = {4},
pages = {337--350},
pmid = {27209009},
publisher = {Springer Netherlands},
title = {{Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations}},
volume = {31},
year = {2016}
}
@article{Chan1997,
author = {Chan, Philip K. and Stolfo, Salvatore J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chan, Stolfo - 1997 - On the accuracy of meta-learning for scalable data mining.pdf:pdf},
journal = {Journal of Intelligent Information Systems},
title = {{On the accuracy of meta-learning for scalable data mining}},
url = {http://www.springerlink.com/index/M27133K052552242.pdf},
year = {1997}
}
@article{Garcia-Laencina2010,
abstract = {Pattern classification has been successfully applied in many problem domains, such as biometric recognition, document classification or medical diagnosis. Missing or unknown data are a common drawback that pattern recognition techniques need to deal with when solving real-life classification tasks. Machine learning approaches and methods imported from statistical learning theory have been most intensively studied and used in this subject. The aim of this work is to analyze the missing data problem in pattern classification tasks, and to summarize and compare some of the well-known methods used for handling missing values.},
author = {Garc{\'{i}}a-Laencina, Pedro J. and Sancho-G{\'{o}}mez, Jos{\'{e}}-Luis and Figueiras-Vidal, An{\'{i}}bal R.},
doi = {10.1007/s00521-009-0295-6},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Garc{\'{i}}a-Laencina, Sancho-G{\'{o}}mez, Figueiras-Vidal - 2010 - Pattern classification with missing data a review.pdf:pdf},
issn = {0941-0643},
journal = {Neural Computing and Applications},
keywords = {data {\'{a}},learning,neural networks {\'{a}} machine,pattern classification {\'{a}} missing},
number = {2},
pages = {263--282},
title = {{Pattern classification with missing data: a review}},
volume = {19},
year = {2010}
}
@article{Sun2013,
author = {Sun, Quan and Pfahringer, Bernhard},
doi = {10.1007/s10994-013-5387-y},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sun, Pfahringer - 2013 - Pairwise meta-rules for better meta-learning-based algorithm ranking.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {algorithm ranking,ensemble learning,meta-learning,ranking trees},
month = {jul},
number = {1},
pages = {141--161},
title = {{Pairwise meta-rules for better meta-learning-based algorithm ranking}},
url = {http://link.springer.com/10.1007/s10994-013-5387-y},
volume = {93},
year = {2013}
}
@article{Anand2013,
abstract = {Mean shift clustering is a powerful nonparametric technique that does not require prior knowledge of the number of clusters and does not constrain the shape of the clusters. However, being completely unsupervised, its performance suffers when the original distance metric fails to capture the underlying cluster structure. Despite recent advances in semi-supervised clustering methods, there has been little effort towards incorporating supervision into mean shift. We propose a semi-supervised framework for kernel mean shift clustering (SKMS) that uses only pairwise constraints to guide the clustering procedure. The points are first mapped to a high-dimensional kernel space where the constraints are imposed by a linear transformation of the mapped points. This is achieved by modifying the initial kernel matrix by minimizing a log det divergence-based objective function.We show the advantages of SKMS by evaluating its performance on various synthetic and real datasets while comparing with state-of-the-art semi-supervised clustering algorithms.},
author = {Anand, Saket and Mittal, Sushil and Tuzel, Oncel and Meer, Peter},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Anand et al. - 2013 - Semi-Supervised Kernel Mean Shift Clustering.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = {sep},
number = {6},
pages = {1201--1215},
pmid = {24101327},
title = {{Semi-Supervised Kernel Mean Shift Clustering.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24101327},
volume = {36},
year = {2013}
}
@article{Rendell1990,
author = {Rendell, Larry and Cho, Howard},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rendell, Cho - 1990 - Empirical learning as a function of concept character.pdf:pdf},
journal = {Machine Learning},
keywords = {concepts as functions,empirical concept learning,experimental studies},
pages = {267--298},
title = {{Empirical learning as a function of concept character}},
url = {http://www.springerlink.com/index/K5311727465WLH07.pdf},
volume = {5},
year = {1990}
}
@article{Kouw2016,
archivePrefix = {arXiv},
arxivId = {1512.04829},
author = {Kouw, Wouter M and Krijthe, Jesse Hendrik and Loog, Marco and {Van der Maaten}, Laurens},
eprint = {1512.04829},
journal = {Journal of Machine Learning Research},
pages = {1--32},
title = {{Feature-Level Domain Adaptation}},
volume = {17},
year = {2016}
}
@article{Rothschild2009,
abstract = {Using the 2008 elections, I explore the accuracy and infor-mational content of forecasts derived from two different types of data: polls and prediction markets. Both types of data suffer from inherent biases, and this is the first analysis to compare the accuracy of these forecasts adjusting for these biases. Moreover, the analysis expands on previous research by evaluating state-level forecasts in Presidential and Senatorial races, rather than just the national popular vote. Utilizing sev-eral different estimation strategies, I demonstrate that early in the cycle and in not-certain races debiased prediction market-based forecasts pro-vide more accurate probabilities of victory and more information than debiased poll-based forecasts. These results are significant because accu-rately documenting the underlying probabilities, at any given day before the election, is critical for enabling academics to determine the impact of shocks to the campaign, for the public to invest wisely and for practi-tioners to spend efficiently. Starting in the 2008 Presidential campaign, Nate Silver's FiveThirtyEight.com revolutionized election forecasting for the general public. Until his website was launched in March of 2008, those interested in predicting election out-comes typically reviewed national polling results that asked a representative cross-section of voters who they would vote for if the election were held that day. Yet, these raw poll numbers are volatile, subject to random sampling error on either side of the true underlying value.},
author = {Rothschild, David},
doi = {10.1093/poq/nfp082},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rothschild - 2009 - Forecasting Elections Comparing prediction markets, polls, and their biases.pdf:pdf},
isbn = {0033-362X},
issn = {0033362X},
journal = {Public Opinion Quarterly},
number = {5},
pages = {895--916},
title = {{Forecasting Elections: Comparing prediction markets, polls, and their biases}},
volume = {73},
year = {2009}
}
@article{Molinaro2005,
abstract = {In genomic studies, thousands of features are collected on relatively few samples. One of the goals of these studies is to build classifiers to predict the outcome of future observations. There are three inherent steps to this process: feature selection, model selection and prediction assessment. With a focus on prediction assessment, we compare several methods for estimating the 'true' prediction error of a prediction model in the presence of feature selection.},
author = {Molinaro, Annette M and Simon, Richard and Pfeiffer, Ruth M},
doi = {10.1093/bioinformatics/bti499},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Molinaro, Simon, Pfeiffer - 2005 - Prediction error estimation a comparison of resampling methods.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Computer Simulation,Data Interpretation, Statistical,Gene Expression Profiling,Gene Expression Profiling: methods,Models, Genetic,Models, Statistical,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Reproducibility of Results,Sample Size,Sensitivity and Specificity,Software},
month = {aug},
number = {15},
pages = {3301--7},
pmid = {15905277},
title = {{Prediction error estimation: a comparison of resampling methods.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15905277},
volume = {21},
year = {2005}
}
@book{Newport,
author = {Newport, Cal},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Newport - Unknown - Deep Work.pdf:pdf},
title = {{Deep Work}}
}
@article{Peng2002,
author = {Peng, Yonghong and Flach, Peter A. and Soares, Carlos and Brazdil, Pavel B.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Peng et al. - 2002 - Improved dataset characterisation for meta-learning.pdf:pdf},
journal = {Lecture Notes in Computer Science},
pages = {141--152},
title = {{Improved dataset characterisation for meta-learning}},
url = {http://link.springer.com/chapter/10.1007/3-540-36182-0{\_}14},
volume = {2534},
year = {2002}
}
@article{Henmi2004,
author = {Henmi, Masayuki and Eguchi, Shinto},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Henmi, Eguchi - 2004 - A paradox concerning nuisance parameters and projected estimating functions.pdf:pdf},
journal = {Biometrika},
number = {4},
pages = {929--941},
title = {{A paradox concerning nuisance parameters and projected estimating functions}},
volume = {91},
year = {2004}
}
@inproceedings{Huang2014,
abstract = {Despite their theoretical appeal and grounding in tractable convex optimization techniques, kernel methods are often not the first choice for large-scale speech applications due to their significant memory requirements and computational expense. In recent years, randomized approximate feature maps have emerged as an elegant mechanism to scale-up kernel methods. Still, in practice, a large number of random features is required to obtain acceptable accuracy in predictive tasks. In this paper, we develop two algorithmic schemes to address this computational bottleneck in the context of kernel ridge regression. The first scheme is a specialized distributed block coordinate descent procedure that avoids the explicit materialization of the feature space data matrix, while the second scheme gains efficiency by combining multiple weak random feature models in an ensemble learning framework. We demonstrate that these schemes enable kernel methods to match the performance of state of the art Deep Neural Networks on TIMIT for speech recognition and classification tasks. In particular, we obtain the best classification error rates reported on TIMIT using kernel methods.},
author = {Huang, Po Sen and Avron, Haim and Sainath, Tara N and Sindhwani, Vikas and Ramabhadran, Bhuvana},
booktitle = {Proceedings of the International Conference on Acoustics, Speech and Signal Processing},
doi = {10.1109/ICASSP.2014.6853587},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huang et al. - 2014 - Kernel methods match deep neural networks on TIMIT.pdf:pdf},
isbn = {9781479928927},
issn = {15206149},
keywords = {deep learning,distributed computing,large-scale kernel machines,random features,speech recognition},
pages = {205--209},
title = {{Kernel methods match deep neural networks on TIMIT}},
year = {2014}
}
@article{Ioannidis2005,
abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
author = {Ioannidis, John P.A.},
doi = {10.1371/journal.pmed.0020124},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ioannidis - 2005 - Why most published research findings are false.pdf:pdf},
issn = {1549-1676},
journal = {PLoS medicine},
keywords = {Bias (Epidemiology),Data Interpretation,Likelihood Functions,Meta-Analysis as Topic,Odds Ratio,Publishing,Reproducibility of Results,Research Design,Sample Size,Statistical},
month = {aug},
number = {8},
pages = {e124},
pmid = {16060722},
title = {{Why most published research findings are false.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1182327{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {2},
year = {2005}
}
@article{McLachlan1975,
author = {McLachlan, Geoffrey J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/McLachlan - 1975 - Iterative Reclassification Procedure for Constructing an Asymptotically Optimal Rule of Allocation in Discriminant An.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {350},
pages = {365--369},
title = {{Iterative Reclassification Procedure for Constructing an Asymptotically Optimal Rule of Allocation in Discriminant Analysis}},
volume = {70},
year = {1975}
}
@incollection{Rifkin2003,
author = {Rifkin, Ryan and Yeo, Gene and Poggio, Tomaso},
booktitle = {Nato Science Series Sub Series III Computer and Systems Sciences 190},
editor = {Suykens, Johan A. K. and Horvath, Gabor and Basu, Sankar and Micchelli, Charles and Vandewalle, Joos},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rifkin, Yeo, Poggio - 2003 - Regularized least-squares classification.pdf:pdf},
pages = {131--154},
publisher = {IOS Press},
title = {{Regularized least-squares classification}},
year = {2003}
}
@article{Maaløe2015a,
abstract = {Deep generative models based upon continuous variational distributions parameterized by deep networks give state-of-the-art performance. In this paper we propose a framework for extending the latent representation with extra auxiliary variables in order to make the variational distribution more expressive for semi-supervised learning. By utilizing the stochasticity of the auxiliary variable we demonstrate how to train discriminative classifiers resulting in state-of-the-art performance within semi-supervised learning exemplified by an 0.96{\%} error on MNIST using 100 labeled data points. Furthermore we observe empirically that using auxiliary variables increases convergence speed suggesting that less expressive variational distributions, not only lead to looser bounds but also slower model training.},
archivePrefix = {arXiv},
arxivId = {1602.05473},
author = {Maal{\o}e, Lars and S{\o}nderby, Casper Kaae and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
eprint = {1602.05473},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Maal{\o}e et al. - 2015 - Improving Semi-Supervised Learning with Auxiliary Deep Generative Models.pdf:pdf},
journal = {NIPS workshop},
pages = {1--5},
title = {{Improving Semi-Supervised Learning with Auxiliary Deep Generative Models}},
year = {2015}
}
@misc{Klein2004,
author = {Klein, Dan},
booktitle = {University of California at Berkeley, Computer Science {\ldots}},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Klein - 2004 - Lagrange Multipliers without Permanent Scarring.pdf:pdf},
title = {{Lagrange Multipliers without Permanent Scarring}},
url = {http://www.ee.columbia.edu/{~}vittorio/LagrangeMultipliers-Klein.pdf},
year = {2004}
}
@article{Kingma2016,
author = {Kingma, Diederik P and Salimans, Tim and Chen, Xi},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/6581-improving-variational-autoencoders-with-inverse-autoregressive-flow.pdf:pdf},
number = {Nips},
title = {{Improved Variational Inference with Inverse Autoregressive Flow}},
year = {2016}
}
@article{Pillai2015,
abstract = {The Cauchy distribution is usually presented as a mathematical curiosity, an exception to the Law of Large Numbers, or even as an "Evil" distribution in some introductory courses. It therefore surprised us when Drton and Xiao (2014) proved the following result for {\$}m=2{\$} and conjectured it for {\$}m\backslashge 3{\$}. Let {\$}X= (X{\_}1,..., X{\_}m){\$} and {\$}Y = (Y{\_}1, ...,Y{\_}m){\$} be i.i.d {\$}N(0,\backslashSigma){\$}, where {\$}\backslashSigma=\backslash{\{}\backslashsigma{\_}{\{}ij{\}}\backslash{\}}\backslashge 0{\$} is an {\$}m\backslashtimes m{\$} and $\backslash$textit{\{}arbitrary{\}} covariance matrix with {\$}\backslashsigma{\_}{\{}jj{\}}{\textgreater}0{\$} for all {\$}1\backslashleq j\backslashleq m{\$}. Then {\$}{\$}Z = $\backslash$sum{\_}{\{}j=1{\}}{\^{}}m w{\_}j $\backslash$frac{\{}X{\_}j{\}}{\{}Y{\_}j{\}} $\backslash$ $\backslash$sim $\backslash$mathrm{\{}Cauchy{\}}(0,1),{\$}{\$} as long as {\$}w=(w{\_}1,..., w{\_}m) {\$} is independent of {\$}(X, Y){\$}, {\$}w{\_}j\backslashge 0, j=1,..., m{\$}, and {\$}\backslashsum{\_}{\{}j=1{\}}{\^{}}m w{\_}j=1{\$}. In this note, we present an elementary proof of this conjecture for any {\$}m \backslashgeq 2{\$} by linking {\$}Z{\$} to a geometric characterization of Cauchy(0,1) given in Willams (1969). This general result is essential to the large sample behavior of Wald tests in many applications such as factor models and contingency tables. It also leads to other unexpected results such as {\$}{\$} $\backslash$sum{\_}{\{}i=1{\}}{\^{}}m$\backslash$sum{\_}{\{}j=1{\}}{\^{}}m $\backslash$frac{\{}w{\_}iw{\_}j$\backslash$sigma{\_}{\{}ij{\}}{\}}{\{}X{\_}iX{\_}j{\}} $\backslash$sim {\{}$\backslash$text{\{}L$\backslash$'{\{}e{\}}vy{\}}{\}}(0, 1). {\$}{\$} This generalizes the "super Cauchy phenomenon" that the average of {\$}m{\$} i.i.d. standard L$\backslash$'evy variables (i.e., inverse chi-squared variables with one degree of freedom) has the same distribution as that of a single standard L$\backslash$'evy variable multiplied by {\$}m{\$} (which is obtained by taking {\$}w{\_}j=1/m{\$} and {\$}\backslashSigma{\$} to be the identity matrix).},
archivePrefix = {arXiv},
arxivId = {1505.01957},
author = {Pillai, Natesh S. and Meng, Xiao-Li},
doi = {10.1214/15-AOS1407},
eprint = {1505.01957},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pillai, Meng - 2015 - An unexpected encounter with Cauchy and Levy.pdf:pdf},
issn = {00905364},
journal = {arXiv.org},
pages = {1--9},
title = {{An unexpected encounter with Cauchy and Levy}},
url = {http://arxiv.org/abs/1505.01957},
year = {2015}
}
@article{Spirtes2010,
author = {Spirtes, Peter},
doi = {10.2202/1557-4679.1203},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Spirtes - 2010 - Introduction to causal inference.pdf:pdf},
issn = {1557-4679},
journal = {Journal of Machine Learning Research},
month = {jan},
pages = {1643--1662},
pmid = {20305706},
title = {{Introduction to causal inference}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2836213{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {11},
year = {2010}
}
@article{Amorim2016,
abstract = {The annotation of large data sets by a classifier is a problem whose challenge increases as the number of labeled samples used to train the classifier reduces in comparison to the number of unlabeled samples. In this context, semi-supervised learning methods aim at discovering and labeling informative samples among the unlabeled ones, such that their addition to the correct class in the training set can improve classification performance. We present a semi-supervised learning approach that connects unlabeled and labeled samples as nodes of a minimum-spanning tree and partitions the tree into an optimum-path forest rooted at the labeled nodes. It is suitable when most samples from a same class are more closely connected through sequences of nearby samples than samples from distinct classes, which is usually the case in data sets with a reasonable relation between number of samples and feature space dimension. The proposed solution is validated by using several data sets and state-of-the-art methods as baselines.},
author = {Amorim, Willian P. and Falc{\~{a}}o, Alexandre X. and Papa, Jo{\~{a}}o P. and Carvalho, Marcelo H.},
doi = {10.1016/j.patcog.2016.04.020},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Amorim et al. - 2016 - Improving semi-supervised learning through optimum connectivity.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Optimum-path forest classifiers,Semi-supervised learning,ers,optimum-path forest classi fi,semi-supervised learning},
pages = {72--85},
publisher = {Elsevier},
title = {{Improving semi-supervised learning through optimum connectivity}},
url = {http://dx.doi.org/10.1016/j.patcog.2016.04.020},
volume = {60},
year = {2016}
}
@inproceedings{Druck2007,
author = {Druck, Gregory and Pal, Chris and McCallum, Andrew Kachites and Zhu, Xiaojin},
booktitle = {Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Druck et al. - 2007 - Semi-supervised classification with hybrid generativediscriminative methods.pdf:pdf},
isbn = {9781595936097},
keywords = {discriminative,hybrid generative,methods,semi-supervised learning,text classification},
pages = {280--289},
title = {{Semi-supervised classification with hybrid generative/discriminative methods}},
url = {http://dl.acm.org/citation.cfm?id=1281225},
year = {2007}
}
@article{Zhou2007b,
address = {New York, New York, USA},
author = {Zhou, Dengyong and Burges, Christopher J. C.},
doi = {10.1145/1273496.1273642},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Burges - 2007 - Spectral clustering and transductive learning with multiple views.pdf:pdf},
isbn = {9781595937933},
journal = {Proceedings of the 24th international conference on Machine learning - ICML '07},
pages = {1159--1166},
publisher = {ACM Press},
title = {{Spectral clustering and transductive learning with multiple views}},
url = {http://portal.acm.org/citation.cfm?doid=1273496.1273642},
year = {2007}
}
@article{Hand2014a,
abstract = {The area under the ROC curve is widely used as a measure of performance of classification rules. However, it has recently been shown that the measure is fundamentally incoherent, in the sense that it treats the relative severities of misclassifications differently when different classifiers are used. The H measure overcomes this by allowing a researcher to fix the distribution of relative severities to a classifier-independent setting on a given problem. This note extends the discussion, and proposes a modified standard distribution for the H measure, which better matches the requirements of researchers, the Beta(??1+1,??0+1) distribution. ?? 2013 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1202.2564},
author = {Hand, D. J. and Anagnostopoulos, C.},
doi = {10.1016/j.patrec.2013.12.011},
eprint = {1202.2564},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/1202.2564.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {AUC,Classifier performance,H measure,ROC curve,Supervised classification},
number = {1},
pages = {41--46},
title = {{A better Beta for the H measure of classification performance}},
volume = {40},
year = {2014}
}
@book{Barnett1999,
author = {Barnett, Vic},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/[Vic{\_}Barnett]{\_}Comparative{\_}statistical{\_}inference(b-ok.org).pdf:pdf},
isbn = {0471976431},
publisher = {John Wiley {\&} Sons},
title = {{Comparative Statistical Inference}},
year = {1999}
}
@misc{Mitchell1980,
author = {Mitchell, Tom M.},
booktitle = {Psychology},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mitchell - 1980 - The need for biases in learning generalizations.pdf:pdf},
title = {{The need for biases in learning generalizations}},
url = {http://dml.cs.byu.edu/{~}cgc/docs/mldm{\_}tools/Reading/Need for Bias.pdf},
year = {1980}
}
@article{Rajput2011a,
author = {Rajput, a H and Voll, a and Rajput, M L},
doi = {10.1212/WNL.0b013e3181ae7af1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rajput{\_}Neurology (2009) Course in PD subtypes.pdf:pdf},
title = {{clinicopathologic study Course in Parkinson disease subtypes}},
year = {2011}
}
@inproceedings{Brefeld2006,
author = {Brefeld, Ulf and G{\"{a}}rtner, Thomas and Scheffer, Tobias and Wrobel, Stefan},
booktitle = {Proceedings of the 23rd International Conference on Machine learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brefeld et al. - 2006 - Efficient co-regularised least squares regression.pdf:pdf},
pages = {137--144},
title = {{Efficient co-regularised least squares regression}},
url = {http://dl.acm.org/citation.cfm?id=1143862},
year = {2006}
}
@inproceedings{Widrow1960,
author = {Widrow, Bernard and Hoff, Marcian E.},
booktitle = {IRE WESCON Convention Record 4},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Widrow, Hoff - 1960 - Adaptive switching circuits.pdf:pdf},
pages = {96--104},
title = {{Adaptive switching circuits.}},
year = {1960}
}
@article{Dawid2007,
abstract = {A decision problem is defined in terms of an outcome space, an action space and a loss function. Starting from these simple ingredients, we can construct: Proper Scoring Rule; Entropy Function; Divergence Function; Riemannian Metric; and Unbiased Estimating Equation. From an abstract viewpoint, the loss function defines a duality between the outcome and action spaces, while the correspondence between a distribution and its Bayes act induces a self-duality. Together these determine a "decision geometry" for the family of distributions on outcome space. This allows generalisation of many standard statistical concepts and properties. In particular we define and study generalised exponential families. Several examples are analysed, including a general Bregman geometry.},
author = {Dawid, A. P.},
doi = {10.1007/s10463-006-0099-8},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dawid - 2007 - The geometry of proper scoring rules.pdf:pdf},
issn = {00203157},
journal = {Annals of the Institute of Statistical Mathematics},
keywords = {Bregman geometry,Decision geometry,Generalised exponential family,Information geometry,Proper scoring rule,Unbiased estimating equation},
number = {1},
pages = {77--93},
title = {{The geometry of proper scoring rules}},
volume = {59},
year = {2007}
}
@article{Tian2016,
abstract = {In this paper, we propose a strategy dealing with the semi-supervised classification problem, in which the support vector machine with self-constructed Universum is iteratively solved. Universum data, which do not belong to either class of interest, have been illustrated to encode some prior knowledge by representing meaningful concepts in the same domain as the problem at hand. Our new method is applied to seek more reliable positive and negative examples from the unlabeled dataset step by step, and the Universum support vector machine(U-SVM) is used iteratively. Different Universum data will result in different performance, so several effective approaches are explored to construct Universum datasets. Experimental results demonstrate that appropriately constructed Universum will improve the accuracy and reduce the number of iterations.},
author = {Tian, Yingjie and Zhang, Ying and Liu, Dalian},
doi = {10.1016/j.neucom.2015.11.041},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tian, Zhang, Liu - 2016 - Semi-supervised support vector classification with self-constructed Universum.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Classification,Semi-supervised,Support vector machine,Universum},
pages = {33--42},
publisher = {Elsevier},
title = {{Semi-supervised support vector classification with self-constructed Universum}},
url = {http://dx.doi.org/10.1016/j.neucom.2015.11.041},
volume = {189},
year = {2016}
}
@article{Hanselmann2008,
abstract = {Imaging mass spectrometry (IMS) is a promising technology which allows for detailed analysis of spatial distributions of (bio)molecules in organic samples. In many current applications, IMS relies heavily on (semi)automated exploratory data analysis procedures to decompose the data into characteristic component spectra and corresponding abundance maps, visualizing spectral and spatial structure. The most commonly used techniques are principal component analysis (PCA) and independent component analysis (ICA). Both methods operate in an unsupervised manner. However, their decomposition estimates usually feature negative counts and are not amenable to direct physical interpretation. We propose probabilistic latent semantic analysis (pLSA) for non-negative decomposition and the elucidation of interpretable component spectra and abundance maps. We compare this algorithm to PCA, ICA, and non-negative PARAFAC (parallel factors analysis) and show on simulated and real-world data that pLSA and non-negative PARAFAC are superior to PCA or ICA in terms of complementarity of the resulting components and reconstruction accuracy. We further combine pLSA decomposition with a statistical complexity estimation scheme based on the Akaike information criterion (AIC) to automatically estimate the number of components present in a tissue sample data set and show that this results in sensible complexity estimates.},
author = {Hanselmann, Michael and Kirchner, Marc and Renard, Bernhard Y and Amstalden, Erika R and Glunde, Kristine and Heeren, Ron M a and Hamprecht, Fred a},
doi = {10.1021/ac801303x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hanselmann et al. - 2008 - Concise representation of mass spectrometry images by probabilistic latent semantic analysis.pdf:pdf},
issn = {1520-6882},
journal = {Analytical chemistry},
keywords = {Algorithms,Breast Neoplasms,Breast Neoplasms: pathology,Computer Simulation,Female,Humans,Image Processing, Computer-Assisted,Mass Spectrometry,Principal Component Analysis,Signal Processing, Computer-Assisted},
month = {dec},
number = {24},
pages = {9649--58},
pmid = {18989936},
title = {{Concise representation of mass spectrometry images by probabilistic latent semantic analysis.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18989936},
volume = {80},
year = {2008}
}
@article{Wang2016a,
abstract = {In adaptive data analysis, the user makes a sequence of queries on the data, where at each step the choice of query may depend on the results in previous steps. The releases are often randomized in order to reduce overfitting for such adaptively chosen queries. In this paper, we propose a minimax framework for adaptive data analysis. Assuming Gaussianity of queries, we establish the first sharp minimax lower bound on the squared error in the order of {\$}O(\backslashfrac{\{}\backslashsqrt{\{}k{\}}\backslashsigma{\^{}}2{\}}{\{}n{\}}){\$}, where {\$}k{\$} is the number of queries asked, and {\$}\backslashsigma{\^{}}2/n{\$} is the ordinary signal-to-noise ratio for a single query. Our lower bound is based on the construction of an approximately least favorable adversary who picks a sequence of queries that are most likely to be affected by overfitting. This approximately least favorable adversary uses only one level of adaptivity, suggesting that the minimax risk for 1-step adaptivity with k-1 initial releases and that for {\$}k{\$}-step adaptivity are on the same order. The key technical component of the lower bound proof is a reduction to finding the convoluting distribution that optimally obfuscates the sign of a Gaussian signal. Our lower bound construction also reveals a transparent and elementary proof of the matching upper bound as an alternative approach to Russo and Zou (2015), who used information-theoretic tools to provide the same upper bound. We believe that the proposed framework opens up opportunities to obtain theoretical insights for many other settings of adaptive data analysis, which would extend the idea to more practical realms.},
archivePrefix = {arXiv},
arxivId = {1602.04287},
author = {Wang, Yu-Xiang and Lei, Jing and Fienberg, Stephen E.},
eprint = {1602.04287},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Lei, Fienberg - 2016 - A Minimax Theory for Adaptive Data Analysis.pdf:pdf},
title = {{A Minimax Theory for Adaptive Data Analysis}},
url = {http://arxiv.org/abs/1602.04287},
year = {2016}
}
@inproceedings{Haffari2007,
archivePrefix = {arXiv},
arxivId = {1206.5240},
author = {Haffari, Gholamreza and Sarkar, Anoop},
booktitle = {Proceedings of the 23rd Conference on Uncertainty in Artificial Intelligence},
eprint = {1206.5240},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Haffari, Sarkar - 2007 - Analysis of semi-supervised learning with the Yarowsky algorithm.pdf:pdf},
title = {{Analysis of semi-supervised learning with the Yarowsky algorithm}},
year = {2007}
}
@article{Bickel2009,
author = {Bickel, Steffen and Br{\"{u}}ckner, Michael and Scheffer, Tobias},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bickel, Br{\"{u}}ckner, Scheffer - 2009 - Discriminative learning under covariate shift.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {covariate shift,discriminative learning,transfer learning},
pages = {2137--2155},
title = {{Discriminative learning under covariate shift}},
url = {http://dl.acm.org/citation.cfm?id=1755858},
volume = {10},
year = {2009}
}
@article{Chapelle2006a,
address = {New York, New York, USA},
author = {Chapelle, Olivier and Chi, Mingmin and Zien, Alexander},
doi = {10.1145/1143844.1143868},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chapelle, Chi, Zien - 2006 - A continuation method for semi-supervised SVMs.pdf:pdf},
isbn = {1595933832},
journal = {Proceedings of the 23rd international conference on Machine learning - ICML '06},
pages = {185--192},
publisher = {ACM Press},
title = {{A continuation method for semi-supervised SVMs}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143868},
year = {2006}
}
@article{Fakeri-Tabrizi2015,
abstract = {In many applications, observations are available with different views. This is, for example, the case with image-text classification, multilingual document classification or document classification on the web. In addition, unlabeled multiview examples can be easily acquired, but assigning labels to these examples is usually a time consuming task. We describe a multiview self-learning strategy which trains different voting classifiers on different views. The margin distributions over the unlabeled training data, obtained with each view-specific classifier are then used to estimate an upper-bound on their transductive Bayes error. Minimizing this upper-bound provides an automatic margin-threshold which is used to assign pseudo-labels to unlabeled examples. Final class labels are then assigned to these examples, by taking a vote on the pool of the previous pseudo-labels. New view-specific classifiers are then trained using the labeled and pseudo-labeled training data. We consider applications to image-text classification and to multilingual document classification. We present experimental results on the NUS-WIDE collection and on Reuters RCV1-RCV2 which show that despite its simplicity, our approach is competitive with other state-of-the-art techniques.},
author = {Fakeri-Tabrizi, Ali and Amini, Massih Reza and Goutte, Cyril and Usunier, Nicolas},
doi = {10.1016/j.neucom.2014.12.041},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fakeri-Tabrizi et al. - 2015 - Multiview self-learning.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Image annotation,Multilingual document categorization,Multiview learning,Self-learning},
pages = {117--127},
publisher = {Elsevier},
title = {{Multiview self-learning}},
url = {http://dx.doi.org/10.1016/j.neucom.2014.12.041},
volume = {155},
year = {2015}
}
@inproceedings{Nigam2000a,
author = {Nigam, Kamal and Ghani, R},
booktitle = {Proceedings of the 9th International Conference on Information and Knowledge Management},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nigam, Ghani - 2000 - Analyzing the effectiveness and applicability of co-training.pdf:pdf},
isbn = {1581133200},
keywords = {a related set of,blum and mitchell 1,for example,in problem domains where,into,present,research uses labeled and,the features naturally divide,two disjoint sets,unlabeled data},
pages = {86--93},
title = {{Analyzing the effectiveness and applicability of co-training}},
url = {http://dl.acm.org/citation.cfm?id=354805},
year = {2000}
}
@article{Schrag2017,
abstract = {Background Parkinson's disease is associated with an increased incidence of cognitive impairment and dementia. Predicting who is at risk of cognitive decline early in the disease course has implications for clinical prognosis and for stratification of participants in clinical trials. We assessed the use of clinical information and biomarkers as predictive factors for cognitive decline in patients with newly diagnosed Parkinson's disease. Methods The Parkinson's Progression Markers Initiative (PPMI) study is a cohort study in patients with newly diagnosed Parkinson's disease. We evaluated cognitive performance (Montreal Cognitive Assessment [MoCA] scores), demographic and clinical data, APOE status, and biomarkers (CSF and dopamine transporter [DAT] imaging results). Using change in MoCA scores over 2 years, MoCA scores at 2 years' follow-up, and a diagnosis of cognitive impairment (combined mild cognitive impairment or dementia) at 2 years as outcome measures, we assessed the predictive values of baseline clinical variables and separate or combined additions of APOE status, DAT imaging, and CSF biomarkers. We did univariate and multivariate linear analyses with MoCA change scores between baseline and 2 years, and with MoCA scores at 2 years as dependent variables, using backwards linear regression analysis. Additionally, we constructed a prediction model for diagnosis of cognitive impairment using logistic regression analysis. Findings 390 patients with Parkinson's disease recruited between July 1, 2010, and May 31, 2013, and for whom data on MoCA scores at baseline and 2 years were available. In multivariate analyses, baseline age, University of Pennsylvania Smell Inventory Test (UPSIT) scores, CSF amyloid — (A$\beta$42) to t-tau ratio, and APOE status were associated with change in MoCA scores over time. Baseline age, MoCA and UPSIT scores, and CSF A$\beta$42 to t-tau ratio were associated with MoCA score at 2 years (using a backwards p-removal threshold of 0{\textperiodcentered}1). Accuracy of prediction of cognitive impairment using age alone (area under the curve 0{\textperiodcentered}68, 95{\%} CI 0{\textperiodcentered}60–0{\textperiodcentered}76) significantly improved by addition of clinical scores (UPSIT, Rapid Eye Movement Sleep Behaviour Disorder Screening Questionnaire [RBDSQ], Geriatric Depression Scale, and Movement Disorder Society Unified Parkinson's Disease Rating Scale motor scores; 0{\textperiodcentered}76, 0{\textperiodcentered}68–0{\textperiodcentered}83), CSF variables (0{\textperiodcentered}74, 0{\textperiodcentered}68–0{\textperiodcentered}81), or DAT imaging results (0{\textperiodcentered}76, 0{\textperiodcentered}68–0{\textperiodcentered}83). In combination, the five variables showing the most significant associations with cognitive impairment (age, UPSIT, RBDSQ, CSF A$\beta$42, and caudate uptake on DAT imaging) allowed prediction of cognitive impairment at 2 years (0{\textperiodcentered}80, 0{\textperiodcentered}74–0{\textperiodcentered}87; p=0{\textperiodcentered}0003 compared to age alone). Interpretation In newly diagnosed Parkinson's disease, the occurrence of cognitive impairment at 2 year follow-up can be predicted with good accuracy using a model combining information on age, non-motor assessments, DAT imaging, and CSF biomarkers. Funding None.},
author = {Schrag, Anette and Siddiqui, Uzma Faisal and Anastasiou, Zacharias and Weintraub, Daniel and Schott, Jonathan M.},
doi = {10.1016/S1474-4422(16)30328-3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schrag{\_}LancetNeur (2017) Clinical variables and biomarkers in prediction of MoCA scores - 2 year follow-up.pdf:pdf},
isbn = {1474-4465},
issn = {14744465},
journal = {The Lancet Neurology},
number = {1},
pages = {1234},
pmid = {27866858},
publisher = {Elsevier Ltd},
title = {{Clinical variables and biomarkers in prediction of cognitive impairment in patients with newly diagnosed Parkinson's disease: a cohort study}},
url = {http://dx.doi.org/10.1016/S1474-4422(16)30328-3},
volume = {16},
year = {2017}
}
@article{Zhou2003,
abstract = {We consider the general problem of hlearning from labelled and unlabelled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufficiently smooth with respect to the intrinsic structure collectively revealed by known labelled and unlabelled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classification problems and demonstrated effective use of unlabelled data.},
author = {Zhou, Dengyong and Bousquet, Olivier and Lal, Thomas Navin and Weston, Jason and Sch, Bernhard},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou et al. - 2003 - Learning with Local and Global Consistency.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {595--602},
title = {{Learning with Local and Global Consistency}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2003{\_}AA41.pdf},
volume = {1},
year = {2003}
}
@inproceedings{Loog2012,
author = {Loog, Marco and Duin, Robert P W},
booktitle = {Structural, Syntactic, and Statistical Pattern Recognition (Lecture Notes in Computer Science Volume 7626)},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog, Duin - 2012 - The dipping phenomenon.pdf:pdf},
pages = {310--317},
publisher = {Springer},
title = {{The dipping phenomenon}},
year = {2012}
}
@article{Niyogi2013,
author = {Niyogi, Partha},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Niyogi - 2013 - Manifold Regularization and Semi-supervised Learning Some Theoretical Analyses.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {graph laplacian,manifold regularization,minimax rates,semi-supervised learning},
pages = {1229--1250},
title = {{Manifold Regularization and Semi-supervised Learning: Some Theoretical Analyses}},
volume = {14},
year = {2013}
}
@article{Kalousis2001,
author = {Kalousis, Alexandros and Hilario, Melanie},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kalousis, Hilario - 2001 - Model selection via meta-learning a comparative study.pdf:pdf},
journal = {International Journal on Artificial Intelligence Tools},
number = {4},
title = {{Model selection via meta-learning: a comparative study}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0218213001000647},
volume = {10},
year = {2001}
}
@article{Leistner2009,
author = {Leistner, Christian and Saffari, Amir and Santner, Jakob and Bischof, Horst},
doi = {10.1109/ICCV.2009.5459198},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Leistner et al. - 2009 - Semi-Supervised Random Forests.pdf:pdf},
isbn = {978-1-4244-4420-5},
journal = {2009 IEEE 12th International Conference on Computer Vision},
month = {sep},
pages = {506--513},
publisher = {Ieee},
title = {{Semi-Supervised Random Forests}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5459198},
year = {2009}
}
@article{Benavoli2015,
archivePrefix = {arXiv},
arxivId = {1505.02288v1},
author = {Benavoli, Alessio and Corani, Giorgio and Mangili, Francesca and Dalle, Istituto and Artificiale, Intelligenza},
eprint = {1505.02288v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Benavoli et al. - 2015 - Should we really use post-hoc tests based on mean-ranks.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {friedman test,post-hoc test,statistical comparison},
pages = {1--10},
title = {{Should we really use post-hoc tests based on mean-ranks?}},
volume = {17},
year = {2015}
}
@inproceedings{Zhou2007,
author = {Zhou, Zhi-hua and Zhan, De-Chuan and Yang, Qiang},
booktitle = {Proceedings of the 22nd national conference on Artificial intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Zhan, Yang - 2007 - Semi-supervised learning with very few labeled training examples.pdf:pdf},
title = {{Semi-supervised learning with very few labeled training examples}},
url = {http://www.aaai.org/Papers/AAAI/2007/AAAI07-107.pdf},
year = {2007}
}
